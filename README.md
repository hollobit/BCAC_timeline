# ChatGPT, GenerativeAI and LLMs Timeline 

This repository organizes a timeline of key events (products, services, papers, GitHub, blog posts and news) that occurred before and after the ChatGPT announcement. 

It's curating a variety of information in this timeline, with a particular focus on LLM and Generative AI. 

Maybe it's a scene from the hottest history, so I thought it would be important to keep those memories well, so I organized them.

## Statistics 

These diagrams were generated by ChatGPT's Code Interpreter.

<img src="statistics-1224-02.png"> 
<img src="statistics-1224-01.png">

## Contributing

Issues and Pull Requests are greatly appreciated. If you've never contributed to an open source project before I'm more than happy to walk you through how to create a pull request.

You can start by [opening an issue](https://github.com/hollobit/BCAC_timeline/issues/new) describing the problem that you're looking to resolve and we'll go from there.

## Emoji 

arXiv :x:, PDF :paperclip:, arxiv-vanity :orange_book:, paper page :house:, papers with code :eight_spoked_asterisk:, Github :octocat:

## License

This document is licensed under the [MIT license](https://opensource.org/licenses/mit-license.php) © Jonghong Jeon(전종홍)

## Timeline
|	Date	|	Announcement	|
|:-:|:--|
| 1.19 | Meta is developing open source AGI, says Zuckerberg ([news](https://venturebeat.com/ai/meta-is-all-in-on-open-source-agi-says-zuckerberg/)) | 
| 1.19 | Mark Zuckerberg’s new goal is creating artificial general intelligence ([news](https://www.theverge.com/2024/1/18/24042354/mark-zuckerberg-meta-agi-reorg-interview)) |
| 1.19 | DiffusionGPT: LLM-Driven Text-to-Image Generation System ([:x:](https://arxiv.org/abs/2401.10061)), ([:book:](https://browse.arxiv.org/pdf/2401.10061.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.10061.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.10061)), ([:house:](https://huggingface.co/papers/2401.10061)), ([HTML](https://browse.arxiv.org/html/2401.10061v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/diffusiongpt-llm-driven-text-to-image)) |
| 1.19 | ChatQA: Building GPT-4 Level Conversational QA Models ([:x:](https://arxiv.org/abs/2401.10225)), ([:book:](https://browse.arxiv.org/pdf/2401.10225.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.10225.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.10225)), ([:house:](https://huggingface.co/papers/2401.10225)), ([HTML](https://browse.arxiv.org/html/2401.10225v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chatqa-building-gpt-4-level-conversational-qa)) |
| 1.19 | VMamba: Visual State Space Model ([:x:](https://arxiv.org/abs/2401.10166)), ([:book:](https://browse.arxiv.org/pdf/2401.10166.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.10166.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.10166)), ([:house:](https://huggingface.co/papers/2401.10166)), ([HTML](https://browse.arxiv.org/html/2401.10166v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vmamba-visual-state-space-model)) |
| 1.19 | SHINOBI: Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild ([:x:](https://arxiv.org/abs/2401.10171)), ([:book:](https://browse.arxiv.org/pdf/2401.10171.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.10171.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.10171)), ([:house:](https://huggingface.co/papers/2401.10171)), ([HTML](https://browse.arxiv.org/html/2401.10171v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/shinobi-shape-and-illumination-using-neural)) |
| 1.18 | RAP-SAM: Towards Real-Time All-Purpose Segment Anything ([:x:](https://arxiv.org/abs/2401.10228)), ([:book:](https://browse.arxiv.org/pdf/2401.10228.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.10228.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.10228)), ([:house:](https://huggingface.co/papers/2401.10228)), ([HTML](https://browse.arxiv.org/html/2401.10228v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rap-sam-towards-real-time-all-purpose-segment)), ([:octocat:](https://github.com/xushilin1/rap-sam)![GitHub Repo stars](https://img.shields.io/github/stars/xushilin1/rap-sam?style=social))  |
| 1.18 | Self-Rewarding Language Models ([:x:](https://arxiv.org/abs/2401.10020)), ([:book:](https://browse.arxiv.org/pdf/2401.10020.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.10020.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.10020)), ([:house:](https://huggingface.co/papers/2401.10020)), ([HTML](https://browse.arxiv.org/html/2401.10020v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/self-rewarding-language-models)), ([:octocat:](https://github.com/mzeromiko/vmamba)![GitHub Repo stars](https://img.shields.io/github/stars/mzeromiko/vmamba?style=social)) |
| 1.18 | WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens ([:x:](https://arxiv.org/abs/2401.09985)), ([:book:](https://browse.arxiv.org/pdf/2401.09985.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.09985.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.09985)), ([:house:](https://huggingface.co/papers/2401.09985)), ([HTML](https://browse.arxiv.org/html/2401.09985v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/worlddreamer-towards-general-world-models-for)) |
| 1.18 | Improving fine-grained understanding in image-text pre-training ([:x:](https://arxiv.org/abs/2401.09865)), ([:book:](https://browse.arxiv.org/pdf/2401.09865.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.09865.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.09865)), ([:house:](https://huggingface.co/papers/2401.09865)), ([HTML](https://browse.arxiv.org/html/2401.09865v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/improving-fine-grained-understanding-in-image)) |
| 1.18 | FreGrad: Lightweight and Fast Frequency-aware Diffusion Vocoder ([:x:](https://arxiv.org/abs/2401.10032)), ([:book:](https://browse.arxiv.org/pdf/2401.10032.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.10032.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.10032)), ([:house:](https://huggingface.co/papers/2401.10032)), ([HTML](https://browse.arxiv.org/html/2401.10032v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fregrad-lightweight-and-fast-frequency-aware)), ([:octocat:](https://github.com/signofthefour/fregrad)![GitHub Repo stars](https://img.shields.io/github/stars/signofthefour/fregrad?style=social)) |
| 1.18 | CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects ([:x:](https://arxiv.org/abs/2401.09962)), ([:book:](https://browse.arxiv.org/pdf/2401.09962.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.09962.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.09962)), ([:house:](https://huggingface.co/papers/2401.09962)), ([HTML](https://browse.arxiv.org/html/2401.09962v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/customvideo-customizing-text-to-video)) |
| 1.18 | Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model ([:x:](https://arxiv.org/abs/2401.09417)), ([:book:](https://browse.arxiv.org/pdf/2401.09417.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.09417.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.09417)), ([:house:](https://huggingface.co/papers/2401.09417)), ([HTML](https://browse.arxiv.org/html/2401.09417v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vision-mamba-efficient-visual-representation)), ([:octocat:](https://github.com/hustvl/vim)![GitHub Repo stars](https://img.shields.io/github/stars/hustvl/vim?style=social))  |
| 1.18 | SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding ([:x:](https://arxiv.org/abs/2401.09340)), ([:book:](https://browse.arxiv.org/pdf/2401.09340.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.09340.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.09340)), ([:house:](https://huggingface.co/papers/2401.09340)), ([HTML](https://browse.arxiv.org/html/2401.09340v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sceneverse-scaling-3d-vision-language)) |
| 1.18 | GARField: Group Anything with Radiance Fields ([:x:](https://arxiv.org/abs/2401.09419)), ([:book:](https://browse.arxiv.org/pdf/2401.09419.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.09419.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.09419)), ([:house:](https://huggingface.co/papers/2401.09419)), ([HTML](https://browse.arxiv.org/html/2401.09419v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/garfield-group-anything-with-radiance-fields)) |
| 1.18 | TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion ([:x:](https://arxiv.org/abs/2401.09416)), ([:book:](https://browse.arxiv.org/pdf/2401.09416.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.09416.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.09416)), ([:house:](https://huggingface.co/papers/2401.09416)), ([HTML](https://browse.arxiv.org/html/2401.09416v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/texturedreamer-image-guided-texture-synthesis)) |
| 1.17 | Foundations of Vector Retrieval ([:x:](https://arxiv.org/abs/2401.09350)), ([:book:](https://browse.arxiv.org/pdf/2401.09350.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.09350.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.09350)), ([:house:](https://huggingface.co/papers/2401.09350)), ([HTML](https://browse.arxiv.org/html/2401.09350v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/foundations-of-vector-retrieval)) |
| 1.17 | ReFT: Reasoning with Reinforced Fine-Tuning ([:x:](https://arxiv.org/abs/2401.08967)), ([:book:](https://browse.arxiv.org/pdf/2401.08967.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.08967.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.08967)), ([:house:](https://huggingface.co/papers/2401.08967)), ([HTML](https://browse.arxiv.org/html/2401.08967v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/reft-reasoning-with-reinforced-fine-tuning)) |
| 1.17 | UniVG: Towards UNIfied-modal Video Generation ([:x:](https://arxiv.org/abs/2401.09084)), ([:book:](https://browse.arxiv.org/pdf/2401.09084.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.09084.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.09084)), ([:house:](https://huggingface.co/papers/2401.09084)), ([HTML](https://browse.arxiv.org/html/2401.09084v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/univg-towards-unified-modal-video-generation)) |
| 1.17 | Asynchronous Local-SGD Training for Language Modeling ([:x:](https://arxiv.org/abs/2401.09135)), ([:book:](https://browse.arxiv.org/pdf/2401.09135.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.09135.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.09135)), ([:house:](https://huggingface.co/papers/2401.09135)), ([HTML](https://browse.arxiv.org/html/2401.09135v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/asynchronous-local-sgd-training-for-language)) |
| 1.17 | VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models ([:x:](https://arxiv.org/abs/2401.09047)), ([:book:](https://browse.arxiv.org/pdf/2401.09047.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.09047.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.09047)), ([:house:](https://huggingface.co/papers/2401.15504)), ([HTML](https://browse.arxiv.org/html/2401.09047v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/videocrafter2-overcoming-data-limitations-for)) |
| 1.17 | SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers ([:x:](https://arxiv.org/abs/2401.08740)), ([:book:](https://browse.arxiv.org/pdf/2401.08740.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.08740.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.08740)), ([:house:](https://huggingface.co/papers/2401.08740)), ([HTML](https://browse.arxiv.org/html/2401.08740v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sit-exploring-flow-and-diffusion-based)) |
| 1.17 | Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis ([:x:](https://arxiv.org/abs/2401.09048)), ([:book:](https://browse.arxiv.org/pdf/2401.09048.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.09048.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.09048)), ([:house:](https://huggingface.co/papers/2401.09048)), ([HTML](https://browse.arxiv.org/html/2401.09048v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/compose-and-conquer-diffusion-based-3d-depth)) |
| 1.16 | Meta release MAGNeT for text2music ([tweet](https://twitter.com/lonziks/status/1746951479334768777)) | 
| 1.16 | Here's how OpenAI plans to address election misinformation on ChatGPT and Dall-E ([news](https://mashable.com/article/openai-2024-election-misinformation-tools)) |
| 1.16 | ChatGPT will have video functionality and more accuracy in future versions — Sam Altman says GPT-5 will be a big improvement ([news](https://www.tomsguide.com/news/chatgpt-will-have-video-functionality-and-more-accuracy-in-future-versions-sam-altman-says-gpt-5-will-be-a-big-improvement)) |
| 1.15 | GPT Store users breaking rules with 'girlfriend' bots ([news](https://mashable.com/article/openai-gpt-store-girlfriend-bots-chatgpt)) |
| 1.15 | ChatGPT for Self-Diagnosis: AI Is Changing the Way We Answer Our Own Health Questions (CNET [news](https://www.cnet.com/health/medical/chatgpt-for-self-diagnosis-ai-is-changing-the-way-we-answer-our-own-health-questions/)) |
| 1.14 | Anthropic researchers find that AI models can be trained to deceive (TechCrunch [news](https://techcrunch.com/2024/01/13/anthropic-researchers-find-that-ai-models-can-be-trained-to-deceive)) |
| 1.12 | AI girlfriend bots are already flooding OpenAI’s GPT store ([news](https://qz.com/ai-girlfriend-bots-are-already-flooding-openai-s-gpt-st-1851159131)) |
| 1.12 | Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender ([:x:](https://arxiv.org/abs/2401.06561)), ([:book:](https://browse.arxiv.org/pdf/2401.06561.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.06561.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.06561)), ([:house:](https://huggingface.co/papers/2401.06561)), ([HTML](https://browse.arxiv.org/html/2401.06561v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/intention-analysis-prompting-makes-large)) |
| 1.12 | DevEval: Evaluating Code Generation in Practical Software Projects ([:x:](https://arxiv.org/abs/2401.06401)), ([:book:](https://browse.arxiv.org/pdf/2401.06401.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.06401.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.06401)), ([:house:](https://huggingface.co/papers/2401.06401)), ([HTML](https://browse.arxiv.org/html/2401.06401v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/deveval-evaluating-code-generation-in)) |
| 1.12 | How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs ([:x:](https://arxiv.org/abs/2401.06373)), ([:book:](https://browse.arxiv.org/pdf/2401.06373.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.06373.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.06373)), ([:house:](https://huggingface.co/papers/2401.06373)), ([HTML](https://browse.arxiv.org/html/2401.06373v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/how-johnny-can-persuade-llms-to-jailbreak)) |
| 1.12 | OpenAI Quietly Deletes Ban on Using ChatGPT for “Military and Warfare” ([news](https://theintercept.com/2024/01/12/open-ai-military-ban-chatgpt/)) |
| 1.12 | Google AI has better bedside manner than human doctors — and makes better diagnoses ([Nature [doi: https://doi.org/10.1038/d41586-024-00099-4](https://www.nature.com/articles/d41586-024-00099-4)) |
| 1.12 | OpenChat: Advancing Open-source Language Models with Mixed-Quality Data ([tweet](https://twitter.com/AlphaSignalAI/status/1745519568720924852)), ([demo](https://openchat.team/ko)), ([:octocat:](https://github.com/imoneoi/openchat)![GitHub Repo stars](https://img.shields.io/github/stars/imoneoi/openchat?style=social)) |
| 1.12 | AMIE: A research AI system for diagnostic medical reasoning and conversations ([tweet](https://twitter.com/GoogleAI/status/1745843740822933517)), ([blog](https://blog.research.google/2024/01/amie-research-ai-system-for-diagnostic_12.html)) |
| 1.12 | PALP: Prompt Aligned Personalization of Text-to-Image Models ([:x:](https://arxiv.org/abs/2401.06105)), ([:book:](https://browse.arxiv.org/pdf/2401.06105.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.06105.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.06105)), ([:house:](https://huggingface.co/papers/2401.06105)), ([HTML](https://browse.arxiv.org/html/2401.06105v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/palp-prompt-aligned-personalization-of-text)) |
| 1.12 | Transformers are Multi-State RNNs ([:x:](https://arxiv.org/abs/2401.06104)), ([:book:](https://browse.arxiv.org/pdf/2401.06104.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.06104.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.06104)), ([:house:](https://huggingface.co/papers/2401.06104)), ([HTML](https://browse.arxiv.org/html/2401.06104v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/transformers-are-multi-state-rnns)) |
| 1.12 | TOFU: A Task of Fictitious Unlearning for LLMs ([:x:](https://arxiv.org/abs/2401.06121)), ([:book:](https://browse.arxiv.org/pdf/2401.06121.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.06121.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.06121)), ([:house:](https://huggingface.co/papers/2401.06121)), ([HTML](https://browse.arxiv.org/html/2401.06121v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tofu-a-task-of-fictitious-unlearning-for-llms)) |
| 1.12 | Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models ([:x:](https://arxiv.org/abs/2401.06102)), ([:book:](https://browse.arxiv.org/pdf/2401.06102.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.06102.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.06102)), ([:house:](https://huggingface.co/papers/2401.06102)), ([HTML](https://browse.arxiv.org/html/2401.06102v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/patchscope-a-unifying-framework-for)) |
| 1.12 | DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models ([:x:](https://arxiv.org/abs/2401.06066)), ([:book:](https://browse.arxiv.org/pdf/2401.06066.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.06066.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.06066)), ([:house:](https://huggingface.co/papers/2401.06066)), ([HTML](https://browse.arxiv.org/html/2401.06066v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/deepseekmoe-towards-ultimate-expert)), ([:octocat:](https://github.com/deepseek-ai/deepseek-moe)![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/deepseek-moe?style=social))  |
| 1.12 | TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering ([:x:](https://arxiv.org/abs/2401.06003)), ([:book:](https://browse.arxiv.org/pdf/2401.06003.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.06003.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.06003)), ([:house:](https://huggingface.co/papers/2401.06003)), ([HTML](https://browse.arxiv.org/html/2401.06003v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/trips-trilinear-point-splatting-for-real-time)) |
| 1.12 | Distilling Vision-Language Models on Millions of Videos ([:x:](https://arxiv.org/abs/2401.06129)), ([:book:](https://browse.arxiv.org/pdf/2401.06129.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.06129.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.06129)), ([:house:](https://huggingface.co/papers/2401.06129)), ([HTML](https://browse.arxiv.org/html/2401.06129v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/distilling-vision-language-models-on-millions)) |
| 1.12 | Secrets of RLHF in Large Language Models Part II: Reward Modeling ([:x:](https://arxiv.org/abs/2401.06080)), ([:book:](https://browse.arxiv.org/pdf/2401.06080.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.06080.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.06080)), ([:house:](https://huggingface.co/papers/2401.06080)), ([HTML](https://browse.arxiv.org/html/2401.06080v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/secrets-of-rlhf-in-large-language-models-part-1)), ([:octocat:](https://github.com/openlmlab/moss-rlhf)![GitHub Repo stars](https://img.shields.io/github/stars/openlmlab/moss-rlhf?style=social))  |
| 1.12 | LEGO:Language Enhanced Multi-modal Grounding Model ([:x:](https://arxiv.org/abs/2401.06071)), ([:book:](https://browse.arxiv.org/pdf/2401.06071.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.06071.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.06071)), ([:house:](https://huggingface.co/papers/2401.06071)), ([HTML](https://browse.arxiv.org/html/2401.06071v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lego-language-enhanced-multi-modal-grounding)), ([:octocat:](https://github.com/lzw-lzw/lego)![GitHub Repo stars](https://img.shields.io/github/stars/lzw-lzw/lego?style=social))  |
| 1.11 | OpenAI debuts ChatGPT subscription aimed at small teams (TechCrunch [news](https://techcrunch.com/2024/01/10/openai-launches-chatgpt-subscription-aimed-at-small-teams/)) |
| 1.11 | OpenAI Signs Up 260 Businesses for Corporate Version of ChatGPT (Bloomberg [news](https://www.bloomberg.com/news/articles/2024-01-11/openai-signs-up-260-businesses-for-corporate-version-of-chatgpt)), ([archive](https://archive.is/kNwmj#selection-4553.0-4553.63)) |
| 1.11 | The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models ([:x:](https://arxiv.org/abs/2401.05618)), ([:book:](https://browse.arxiv.org/pdf/2401.05618.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.05618.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.05618)), ([:house:](https://huggingface.co/papers/2401.05618)), ([HTML](https://browse.arxiv.org/html/2401.05618v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-benefits-of-a-concise-chain-of-thought-on)) |
| 1.11 | Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems ([:x:](https://arxiv.org/abs/2401.05778)), ([:book:](https://browse.arxiv.org/pdf/2401.05778.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.05778.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.05778)), ([:house:](https://huggingface.co/papers/2401.05778)), ([HTML](https://browse.arxiv.org/html/2401.05778v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/risk-taxonomy-mitigation-and-assessment)) |
| 1.11 | Surgical-DINO: Adapter Learning of Foundation Model for Depth Estimation in Endoscopic Surgery ([:x:](https://arxiv.org/abs/2401.06013)), ([:book:](https://browse.arxiv.org/pdf/2401.06013.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.06013.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.06013)), ([:house:](https://huggingface.co/papers/2401.06013)), ([HTML](https://browse.arxiv.org/html/2401.06013v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/surgical-dino-adapter-learning-of-foundation)) |
| 1.11 | Seven Failure Points When Engineering a Retrieval Augmented Generation System ([:x:](https://arxiv.org/abs/2401.05856)), ([:book:](https://browse.arxiv.org/pdf/2401.05856.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.05856.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.05856)), ([:house:](https://huggingface.co/papers/2401.05856)), ([HTML](https://browse.arxiv.org/html/2401.05856v1)), ([:eight_spoked_asterisk:]()) |
| 1.11 | Can large language models identify and correct their mistakes? ([blog](https://blog.research.google/2024/01/can-large-language-models-identify-and.html)) |
| 1.11 | PIXART-δ: Fast and Controllable Image Generation with Latent Consistency Models ([:x:](https://arxiv.org/abs/2401.05252)), ([:book:](https://browse.arxiv.org/pdf/2401.05252.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.05252.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.05252)), ([:house:](https://huggingface.co/papers/2401.05252)), ([HTML](https://browse.arxiv.org/html/2401.05252v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pixart-d-fast-and-controllable-image)), ([:octocat:](https://github.com/PixArt-alpha/PixArt-alpha)![GitHub Repo stars](https://img.shields.io/github/stars/PixArt-alpha/PixArt-alpha?style=social))  |
| 1.11 | InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes ([:x:](https://arxiv.org/abs/2401.05335)), ([:book:](https://browse.arxiv.org/pdf/2401.05335.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.05335.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.05335)), ([:house:](https://huggingface.co/papers/2401.05335)), ([HTML](https://browse.arxiv.org/html/2401.05335v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/inserf-text-driven-generative-object)) |
| 1.11 | URHand: Universal Relightable Hands ([:x:](https://arxiv.org/abs/2401.05334)), ([:book:](https://browse.arxiv.org/pdf/2401.05334.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.05334.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.05334)), ([:house:](https://huggingface.co/papers/2401.05334)), ([HTML](https://browse.arxiv.org/html/2401.05334v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/urhand-universal-relightable-hands)) |
| 1.11 | ANIM-400K: A Large-Scale Dataset for Automated End-To-End Dubbing of Video ([:x:](https://arxiv.org/abs/2401.05314)), ([:book:](https://browse.arxiv.org/pdf/2401.05314.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.05314.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.05314)), ([:house:](https://huggingface.co/papers/2401.05314)), ([HTML](https://browse.arxiv.org/html/2401.05314v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/anim-400k-a-large-scale-dataset-for-automated)), ([:octocat:](https://github.com/davidmchan/anim400k)![GitHub Repo stars](https://img.shields.io/github/stars/davidmchan/anim400k?style=social))  |
| 1.11 | Score Distillation Sampling with Learned Manifold Corrective ([:x:](https://arxiv.org/abs/2401.05293)), ([:book:](https://browse.arxiv.org/pdf/2401.05293.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.05293.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.05293)), ([:house:](https://huggingface.co/papers/2401.05293)), ([HTML](https://browse.arxiv.org/html/2401.05293v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/score-distillation-sampling-with-learned)) |
| 1.11 | Parrot: Pareto-optimal Multi-Reward Reinforcement Learning Framework for Text-to-Image Generation ([:x:](https://arxiv.org/abs/2401.05675)), ([:book:](https://browse.arxiv.org/pdf/2401.05675.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.05675.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.05675)), ([:house:](https://huggingface.co/papers/2401.05675)), ([HTML](https://browse.arxiv.org/html/2401.05675v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/parrot-pareto-optimal-multi-reward)) |
| 1.11 | Object-Centric Diffusion for Efficient Video Editing ([:x:](https://arxiv.org/abs/2401.05735)), ([:book:](https://browse.arxiv.org/pdf/2401.05735.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.05735.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.05735)), ([:house:](https://huggingface.co/papers/2401.05735)), ([HTML](https://browse.arxiv.org/html/2401.05735v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/object-centric-diffusion-for-efficient-video)) |
| 1.11 | Diffusion Priors for Dynamic View Synthesis from Monocular Videos ([:x:](https://arxiv.org/abs/2401.05583)), ([:book:](https://browse.arxiv.org/pdf/2401.05583.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.05583.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.05583)), ([:house:](https://huggingface.co/papers/2401.05583)), ([HTML](https://browse.arxiv.org/html/2401.05583v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/diffusion-priors-for-dynamic-view-synthesis)) |
| 1.11 | A Shocking Amount of the Web is Machine Translated: Insights from Multi-Way Parallelism ([:x:](https://arxiv.org/abs/2401.05749)), ([:book:](https://browse.arxiv.org/pdf/2401.05749.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.05749.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.05749)), ([:house:](https://huggingface.co/papers/2401.05749)), ([HTML](https://browse.arxiv.org/html/2401.05749v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-shocking-amount-of-the-web-is-machine)) |
| 1.11 | Towards Conversational Diagnostic AI ([:x:](https://arxiv.org/abs/2401.05654)), ([:book:](https://browse.arxiv.org/pdf/2401.05654.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.05654.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.05654)), ([:house:](https://huggingface.co/papers/2401.05654)), ([HTML](https://browse.arxiv.org/html/2401.05654v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-conversational-diagnostic-ai)) |
| 1.11 | Tuning LLMs with Contrastive Alignment Instructions for Machine Translation in Unseen, Low-resource Languages ([:x:](https://arxiv.org/abs/2401.05811)), ([:book:](https://browse.arxiv.org/pdf/2401.05811.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.05811.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.05811)), ([:house:](https://huggingface.co/papers/2401.05811)), ([HTML](https://browse.arxiv.org/html/2401.05811v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tuning-llms-with-contrastive-alignment)) |
| 1.11 | Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training ([:x:](https://arxiv.org/abs/2401.05566)), ([:book:](https://browse.arxiv.org/pdf/2401.05566.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.05566.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.05566)), ([:house:](https://huggingface.co/papers/2401.05566)), ([HTML](https://browse.arxiv.org/html/2401.05566v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sleeper-agents-training-deceptive-llms-that)), ([:octocat:](https://github.com/anthropics/sleeper-agents-paper)![GitHub Repo stars](https://img.shields.io/github/stars/anthropics/sleeper-agents-paper?style=social))  |
| 1.10 | OpenAI's GPT Store Now Offers a Selection of 3 Million Custom AI Bots ([news](https://www.cnet.com/tech/computing/openais-gpt-store-now-offers-a-selection-of-3-million-custom-ai-bots/)) |
| 1.10 | VLP: Vision Language Planning for Autonomous Driving ([:x:](https://arxiv.org/abs/2401.05577)), ([:book:](https://browse.arxiv.org/pdf/2401.05577.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.05577.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.05577)), ([:house:](https://huggingface.co/papers/2401.05577)), ([HTML](https://browse.arxiv.org/html/2401.05577v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vlp-vision-language-planning-for-autonomous)) |
| 1.10 | Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security ([:x:](https://arxiv.org/abs/2401.05459)), ([:book:](https://browse.arxiv.org/pdf/2401.05459.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.05459.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.05459)), ([:house:](https://huggingface.co/papers/2401.05459)), ([HTML](https://browse.arxiv.org/html/2401.05459v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/personal-llm-agents-insights-and-survey-about)), ([:octocat:](https://github.com/mobilellm/personal_llm_agents_survey)![GitHub Repo stars](https://img.shields.io/github/stars/mobilellm/personal_llm_agents_survey?style=social))  |
| 1.10 | Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models ([:x:](https://arxiv.org/abs/2401.04658)), ([:book:](https://browse.arxiv.org/pdf/2401.04658.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.04658.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.04658)), ([:house:](https://huggingface.co/papers/2401.04658)), ([HTML](https://browse.arxiv.org/html/2401.04658v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lightning-attention-2-a-free-lunch-for)) |
| 1.10 | Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with Multi-Granularity Answers ([:x:](https://arxiv.org/abs/2401.04695)), ([:book:](https://browse.arxiv.org/pdf/2401.04695.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.04695.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.04695)), ([:house:](https://huggingface.co/papers/2401.04695)), ([HTML](https://browse.arxiv.org/html/2401.04695v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/narrowing-the-knowledge-evaluation-gap-open)) |
| 1.10 | The Impact of Reasoning Step Length on Large Language Models ([:x:](https://arxiv.org/abs/2401.04925)), ([:book:](https://browse.arxiv.org/pdf/2401.04925.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.04925.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.04925)), ([:house:](https://huggingface.co/papers/2401.04925)), ([HTML](https://browse.arxiv.org/html/2401.04925v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-impact-of-reasoning-step-length-on-large)) |
| 1.10 | Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk ([:x:](https://arxiv.org/abs/2401.05033)), ([:book:](https://browse.arxiv.org/pdf/2401.05033.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.05033.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.05033)), ([:house:](https://huggingface.co/papers/2401.05033)), ([HTML](https://browse.arxiv.org/html/2401.05033v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/bootstrapping-llm-based-task-oriented)) |
| 1.10 | Introducing the GPT Store ([blog](https://openai.com/blog/introducing-the-gpt-store)) |
| 1.10 | TrustLLM: Trustworthiness in Large Language Models ([project](https://trustllmbenchmark.github.io/TrustLLM-Website/)), ([:x:](https://arxiv.org/abs/2401.05561)), ([:book:](https://browse.arxiv.org/pdf/2401.05561.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.05561.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.05561)), ([:house:](https://huggingface.co/papers/2401.05561)), ([HTML](https://browse.arxiv.org/html/2401.05561v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/trustllm-trustworthiness-in-large-language)), ([:octocat:](https://github.com/HowieHwong/TrustLLM)![GitHub Repo stars](https://img.shields.io/github/stars/HowieHwong/TrustLLM?style=social)) |
| 1.10 | Jump Cut Smoothing for Talking Heads ([:x:](https://arxiv.org/abs/2401.04718)), ([:book:](https://browse.arxiv.org/pdf/2401.04718.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.04718.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.04718)), ([:house:](https://huggingface.co/papers/2401.04718)), ([:eight_spoked_asterisk:]()) |
| 1.9 | DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference ([:x:](https://arxiv.org/abs/2401.08671)), ([:book:](https://browse.arxiv.org/pdf/2401.08671.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.08671.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.08671)), ([:house:](https://huggingface.co/papers/2401.08671)), ([HTML](https://browse.arxiv.org/html/2401.08671v1)), ([:eight_spoked_asterisk:]()) |
| 1.9 | Masked Audio Generation using a Single Non-Autoregressive Transformer ([project](https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT/)), ([:x:](https://arxiv.org/abs/2401.04577)), ([:book:](https://browse.arxiv.org/pdf/2401.04577.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.04577.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.04577)), ([:house:](https://huggingface.co/papers/2401.04577)), ([HTML](https://browse.arxiv.org/html/2401.04577v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/masked-audio-generation-using-a-single-non)) |
| 1.9 | Let's Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual Concept Understanding ([:x:](https://arxiv.org/abs/2401.04575)), ([:book:](https://browse.arxiv.org/pdf/2401.04575.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.04575.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.04575)), ([:house:](https://huggingface.co/papers/2401.04575)), ([HTML](https://browse.arxiv.org/html/2401.04575v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/let-s-go-shopping-lgs-web-scale-image-text)) |
| 1.9 | Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding ([:x:](https://arxiv.org/abs/2401.04398)), ([:book:](https://browse.arxiv.org/pdf/2401.04398.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.04398.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.04398)), ([:house:](https://huggingface.co/papers/2401.04398)), ([HTML](https://browse.arxiv.org/html/2401.04398v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chain-of-table-evolving-tables-in-the)) |
| 1.9 | FADI-AEC: Fast Score Based Diffusion Model Guided by Far-end Signal for Acoustic Echo Cancellation ([:x:](https://arxiv.org/abs/2401.04283)), ([:book:](https://browse.arxiv.org/pdf/2401.04283.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.04283.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.04283)), ([:house:](https://huggingface.co/papers/2401.04283)), ([HTML](https://browse.arxiv.org/html/2401.04283v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fadi-aec-fast-score-based-diffusion-model)) |
| 1.9 | New York Times-ChatGPT lawsuit poses new legal threats to artificial intelligence ([news](https://thehill.com/policy/technology/4392624-new-york-times-chatgpt-lawsuit-poses-new-legal-threats-to-artificial-intelligence/)) |
| 1.9 | GPT-Pilot: Dev tool that writes scalable apps from scratch while the developer oversees the implementation ([:octocat:](https://github.com/Pythagora-io/gpt-pilot)![GitHub Repo stars](https://img.shields.io/github/stars/Pythagora-io/gpt-pilot?style=social)) |
| 1.9 | MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation ([proejct](https://magicvideov2.github.io/)), ([:x:](https://arxiv.org/abs/2401.04468)), ([:book:](https://browse.arxiv.org/pdf/2401.04468.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.04468.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.04468)), ([:house:](https://huggingface.co/papers/2401.04468)), ([:eight_spoked_asterisk:]()) |
| 1.9 | AGG: Amortized Generative 3D Gaussians for Single Image to 3D ([:x:](https://arxiv.org/abs/2401.04099)), ([:book:](https://browse.arxiv.org/pdf/2401.04099.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.04099.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.04099)), ([:house:](https://huggingface.co/papers/2401.04099)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/agg-amortized-generative-3d-gaussians-for)) |
| 1.9 | MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts ([:x:](https://arxiv.org/abs/2401.04081)), ([:book:](https://browse.arxiv.org/pdf/2401.04081.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.04081.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.04081)), ([:house:](https://huggingface.co/papers/2401.04081)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/moe-mamba-efficient-selective-state-space)) |
| 1.9 | GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation ([:x:](https://arxiv.org/abs/2401.04092)), ([:book:](https://browse.arxiv.org/pdf/2401.04092.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.04092.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.04092)), ([:house:](https://huggingface.co/papers/2401.04092)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gpt-4v-ision-is-a-human-aligned-evaluator-for)), ([:octocat:](https://github.com/3DTopia/GPTEval3D)![GitHub Repo stars](https://img.shields.io/github/stars/3DTopia/GPTEval3D?style=social))  |
| 1.8 | Volkswagen brings ChatGPT into compact cars ([news](https://www.foxbusiness.com/technology/volkswagen-brings-chatgpt-into-compact-cars)) |
| 1.8 | OpenAI - OpenAI and journalism ([blog](https://openai.com/blog/openai-and-journalism)) |
| 1.8 | FlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGAs ([:x:](https://arxiv.org/abs/2401.03868)), ([:book:](https://browse.arxiv.org/pdf/2401.03868.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.03868.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.03868)), ([:house:](https://huggingface.co/papers/2401.03868)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/flightllm-efficient-large-language-model)) |
| 1.8 | TeleChat Technical Report ([:x:](https://arxiv.org/abs/2401.03804)), ([:book:](https://browse.arxiv.org/pdf/2401.03804.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.03804.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.03804)), ([:house:](https://huggingface.co/papers/2401.03804)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/telechat-technical-report)) |
| 1.8 | From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations ([:octocat:](https://github.com/facebookresearch/audio2photoreal)![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/audio2photoreal?style=social)) | 
| 1.8 | A Complete List of ArXiv Papers on Alignment, Safety, and Security of Large Language Models (LLMs) ([list](https://unispac.github.io/arxiv-llm-alignment-safety-security/)) |
| 1.8 | Mixtral of Experts ([:x:](https://arxiv.org/abs/2401.04088)), ([:book:](https://browse.arxiv.org/pdf/2401.04088.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.04088.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.04088)), ([:house:](https://huggingface.co/papers/2401.04088)), ([:eight_spoked_asterisk:]()) |
| 1.7 | DiarizationLM: Speaker Diarization Post-Processing with Large Language Models ([:x:](https://arxiv.org/abs/2401.03506)), ([:book:](https://browse.arxiv.org/pdf/2401.03506.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.03506.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.03506)), ([:house:](https://huggingface.co/papers/2401.03506)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/diarizationlm-speaker-diarization-post)), ([:octocat:](https://github.com/google/speaker-id)![GitHub Repo stars](https://img.shields.io/github/stars/google/speaker-id?style=social))  |
| 1.7 | Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon ([:x:](https://arxiv.org/abs/2401.03462)), ([:book:](https://browse.arxiv.org/pdf/2401.03462.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.03462.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.03462)), ([:house:](https://huggingface.co/papers/2401.03462)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/soaring-from-4k-to-400k-extending-llm-s)), ([:octocat:](https://github.com/flagopen/flagembedding)![GitHub Repo stars](https://img.shields.io/github/stars/flagopen/flagembedding?style=social))  |
| 1.7 | How To Make Money In 2024 Using ChatGPT’s GPT Store (Forbes [news](https://www.forbes.com/sites/rachelwells/2024/01/07/how-to-make-money-in-2024-using-chatgpts-gpt-store)) |
| 1.6 | CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution ([:x:](https://arxiv.org/abs/2401.03065)), ([:book:](https://browse.arxiv.org/pdf/2401.03065.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.03065.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.03065)), ([:house:](https://huggingface.co/papers/2401.03065)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/cruxeval-a-benchmark-for-code-reasoning)) |
| 1.6 | Denoising Vision Transformers ([:x:](https://arxiv.org/abs/2401.02957)), ([:book:](https://browse.arxiv.org/pdf/2401.02957.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02957.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02957)), ([:house:](https://huggingface.co/papers/2401.02957)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/denoising-vision-transformers)) |
| 1.6 | Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively ([:x:](https://arxiv.org/abs/2401.02955)), ([:book:](https://browse.arxiv.org/pdf/2401.02955.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02955.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02955)), ([:house:](https://huggingface.co/papers/2401.02955)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/open-vocabulary-sam-segment-and-recognize)), ([:octocat:](https://github.com/harboryuan/ovsam)![GitHub Repo stars](https://img.shields.io/github/stars/harboryuan/ovsam?style=social))  |
| 1.6 | Microsoft Phi-2 model changes licence to MIT ([news](https://news.ycombinator.com/item?id=38889539)) |
| 1.6 | Microsoft, OpenAI sued for copyright infringement by nonfiction book authors in class action claim (CNBC [news](https://www.cnbc.com/2024/01/05/microsoft-openai-sued-over-copyright-infringement-by-authors.html)) |
| 1.5 | Levels of AGI: Operationalizing Progress on the Path to AGI (v2) ([:x:](https://arxiv.org/abs/2311.02462)), ([:book:](https://browse.arxiv.org/pdf/2311.02462.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.02462.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.02462)), ([:house:](https://huggingface.co/papers/2311.02462)), ([HTML](https://browse.arxiv.org/html/2311.02462v2)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/levels-of-agi-operationalizing-progress-on)), ([SS](https://www.semanticscholar.org/paper/Levels-of-AGI%3A-Operationalizing-Progress-on-the-to-Morris-Sohl-Dickstein/a2160ce64f13948222d6619d8b8b3a86d2991772)) |
| 1.5 | CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal Models with Multiple Image Inputs ([:x:](https://arxiv.org/abs/2401.02582)), ([:book:](https://browse.arxiv.org/pdf/2401.02582.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02582.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02582)), ([:house:](https://huggingface.co/papers/2401.02582)), ([HTML](https://browse.arxiv.org/html/2401.02582v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/cocot-contrastive-chain-of-thought-prompting)) |
| 1.5 | Latte: Latent Diffusion Transformer for Video Generation ([:x:](https://arxiv.org/abs/2401.03048)), ([:book:](https://browse.arxiv.org/pdf/2401.03048.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.03048.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.03048)), ([:house:](https://huggingface.co/papers/2401.03048)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/latte-latent-diffusion-transformer-for-video)) |
| 1.5 | From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models ([:x:](https://arxiv.org/abs/2401.02777)), ([:book:](https://browse.arxiv.org/pdf/2401.02777.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02777.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02777)), ([:house:](https://huggingface.co/papers/2401.02777)), ([HTML](https://browse.arxiv.org/html/2401.02777v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/from-llm-to-conversational-agent-a-memory)) |
| 1.5 | DeepSeek LLM: Scaling Open-Source Language Models with Longtermism ([:x:](https://arxiv.org/abs/2401.02954)), ([:book:](https://browse.arxiv.org/pdf/2401.02954.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02954.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02954)), ([:house:](https://huggingface.co/papers/2401.02954)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/deepseek-llm-scaling-open-source-language)) |
| 1.5 | AST-T5: Structure-Aware Pretraining for Code Generation and Understanding ([:x:](https://arxiv.org/abs/2401.03003)), ([:book:](https://browse.arxiv.org/pdf/2401.03003.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.03003.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.03003)), ([:house:](https://huggingface.co/papers/2401.03003)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ast-t5-structure-aware-pretraining-for-code)), ([:octocat:](https://github.com/gonglinyuan/ast_t5)![GitHub Repo stars](https://img.shields.io/github/stars/gonglinyuan/ast_t5?style=social))  |
| 1.5 | Thousands of AI Authors on the Future of AI ([:x:](https://arxiv.org/abs/2401.02843)), ([:book:](https://browse.arxiv.org/pdf/2401.02843.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02843.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02843)), ([:house:](https://huggingface.co/papers/2401.02843)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/thousands-of-ai-authors-on-the-future-of-ai)) |
| 1.5 | MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance ([:x:](https://arxiv.org/abs/2401.02906)), ([:book:](https://browse.arxiv.org/pdf/2401.02906.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02906.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02906)), ([:house:](https://huggingface.co/papers/2401.02906)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mllm-protector-ensuring-mllm-s-safety-without)) |
| 1.5 | DocGraphLM: Documental Graph Language Model for Information Extraction ([:x:](https://arxiv.org/abs/2401.02823)), ([:book:](https://browse.arxiv.org/pdf/2401.02823.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02823.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02823)), ([:house:](https://huggingface.co/papers/2401.02823)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/docgraphlm-documental-graph-language-model)) |
| 1.5 | Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer Level Loss ([:x:](https://arxiv.org/abs/2401.02677)), ([:book:](https://browse.arxiv.org/pdf/2401.02677.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02677.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02677)), ([:house:](https://huggingface.co/papers/2401.02677)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/progressive-knowledge-distillation-of-stable)) |
| 1.5 | Pheme: Efficient and Conversational Speech Generation ([:x:](https://arxiv.org/abs/2401.02839)), ([:book:](https://browse.arxiv.org/pdf/2401.02839.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02839.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02839)), ([:house:](https://huggingface.co/papers/2401.02839)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pheme-efficient-and-conversational-speech)) |
| 1.5 | Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache ([:x:](https://arxiv.org/abs/2401.02669)), ([:book:](https://browse.arxiv.org/pdf/2401.02669.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02669.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02669)), ([:house:](https://huggingface.co/papers/2401.02669)), ([:eight_spoked_asterisk:]()) |
| 1.5 | The GPT store will launch next week ([blog](https://community.openai.com/t/the-gpt-store-will-launch-next-week/578337/1)) | 
| 1.5 | TinyLlama: An Open-Source Small Language Model ([:x:](https://arxiv.org/abs/2401.02385)), ([:book:](https://browse.arxiv.org/pdf/2401.02385.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02385.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02385)), ([:house:](https://huggingface.co/papers/2401.02385)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tinyllama-an-open-source-small-language-model)), ([:octocat:](https://github.com/jzhang38/tinyllama)![GitHub Repo stars](https://img.shields.io/github/stars/jzhang38/tinyllama?style=social)) |
| 1.5 | LLaMA Pro: Progressive LLaMA with Block Expansion ([:x:](https://arxiv.org/abs/2401.02415)), ([:book:](https://browse.arxiv.org/pdf/2401.02415.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02415.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02415)), ([:house:](https://huggingface.co/papers/2401.02415)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llama-pro-progressive-llama-with-block)), ([:octocat:](https://github.com/tencentarc/llama-pro)![GitHub Repo stars](https://img.shields.io/github/stars/tencentarc/llama-pro?style=social))  |
| 1.5 | What You See is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs ([:x:](https://arxiv.org/abs/2401.02411)), ([:book:](https://browse.arxiv.org/pdf/2401.02411.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02411.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02411)), ([:house:](https://huggingface.co/papers/2401.02411)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/what-you-see-is-what-you-gan-rendering-every)) |
| 1.5 | Learning the 3D Fauna of the Web ([:x:](https://arxiv.org/abs/2401.02400)), ([:book:](https://browse.arxiv.org/pdf/2401.02400.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02400.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02400)), ([:house:](https://huggingface.co/papers/2401.02400)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/learning-the-3d-fauna-of-the-web)) |
| 1.5 | ODIN: A Single Model for 2D and 3D Perception ([:x:](https://arxiv.org/abs/2401.02416)), ([:book:](https://browse.arxiv.org/pdf/2401.02416.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02416.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02416)), ([:house:](https://huggingface.co/papers/2401.02416)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/odin-a-single-model-for-2d-and-3d-perception)) |
| 1.5 | LLaVA-φ: Efficient Multi-Modal Assistant with Small Language Model ([:x:](https://arxiv.org/abs/2401.02330)), ([:book:](https://browse.arxiv.org/pdf/2401.02330.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02330.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02330)), ([:house:](https://huggingface.co/papers/2401.02330)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llava-ph-efficient-multi-modal-assistant-with)) |
| 1.4 | Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM ([:x:](https://arxiv.org/abs/2401.02994)), ([:book:](https://browse.arxiv.org/pdf/2401.02994.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02994.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02994)), ([:house:](https://huggingface.co/papers/2401.02994)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/blending-is-all-you-need-cheaper-better)) |
| 1.4 | Microsoft adding new PC button in its first significant keyboard change in decades (CNBC [news](https://www.cnbc.com/2024/01/05/microsoft-openai-sued-over-copyright-infringement-by-authors.html)) |
| 1.4 | LLM Augmented LLMs: Expanding Capabilities through Composition ([:x:](https://arxiv.org/abs/2401.02412)), ([:book:](https://browse.arxiv.org/pdf/2401.02412.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02412.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02412)), ([:house:](https://huggingface.co/papers/2401.02412)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llm-augmented-llms-expanding-capabilities)) |
| 1.4 | Correctness Comparison of ChatGPT-4, Bard, Claude-2, and Copilot for Spatial Tasks ([:x:](https://arxiv.org/abs/2401.02404)), ([:book:](https://browse.arxiv.org/pdf/2401.02404.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02404.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02404)), ([:house:](https://huggingface.co/papers/2401.02404)), ([:eight_spoked_asterisk:]()) |
| 1.4 | Understanding LLMs: A Comprehensive Overview from Training to Inference ([:x:](https://arxiv.org/abs/2401.02038)), ([:book:](https://browse.arxiv.org/pdf/2401.02038.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02038.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02038)), ([:house:](https://huggingface.co/papers/2401.02038)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/understanding-llms-a-comprehensive-overview)) |
| 1.4 | Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation ([:x:](https://arxiv.org/abs/2401.02117)), ([:book:](https://browse.arxiv.org/pdf/2401.02117.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02117.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02117)), ([:house:](https://huggingface.co/papers/2401.02117)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mobile-aloha-learning-bimanual-mobile)) |
| 1.4 | Instruct-Imagen: Image Generation with Multi-modal Instruction ([:x:](https://arxiv.org/abs/2401.01952)), ([:book:](https://browse.arxiv.org/pdf/2401.01952.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01952.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01952)), ([:house:](https://huggingface.co/papers/2401.01952)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/instruct-imagen-image-generation-with-multi)) |
| 1.4 | ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers ([:x:](https://arxiv.org/abs/2401.02072)), ([:book:](https://browse.arxiv.org/pdf/2401.02072.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02072.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02072)), ([:house:](https://huggingface.co/papers/2401.02072)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ice-grt-instruction-context-enhancement-by)) |
| 1.4 | Improving Diffusion-Based Image Synthesis with Context Prediction ([:x:](https://arxiv.org/abs/2401.02015)), ([:book:](https://browse.arxiv.org/pdf/2401.02015.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02015.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02015)), ([:house:](https://huggingface.co/papers/2401.02015)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/improving-diffusion-based-image-synthesis-1)) |
| 1.4 | Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers ([:x:](https://arxiv.org/abs/2401.01974)), ([:book:](https://browse.arxiv.org/pdf/2401.01974.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01974.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01974)), ([:house:](https://huggingface.co/papers/2401.01974)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-truly-zero-shot-compositional-visual)) |
| 1.4 | FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding ([:x:](https://arxiv.org/abs/2401.01970)), ([:book:](https://browse.arxiv.org/pdf/2401.01970.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01970.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01970)), ([:house:](https://huggingface.co/papers/2401.01970)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fmgs-foundation-model-embedded-3d-gaussian)) |
| 1.4 | aMUSEd: An Open MUSE Reproduction ([:x:](https://arxiv.org/abs/2401.01808)), ([:book:](https://browse.arxiv.org/pdf/2401.01808.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01808.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01808)), ([:house:](https://huggingface.co/papers/2401.01808)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/amused-an-open-muse-reproduction)) |
| 1.4 | From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations ([:x:](https://arxiv.org/abs/2401.01885)), ([:book:](https://browse.arxiv.org/pdf/2401.01885.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01885.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01885)), ([:house:](https://huggingface.co/papers/2401.01885)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/from-audio-to-photoreal-embodiment)), ([:octocat:](https://github.com/facebookresearch/audio2photoreal)![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/audio2photoreal?style=social)) |
| 1.4 | Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions ([:x:](https://arxiv.org/abs/2401.01827)), ([:book:](https://browse.arxiv.org/pdf/2401.01827.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01827.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01827)), ([:house:](https://huggingface.co/papers/2401.01827)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/moonshot-towards-controllable-video)), ([:octocat:](https://github.com/salesforce/lavis)![GitHub Repo stars](https://img.shields.io/github/stars/salesforce/lavis?style=social))  |
| 1.4 | CoMoSVC: Consistency Model-based Singing Voice Conversion ([:x:](https://arxiv.org/abs/2401.01792)), ([:book:](https://browse.arxiv.org/pdf/2401.01792.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01792.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01792)), ([:house:](https://huggingface.co/papers/2401.01792)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/comosvc-consistency-model-based-singing-voice)) |
| 1.4 | A Vision Check-up for Language Models ([:x:](https://arxiv.org/abs/2401.01862)), ([:book:](https://browse.arxiv.org/pdf/2401.01862.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01862.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01862)), ([:house:](https://huggingface.co/papers/2401.01862)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-vision-check-up-for-language-models)) |
| 1.4 | Multilingual Instruction Tuning With Just a Pinch of Multilinguality ([:x:](https://arxiv.org/abs/2401.01854)), ([:book:](https://browse.arxiv.org/pdf/2401.01854.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01854.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01854)), ([:house:](https://huggingface.co/papers/2401.01854)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/multilingual-instruction-tuning-with-just-a)) |
| 1.3 | Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review ([:x:](https://arxiv.org/abs/2401.01519)), ([:book:](https://browse.arxiv.org/pdf/2401.01519.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01519.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01519)), ([:house:](https://huggingface.co/papers/2401.01519)), ([HTML](https://browse.arxiv.org/html/2401.01519v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/exploring-the-frontiers-of-llms-in)) |
| 1.3 | Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach ([:x:](https://arxiv.org/abs/2401.02987)), ([:book:](https://browse.arxiv.org/pdf/2401.02987.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.02987.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.02987)), ([:house:](https://huggingface.co/papers/2401.02987)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/has-your-pretrained-model-improved-a-multi)) |
| 1.3 | GPT-4V(ision) is a Generalist Web Agent, if Grounded ([:x:](https://arxiv.org/abs/2401.01614)), ([:book:](https://browse.arxiv.org/pdf/2401.01614.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01614.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01614)), ([:house:](https://huggingface.co/papers/2401.01614)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gpt-4v-ision-is-a-generalist-web-agent-if)) |
| 1.3 | Image Sculpting: Precise Object Editing with 3D Geometry Control ([:x:](https://arxiv.org/abs/2401.01702)), ([:book:](https://browse.arxiv.org/pdf/2401.01702.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01702.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01702)), ([:house:](https://huggingface.co/papers/2401.01702)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/image-sculpting-precise-object-editing-with)) |
| 1.3 | SIGNeRF: Scene Integrated Generation for Neural Radiance Fields ([:x:](https://arxiv.org/abs/2401.01647)), ([:book:](https://browse.arxiv.org/pdf/2401.01647.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01647.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01647)), ([:house:](https://huggingface.co/papers/2401.01647)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/signerf-scene-integrated-generation-for)) |
| 1.3 | Incremental FastPitch: Chunk-based High Quality Text to Speech ([:x:](https://arxiv.org/abs/2401.01755)), ([:book:](https://browse.arxiv.org/pdf/2401.01755.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01755.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01755)), ([:house:](https://huggingface.co/papers/2401.01755)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/incremental-fastpitch-chunk-based-high)) |
| 1.3 | Efficient Hybrid Zoom using Camera Fusion on Mobile Phones ([:x:](https://arxiv.org/abs/2401.01461)), ([:book:](https://browse.arxiv.org/pdf/2401.01461.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01461.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01461)), ([:house:](https://huggingface.co/papers/2401.01461)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/efficient-hybrid-zoom-using-camera-fusion-on)) |
| 1.3 | WordArt Designer API: User-Driven Artistic Typography Synthesis with Large Language Models on ModelScope ([:x:](https://arxiv.org/abs/2401.01699)), ([:book:](https://browse.arxiv.org/pdf/2401.01699.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01699.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01699)), ([:house:](https://huggingface.co/papers/2401.01699)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/wordart-designer-api-user-driven-artistic)) |
| 1.3 | Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models ([:x:](https://arxiv.org/abs/2401.01335)), ([:book:](https://browse.arxiv.org/pdf/2401.01335.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01335.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01335)), ([:house:](https://huggingface.co/papers/2401.01335)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/self-play-fine-tuning-converts-weak-language)) |
| 1.3 | LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning ([:x:](https://arxiv.org/abs/2401.01325)), ([:book:](https://browse.arxiv.org/pdf/2401.01325.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01325.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01325)), ([:house:](https://huggingface.co/papers/2401.01325)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llm-maybe-longlm-self-extend-llm-context)) |
| 1.3 | VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM ([:x:](https://arxiv.org/abs/2401.01256)), ([:book:](https://browse.arxiv.org/pdf/2401.01256.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01256.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01256)), ([:house:](https://huggingface.co/papers/2401.01256)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/videodrafter-content-consistent-multi-scene)) |
| 1.3 | A Comprehensive Study of Knowledge Editing for Large Language Models ([:x:](https://arxiv.org/abs/2401.01286)), ([:book:](https://browse.arxiv.org/pdf/2401.01286.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01286.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01286)), ([:house:](https://huggingface.co/papers/2401.01286)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-comprehensive-study-of-knowledge-editing)), ([:octocat:](https://github.com/zjunlp/easyedit)![GitHub Repo stars](https://img.shields.io/github/stars/zjunlp/easyedit?style=social))  |
| 1.3 | Few-shot Adaptation of Multi-modal Foundation Models: A Survey ([:x:](https://arxiv.org/abs/2401.01736)), ([:book:](https://browse.arxiv.org/pdf/2401.01736.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01736.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01736)), ([:house:](https://huggingface.co/papers/2401.01736)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/few-shot-adaptation-of-multi-modal-foundation)) |
| 1.3 | Can AI Be as Creative as Humans? ([:x:](https://arxiv.org/abs/2401.15504)), ([:book:](https://browse.arxiv.org/pdf/2401.01623.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01623.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01623)), ([:house:](https://huggingface.co/papers/2401.01623)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/can-ai-be-as-creative-as-humans)) |
| 1.3 | Enhancing the medical foundation model with multi-scale and cross-modality feature learning ([:x:](https://arxiv.org/abs/2401.01583)), ([:book:](https://browse.arxiv.org/pdf/2401.01583.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01583.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01583)), ([:house:](https://huggingface.co/papers/2401.01583)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/enhancing-the-medical-foundation-model-with)) |
| 1.2 | Diagnostic Accuracy of a Large Language Model in Pediatric Case Studies (JAMA [doi:10.1001/jamapediatrics.2023.5750](https://jamanetwork.com/journals/jamapediatrics/article-abstract/2813283)) |
| 1.2 | An Overarching Framework for the Ethics of Artificial Intelligence in Pediatrics (JAMA [doi:10.1001/jamapediatrics.2023.5761](https://jamanetwork.com/journals/jamapediatrics/article-abstract/2813281)) |
| 1.2 | A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models ([:x:](https://arxiv.org/abs/2401.01313)), ([:book:](https://browse.arxiv.org/pdf/2401.01313.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01313.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01313)), ([:house:](https://huggingface.co/papers/2401.01313)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-comprehensive-survey-of-hallucination)) |
| 1.2 | LLaMA Beyond English: An Empirical Study on Language Capability Transfer  ([:x:](https://arxiv.org/abs/2401.01055)), ([:book:](https://browse.arxiv.org/pdf/2401.01055.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01055.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01055)), ([:house:](https://huggingface.co/papers/2401.01055)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llama-beyond-english-an-empirical-study-on)) |
| 1.2 | Boundary Attention: Learning to Find Faint Boundaries at Any Resolution ([:x:](https://arxiv.org/abs/2401.00935)), ([:book:](https://browse.arxiv.org/pdf/2401.00935.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00935.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00935)), ([:house:](https://huggingface.co/papers/2401.15504)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/boundary-attention-learning-to-find-faint)) |
| 1.2 | Q-Refine: A Perceptual Quality Refiner for AI-Generated Image ([:x:](https://arxiv.org/abs/2401.01117)), ([:book:](https://browse.arxiv.org/pdf/2401.01117.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01117.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01117)), ([:house:](https://huggingface.co/papers/2401.01117)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/q-refine-a-perceptual-quality-refiner-for-ai)) |
| 1.2 | En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D Synthetic Data ([:x:](https://arxiv.org/abs/2401.01173)), ([:book:](https://browse.arxiv.org/pdf/2401.01173.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.01173.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.01173)), ([:house:](https://huggingface.co/papers/2401.01173)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/en3d-an-enhanced-generative-model-for)) |
| 1.2 | Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models ([:x:](https://arxiv.org/abs/2401.00788)), ([:book:](https://browse.arxiv.org/pdf/2401.00788.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00788.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00788)), ([:house:](https://huggingface.co/papers/2401.00788)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/astraios-parameter-efficient-instruction)), ([:octocat:](https://github.com/bigcode-project/astraios)![GitHub Repo stars](https://img.shields.io/github/stars/bigcode-project/astraios?style=social))  |
| 1.2 | COSMO: COntrastive Streamlined MultimOdal Model with Interleaved Pre-Training ([:x:](https://arxiv.org/abs/2401.00849)), ([:book:](https://browse.arxiv.org/pdf/2401.00849.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00849.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00849)), ([:house:](https://huggingface.co/papers/2401.00849)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/cosmo-contrastive-streamlined-multimodal)) |
| 1.1 | A Computational Framework for Behavioral Assessment of LLM Therapists ([:x:](https://arxiv.org/abs/2401.00820)), ([:book:](https://browse.arxiv.org/pdf/2401.00820.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00820.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00820)), ([:house:](https://huggingface.co/papers/2401.00820)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-computational-framework-for-behavioral)), ([:octocat:](https://github.com/behavioral-data/bolt)![GitHub Repo stars](https://img.shields.io/github/stars/behavioral-data/bolt?style=social))  |
| 1.1 | DocLLM: A layout-aware generative language model for multimodal document understanding ([:x:](https://arxiv.org/abs/2401.00908)), ([:book:](https://browse.arxiv.org/pdf/2401.00908.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00908.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00908)), ([:house:](https://huggingface.co/papers/2401.00908)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/docllm-a-layout-aware-generative-language)) |
| 1.1 | Taming Mode Collapse in Score Distillation for Text-to-3D Generation ([:x:](https://arxiv.org/abs/2401.00909)), ([:book:](https://browse.arxiv.org/pdf/2401.00909.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00909.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00909)), ([:house:](https://huggingface.co/papers/2401.00909)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/taming-mode-collapse-in-score-distillation)) |
| 1.1 | The Earth is Flat? Unveiling Factual Errors in Large Language Models ([:x:](https://arxiv.org/abs/2401.00761)), ([:book:](https://browse.arxiv.org/pdf/2401.00761.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00761.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00761)), ([:house:](https://huggingface.co/papers/2401.00761)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-earth-is-flat-unveiling-factual-errors-in)) |
| 1.1 | General-purpose foundation models for increased autonomy in robot-assisted surgery ([:x:](https://arxiv.org/abs/2401.00678)), ([:book:](https://browse.arxiv.org/pdf/2401.00678.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00678.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00678)), ([:house:](https://huggingface.co/papers/2401.00678)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/general-purpose-foundation-models-for)) |
| 1.1 | PROMPT-IML: Image Manipulation Localization with Pre-trained Foundation Models Through Prompt Tuning ([:x:](https://arxiv.org/abs/2401.00653)), ([:book:](https://browse.arxiv.org/pdf/2401.00653.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00653.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00653)), ([:house:](https://huggingface.co/papers/2401.00653)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/prompt-iml-image-manipulation-localization)) |
| 1.1 | Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models ([:x:](https://arxiv.org/abs/2401.00625)), ([:book:](https://browse.arxiv.org/pdf/2401.00625.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00625.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00625)), ([:house:](https://huggingface.co/papers/2401.00625)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/beyond-efficiency-a-systematic-survey-of)) |
| 1.1 | SteinDreamer: Variance Reduction for Text-to-3D Score Distillation via Stein Identity ([:x:](https://arxiv.org/abs/2401.00604)), ([:book:](https://browse.arxiv.org/pdf/2401.00604.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00604.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00604)), ([:house:](https://huggingface.co/papers/2401.00604)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/steindreamer-variance-reduction-for-text-to)) |
| 2024.01.01 | | 
| 12.31 | Opening A Pandora's Box: Things You Should Know in the Era of Custom GPTs ([:x:](https://arxiv.org/abs/2401.00905)), ([:book:](https://browse.arxiv.org/pdf/2401.00905.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00905.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00905)), ([:house:](https://huggingface.co/papers/2401.00905)), ([:eight_spoked_asterisk:]()) |
| 12.31 | State of Open Source AI Book - 2023 Edition ([book](https://book.premai.io/state-of-open-source-ai/)) |
| 12.31 | TrailBlazer: Trajectory Control for Diffusion-Based Video Generation ([:x:](https://arxiv.org/abs/2401.00896)), ([:book:](https://browse.arxiv.org/pdf/2401.00896.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00896.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00896)), ([:house:](https://huggingface.co/papers/2401.00896)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/trailblazer-trajectory-control-for-diffusion)) |
| 12.31 | Improving Text Embeddings with Large Language Models ([:x:](https://arxiv.org/abs/2401.00368)), ([:book:](https://browse.arxiv.org/pdf/2401.00368.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00368.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00368)), ([:house:](https://huggingface.co/papers/2401.00368)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/improving-text-embeddings-with-large-language)) |
| 12.31 | Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws ([:x:](https://arxiv.org/abs/2401.00448)), ([:book:](https://browse.arxiv.org/pdf/2401.00448.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00448.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00448)), ([:house:](https://huggingface.co/papers/2401.00448)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/beyond-chinchilla-optimal-accounting-for)) |
| 12.31 | GraphGPT: Graph Learning with Generative Pre-trained Transformers ([:x:](https://arxiv.org/abs/2401.00529)), ([:book:](https://browse.arxiv.org/pdf/2401.00529.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00529.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00529)), ([:house:](https://huggingface.co/papers/2401.00529)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/graphgpt-graph-learning-with-generative-pre)) |
| 12.31 | Brain-Conditional Multimodal Synthesis: A Survey and Taxonomy ([:x:](https://arxiv.org/abs/2401.00430)), ([:book:](https://browse.arxiv.org/pdf/2401.00430.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00430.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00430)), ([:house:](https://huggingface.co/papers/2401.00430)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/brain-conditional-multimodal-synthesis-a)) |
| 12.31 | GeoGalactica: A Scientific Large Language Model in Geoscience ([:x:](https://arxiv.org/abs/2401.00434)), ([:book:](https://browse.arxiv.org/pdf/2401.00434.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00434.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00434)), ([:house:](https://huggingface.co/papers/2401.00434)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/geogalactica-a-scientific-large-language)), ([:octocat:](https://github.com/geobrain-ai/geogalactica)![GitHub Repo stars](https://img.shields.io/github/stars/geobrain-ai/geogalactica?style=social))  |
| 12.30 | Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models ([:x:](https://arxiv.org/abs/2312.17661)), ([:book:](https://browse.arxiv.org/pdf/2312.17661.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.17661.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.17661)), ([:house:](https://huggingface.co/papers/2312.17661)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gemini-in-reasoning-unveiling-commonsense-in)) |
| 12.30 | Boosting Large Language Model for Speech Synthesis: An Empirical Study ([:x:](https://arxiv.org/abs/2401.00246)), ([:book:](https://browse.arxiv.org/pdf/2401.00246.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00246.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00246)), ([:house:](https://huggingface.co/papers/2401.00246)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/boosting-large-language-model-for-speech)) |
| 12.30 | Unicron: Economizing Self-Healing LLM Training at Scale ([:x:](https://arxiv.org/abs/2401.00134)), ([:book:](https://browse.arxiv.org/pdf/2401.00134.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00134.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00134)), ([:house:](https://huggingface.co/papers/2401.00134)), ([:eight_spoked_asterisk:]()) |
| 12.30 | Autonomous Threat Hunting: A Future Paradigm for AI-Driven Threat Intelligence ([:x:](https://arxiv.org/abs/2401.00286)), ([:book:](https://browse.arxiv.org/pdf/2401.00286.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00286.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00286)), ([:house:](https://huggingface.co/papers/2401.00286)), ([:eight_spoked_asterisk:]()) |
| 12.30 | Promoting Segment Anything Model towards Highly Accurate Dichotomous Image Segmentation ([:x:](https://arxiv.org/abs/2401.00248)), ([:book:](https://browse.arxiv.org/pdf/2401.00248.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00248.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00248)), ([:house:](https://huggingface.co/papers/2401.00248)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/promoting-segment-anything-model-towards)) |
| 12.30 | USFM: A Universal Ultrasound Foundation Model Generalized to Tasks and Organs towards Label Efficient Image Analysis ([:x:](https://arxiv.org/abs/2401.00153)), ([:book:](https://browse.arxiv.org/pdf/2401.00153.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00153.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00153)), ([:house:](https://huggingface.co/papers/2401.00153)), ([:eight_spoked_asterisk:]()) |
| 12.30 | Pushing Boundaries: Exploring Zero Shot Object Classification with Large Multimodal Models ([:x:](https://arxiv.org/abs/2401.00127)), ([:book:](https://browse.arxiv.org/pdf/2401.00127.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00127.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00127)), ([:house:](https://huggingface.co/papers/2401.00127)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pushing-boundaries-exploring-zero-shot-object)) |
| 12.30 | FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis ([:x:](https://arxiv.org/abs/2312.17681)), ([:book:](https://browse.arxiv.org/pdf/2312.17681.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.17681.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.17681)), ([:house:](https://huggingface.co/papers/2312.17681)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/flowvid-taming-imperfect-optical-flows-for)) |
| 12.29 | A foundation model for atomistic materials chemistry ([:x:](https://arxiv.org/abs/2401.00096)), ([:book:](https://browse.arxiv.org/pdf/2401.00096.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00096.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00096)), ([:house:](https://huggingface.co/papers/2401.00096)), ([:eight_spoked_asterisk:](https://physics.paperswithcode.com/paper/a-foundation-model-for-atomistic-materials)), ([:octocat:](https://github.com/ACEsuit/mace)![GitHub Repo stars](https://img.shields.io/github/stars/ACEsuit/mace?style=social))  |
| 12.29 | Self-supervised Pretraining for Decision Foundation Model: Formulation, Pipeline and Challenges ([:x:](https://arxiv.org/abs/2401.00031)), ([:book:](https://browse.arxiv.org/pdf/2401.00031.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.00031.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.00031)), ([:house:](https://huggingface.co/papers/2401.00031)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/self-supervised-pretraining-for-decision)) |
| 12.29 | EHR Interaction Between Patients and AI: NoteAid EHR Interaction ([:x:](https://arxiv.org/abs/2312.17475)), ([:book:](https://browse.arxiv.org/pdf/2312.17475.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.17475.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.17475)), ([:house:](https://huggingface.co/papers/2312.17475)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ehr-interaction-between-patients-and-ai)) |
| 12.29 | Learning Vision from Models Rivals Learning Vision from Data ([:x:](https://arxiv.org/abs/2312.17742)), ([:book:](https://browse.arxiv.org/pdf/2312.17742.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.17742.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.17742)), ([:house:](https://huggingface.co/papers/2312.17742)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/learning-vision-from-models-rivals-learning)), ([:octocat:](https://github.com/google-research/syn-rep-learn)![GitHub Repo stars](https://img.shields.io/github/stars/google-research/syn-rep-learn?style=social))  |
| 12.29 | Generative AI for Math: Part I -- MathPile: A Billion-Token-Scale Pretraining Corpus for Math ([:x:](https://arxiv.org/abs/2312.17120)), ([:book:](https://browse.arxiv.org/pdf/2312.17120.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.17120.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.17120)), ([:house:](https://huggingface.co/papers/2312.17120)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/generative-ai-for-math-part-i-mathpile-a)), ([:octocat:](https://github.com/gair-nlp/mathpile)![GitHub Repo stars](https://img.shields.io/github/stars/gair-nlp/mathpile?style=social))  |
| 12.29 | Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action ([:x:](https://arxiv.org/abs/2312.17172)), ([:book:](https://browse.arxiv.org/pdf/2312.17172.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.17172.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.17172)), ([:house:](https://huggingface.co/papers/2312.17172)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/unified-io-2-scaling-autoregressive)) |
| 12.29 | Unsupervised Universal Image Segmentation ([:x:](https://arxiv.org/abs/2312.17243)), ([:book:](https://browse.arxiv.org/pdf/2312.17243.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.17243.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.17243)), ([:house:](https://huggingface.co/papers/2312.17243)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/unsupervised-universal-image-segmentation)), ([:octocat:](https://github.com/u2seg/u2seg)![GitHub Repo stars](https://img.shields.io/github/stars/u2seg/u2seg?style=social))  |
| 12.29 | DreamGaussian4D: Generative 4D Gaussian Splatting ([:x:](https://arxiv.org/abs/2312.17142)), ([:book:](https://browse.arxiv.org/pdf/2312.17142.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.17142.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.17142)), ([:house:](https://huggingface.co/papers/2312.17142)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dreamgaussian4d-generative-4d-gaussian)), ([:octocat:](https://github.com/jiawei-ren/dreamgaussian4d)![GitHub Repo stars](https://img.shields.io/github/stars/jiawei-ren/dreamgaussian4d?style=social))  |
| 12.29 | InsActor: Instruction-driven Physics-based Characters ([:x:](https://arxiv.org/abs/2312.17135)), ([:book:](https://browse.arxiv.org/pdf/2312.17135.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.17135.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.17135)), ([:house:](https://huggingface.co/papers/2312.17135)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/insactor-instruction-driven-physics-based-1)) |
| 12.29 | The LLM Surgeon ([:x:](https://arxiv.org/abs/2312.17244)), ([:book:](https://browse.arxiv.org/pdf/2312.17244.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.17244.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.17244)), ([:house:](https://huggingface.co/papers/2312.17244)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-llm-surgeon)) |
| 12.29 | Compact Neural Graphics Primitives with Learned Hash Probing ([:x:](https://arxiv.org/abs/2312.17241)), ([:book:](https://browse.arxiv.org/pdf/2312.17241.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.17241.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.17241)), ([:house:](https://huggingface.co/papers/2312.17241)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/compact-neural-graphics-primitives-with)) |
| 12.29 | Restoration by Generation with Constrained Priors ([:x:](https://arxiv.org/abs/2312.17161)), ([:book:](https://browse.arxiv.org/pdf/2312.17161.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.17161.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.17161)), ([:house:](https://huggingface.co/papers/2312.17161)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/restoration-by-generation-with-constrained)) |
| 12.28 | Fast Inference of Mixture-of-Experts Language Models with Offloading ([:x:](https://arxiv.org/abs/2312.17238)), ([:book:](https://browse.arxiv.org/pdf/2312.17238.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.17238.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.17238)), ([:house:](https://huggingface.co/papers/2312.17238)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fast-inference-of-mixture-of-experts-language)), ([:octocat:](https://github.com/dvmazur/mixtral-offloading)![GitHub Repo stars](https://img.shields.io/github/stars/dvmazur/mixtral-offloading?style=social)) |
| 12.28 | MobileVLM : A Fast, Reproducible and Strong Vision Language Assistant for Mobile Devices ([:x:](https://arxiv.org/abs/2312.16886)), ([:book:](https://browse.arxiv.org/pdf/2312.16886.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.16886.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.16886)), ([:house:](https://huggingface.co/papers/2312.16886)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mobilevlm-a-fast-reproducible-and-strong)), ([:octocat:](https://github.com/meituan-automl/mobilevlm)![GitHub Repo stars](https://img.shields.io/github/stars/meituan-automl/mobilevlm?style=social))  |
| 12.28 | I2V-Adapter: A General Image-to-Video Adapter for Video Diffusion Models ([:x:](https://arxiv.org/abs/2312.16693)), ([:book:](https://browse.arxiv.org/pdf/2312.16693.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.16693.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.16693)), ([:house:](https://huggingface.co/papers/2312.16693)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/i2v-adapter-a-general-image-to-video-adapter)) |
| 12.28 | Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis ([:x:](https://arxiv.org/abs/2312.16812)), ([:book:](https://browse.arxiv.org/pdf/2312.16812.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.16812.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.16812)), ([:house:](https://huggingface.co/papers/2312.16812)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/spacetime-gaussian-feature-splatting-for-real)) |
| 12.28 | DiffusionGAN3D: Boosting Text-guided 3D Generation and Domain Adaption by Combining 3D GANs and Diffusion Priors ([:x:](https://arxiv.org/abs/2312.16837)), ([:book:](https://browse.arxiv.org/pdf/2312.16837.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.16837.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.16837)), ([:house:](https://huggingface.co/papers/2312.16837)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/diffusiongan3d-boosting-text-guided-3d)) |
| 12.28 | Prompt Expansion for Adaptive Text-to-Image Generation ([:x:](https://arxiv.org/abs/2312.16720)), ([:book:](https://browse.arxiv.org/pdf/2312.16720.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.16720.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.16720)), ([:house:](https://huggingface.co/papers/2312.16720)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/prompt-expansion-for-adaptive-text-to-image)) |
| 12.28 | TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones ([:x:](https://arxiv.org/abs/2312.16862)), ([:book:](https://browse.arxiv.org/pdf/2312.16862.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.16862.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.16862)), ([:house:](https://huggingface.co/papers/2312.16862)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tinygpt-v-efficient-multimodal-large-language)), ([:octocat:](https://github.com/dlyuangod/tinygpt-v)![GitHub Repo stars](https://img.shields.io/github/stars/dlyuangod/tinygpt-v?style=social))  |
| 12.28 | Segment3D: Learning Fine-Grained Class-Agnostic 3D Segmentation without Manual Labels ([:x:](https://arxiv.org/abs/2312.17232)), ([:book:](https://browse.arxiv.org/pdf/2312.17232.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.17232.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.17232)), ([:house:](https://huggingface.co/papers/2312.17232)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/segment3d-learning-fine-grained-class)) |
| 12.28 | Challenge LLMs to Reason About Reasoning: A Benchmark to Unveil Cognitive Depth in LLMs ([:x:](https://arxiv.org/abs/2312.17080)), ([:book:](https://browse.arxiv.org/pdf/2312.17080.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.17080.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.17080)), ([:house:](https://huggingface.co/papers/2312.17080)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/challenge-llms-to-reason-about-reasoning-a)) |
| 12.27 | New York Times sues Microsoft and OpenAI for 'billions' (BBC [news](https://www.bbc.com/news/technology-67826601)) |
| 12.27 | PanGu-π: Enhancing Language Model Architectures via Nonlinearity Compensation ([:x:](https://arxiv.org/abs/2312.17276)), ([:book:](https://browse.arxiv.org/pdf/2312.17276.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.17276.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.17276)), ([:house:](https://huggingface.co/papers/2312.17276)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pangu-p-enhancing-language-model)) |
| 12.27 | City-on-Web: Real-time Neural Rendering of Large-scale Scenes on the Web ([:x:](https://arxiv.org/abs/2312.16457)), ([:book:](https://browse.arxiv.org/pdf/2312.16457.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.16457.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.16457)), ([:house:](https://huggingface.co/papers/2312.16457)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/city-on-web-real-time-neural-rendering-of)) |
| 12.27 | PanGu-Draw: Advancing Resource-Efficient Text-to-Image Synthesis with Time-Decoupled Training and Reusable Coop-Diffusion ([:x:](https://arxiv.org/abs/2312.16486)), ([:book:](https://browse.arxiv.org/pdf/2312.16486.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.16486.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.16486)), ([:house:](https://huggingface.co/papers/2312.16486)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pangu-draw-advancing-resource-efficient-text)) |
| 12.27 | Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4 ([:x:](https://arxiv.org/abs/2312.16171)), ([:book:](https://browse.arxiv.org/pdf/2312.16171.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.16171.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.16171)), ([:house:](https://huggingface.co/papers/2312.16171)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/principled-instructions-are-all-you-need-for)), ([:octocat:](https://github.com/vila-lab/atlas)![GitHub Repo stars](https://img.shields.io/github/stars/vila-lab/atlas?style=social))  |
| 12.27 | LangSplat: 3D Language Gaussian Splatting ([:x:](https://arxiv.org/abs/2312.16084)), ([:book:](https://browse.arxiv.org/pdf/2312.16084.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.16084.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.16084)), ([:house:](https://huggingface.co/papers/2312.16084)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/langsplat-3d-language-gaussian-splatting)) |
| 12.27 | One-dimensional Adapter to Rule Them All: Concepts, Diffusion Models and Erasing Applications ([:x:](https://arxiv.org/abs/2312.16145)), ([:book:](https://browse.arxiv.org/pdf/2312.16145.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.16145.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.16145)), ([:house:](https://huggingface.co/papers/2312.16145)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/one-dimensional-adapter-to-rule-them-all)) |
| 12.26 | DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision ([:x:](https://arxiv.org/abs/2312.16256)), ([:book:](https://browse.arxiv.org/pdf/2312.16256.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.16256.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.16256)), ([:house:](https://huggingface.co/papers/2312.16256)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dl3dv-10k-a-large-scale-scene-dataset-for)) |
| 12.26 | SSR-Encoder: Encoding Selective Subject Representation for Subject-Driven Generation ([:x:](https://arxiv.org/abs/2312.16272)), ([:book:](https://browse.arxiv.org/pdf/2312.16272.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.16272.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.16272)), ([:house:](https://huggingface.co/papers/2312.16272)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ssr-encoder-encoding-selective-subject)) |
| 12.26 | Audiobox: Unified Audio Generation with Natural Language Prompts ([:x:](https://arxiv.org/abs/2312.15821)), ([:book:](https://browse.arxiv.org/pdf/2312.15821.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.15821.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.15821)), ([:house:](https://huggingface.co/papers/2312.15821)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/audiobox-unified-audio-generation-with)) |
| 12.26 | A Recipe for Scaling up Text-to-Video Generation with Text-free Videos ([:x:](https://arxiv.org/abs/2312.15770)), ([:book:](https://browse.arxiv.org/pdf/2312.15770.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.15770.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.15770)), ([:house:](https://huggingface.co/papers/2312.15770)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-recipe-for-scaling-up-text-to-video)) |
| 12.26 | HarmonyView: Harmonizing Consistency and Diversity in One-Image-to-3D ([:x:](https://arxiv.org/abs/2312.15980)), ([:book:](https://browse.arxiv.org/pdf/2312.15980.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.15980.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.15980)), ([:house:](https://huggingface.co/papers/2312.15980)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/harmonyview-harmonizing-consistency-and)) |
| 12.26 | Supervised Knowledge Makes Large Language Models Better In-context Learners ([:x:](https://arxiv.org/abs/2312.15918)), ([:book:](https://browse.arxiv.org/pdf/2312.15918.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.15918.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.15918)), ([:house:](https://huggingface.co/papers/2312.15918)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/supervised-knowledge-makes-large-language)) |
| 12.25 | UniRef++: Segment Every Reference Object in Spatial and Temporal Spaces ([:x:](https://arxiv.org/abs/2312.15715)), ([:book:](https://browse.arxiv.org/pdf/2312.15715.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.15715.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.15715)), ([:house:](https://huggingface.co/papers/2312.15715)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/uniref-segment-every-reference-object-in)), ([:octocat:](https://github.com/foundationvision/uniref)![GitHub Repo stars](https://img.shields.io/github/stars/foundationvision/uniref?style=social))  |
| 12.24 | Hyper-VolTran: Fast and Generalizable One-Shot Image to 3D Object Structure via HyperNetworks ([:x:](https://arxiv.org/abs/2312.16218)), ([:book:](https://browse.arxiv.org/pdf/2312.16218.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.16218.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.16218)), ([:house:](https://huggingface.co/papers/2312.16218)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hyper-voltran-fast-and-generalizable-one-shot)) |
| 12.24 | ChatGPT 3.5 fails to write appropriate multiple choice practice exam questions ([paper](https://www.sciencedirect.com/science/article/pii/S2374289523000313)), ([PDF](https://www.sciencedirect.com/science/article/pii/S2374289523000313/pdfft?md5=a10c566ba8cc29483c2eb3deca716a25&pid=1-s2.0-S2374289523000313-main.pdf)) |
| 12.24 | LARP: Language-Agent Role Play for Open-World Games ([:x:](https://arxiv.org/abs/2312.17653)), ([:book:](https://browse.arxiv.org/pdf/2312.17653.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.17653.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.17653)), ([:house:](https://huggingface.co/papers/2312.17653)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/larp-language-agent-role-play-for-open-world)) |
| 12.24 | Make-A-Character: High Quality Text-to-3D Character Generation within Minutes ([:x:](https://arxiv.org/abs/2312.15430)), ([:book:](https://browse.arxiv.org/pdf/2312.15430.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.15430.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.15430)), ([:house:](https://huggingface.co/papers/2312.15430)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/make-a-character-high-quality-text-to-3d)) |
| 12.23 | SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling ([:x:](https://arxiv.org/abs/2312.15166)), ([:book:](https://browse.arxiv.org/pdf/2312.15166.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.15166.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.15166)), ([:house:](https://huggingface.co/papers/2312.15166)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/solar-10-7b-scaling-large-language-models)) |
| 12.23 | Gemini vs GPT-4V: A Preliminary Comparison and Combination of Vision-Language Models Through Qualitative Cases ([:x:](https://arxiv.org/abs/2312.15011)), ([:book:](https://browse.arxiv.org/pdf/2312.15011.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.15011.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.15011)), ([:house:](https://huggingface.co/papers/2312.15011)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gemini-vs-gpt-4v-a-preliminary-comparison-and)), ([:octocat:](https://github.com/qi-zhangyang/gemini-vs-gpt4v)![GitHub Repo stars](https://img.shields.io/github/stars/qi-zhangyang/gemini-vs-gpt4v?style=social))  |
| 12.23 | Human101: Training 100+FPS Human Gaussians in 100s from 1 View ([:x:](https://arxiv.org/abs/2312.15258)), ([:book:](https://browse.arxiv.org/pdf/2312.15258.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.15258.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.15258)), ([:house:](https://huggingface.co/papers/2312.15258)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/human101-training-100-fps-human-gaussians-in)), ([:octocat:](https://github.com/longxiang-ai/human101)![GitHub Repo stars](https://img.shields.io/github/stars/longxiang-ai/human101?style=social))  |
| 12.23 | On the Promises and Challenges of Multimodal Foundation Models for Geographical, Environmental, Agricultural, and Urban Planning Applications ([:x:](https://arxiv.org/abs/2312.17016)), ([:book:](https://browse.arxiv.org/pdf/2312.17016.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.17016.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.17016)), ([:house:](https://huggingface.co/papers/2312.17016)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/on-the-promises-and-challenges-of-multimodal)) |
| 12.22 | Awesome LLM Interpretability - A curated list of Large Language Model (LLM) Interpretability resources ([:octocat:](https://github.com/JShollaj/awesome-llm-interpretability)![GitHub Repo stars](https://img.shields.io/github/stars/JShollaj/awesome-llm-interpretability?style=social)) 
| 12.22 | HeadCraft: Modeling High-Detail Shape Variations for Animated 3DMMs ([:x:](https://arxiv.org/abs/2312.14140)), ([:book:](https://browse.arxiv.org/pdf/2312.14140.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.14140.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.14140)), ([:house:](https://huggingface.co/papers/2312.14140)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/headcraft-modeling-high-detail-shape)) |
| 12.22 | Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion Models with RL Finetuning ([:x:](https://arxiv.org/abs/2312.13980)), ([:book:](https://browse.arxiv.org/pdf/2312.13980.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.13980.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.13980)), ([:house:](https://huggingface.co/papers/2312.13980)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/carve3d-improving-multi-view-reconstruction)) |
| 12.22 | Paint3D: Paint Anything 3D with Lighting-Less Texture Diffusion Models ([:x:](https://arxiv.org/abs/2312.13913)), ([:book:](https://browse.arxiv.org/pdf/2312.13913.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.13913.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.13913)), ([:house:](https://huggingface.co/papers/2312.13913)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/paint3d-paint-anything-3d-with-lighting-less)), ([:octocat:](https://github.com/opentexture/paint3d)![GitHub Repo stars](https://img.shields.io/github/stars/opentexture/paint3d?style=social))  |
| 12.22 | HD-Painter: High-Resolution and Prompt-Faithful Text-Guided Image Inpainting with Diffusion Models ([:x:](https://arxiv.org/abs/2312.14091)), ([:book:](https://browse.arxiv.org/pdf/2312.14091.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.14091.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.14091)), ([:house:](https://huggingface.co/papers/2312.14091)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hd-painter-high-resolution-and-prompt)) |
| 12.22 | PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models ([:x:](https://arxiv.org/abs/2312.13964)), ([:book:](https://browse.arxiv.org/pdf/2312.13964.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.13964.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.13964)), ([:house:](https://huggingface.co/papers/2312.13964)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pia-your-personalized-image-animator-via-plug)) |
| 12.22 | VideoPoet: A Large Language Model for Zero-Shot Video Generation ([:x:](https://arxiv.org/abs/2312.14125)), ([:book:](https://browse.arxiv.org/pdf/2312.14125.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.14125.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.14125)), ([:house:](https://huggingface.co/papers/2312.14125)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/videopoet-a-large-language-model-for-zero)) |
| 12.21 | Exploring the intersection of Generative AI and Software Development ([:x:](https://arxiv.org/abs/2312.14262)), ([:book:](https://browse.arxiv.org/pdf/2312.14262.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.14262.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.14262)), ([:house:](https://huggingface.co/papers/2312.14262)), ([HTML](https://browse.arxiv.org/html/2412.14262v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/exploring-the-intersection-of-generative-ai)) |
| 12.21 | Generative Multimodal Models are In-Context Learners ([:x:](https://arxiv.org/abs/2312.13286)), ([:book:](https://browse.arxiv.org/pdf/2312.13286.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.13286.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.13286)), ([:house:](https://huggingface.co/papers/2312.13286)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/generative-multimodal-models-are-in-context)), ([:octocat:](https://github.com/baaivision/emu)![GitHub Repo stars](https://img.shields.io/github/stars/baaivision/emu?style=social))  |
| 12.21 | Zero-Shot Metric Depth with a Field-of-View Conditioned Diffusion Model ([:x:](https://arxiv.org/abs/2312.13252)), ([:book:](https://browse.arxiv.org/pdf/2312.13252.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.13252.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.13252)), ([:house:](https://huggingface.co/papers/2312.13252)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/zero-shot-metric-depth-with-a-field-of-view)) |
| 12.21 | Splatter Image: Ultra-Fast Single-View 3D Reconstruction ([:x:](https://arxiv.org/abs/2312.13150)), ([:book:](https://browse.arxiv.org/pdf/2312.13150.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.13150.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.13150)), ([:house:](https://huggingface.co/papers/2312.13150)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/splatter-image-ultra-fast-single-view-3d)), ([:octocat:](https://github.com/szymanowiczs/splatter-image)![GitHub Repo stars](https://img.shields.io/github/stars/szymanowiczs/splatter-image?style=social))  |
| 12.21 | UniSDF: Unifying Neural Representations for High-Fidelity 3D Reconstruction of Complex Scenes with Reflections ([:x:](https://arxiv.org/abs/2312.13285)), ([:book:](https://browse.arxiv.org/pdf/2312.13285.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.13285.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.13285)), ([:house:](https://huggingface.co/papers/2312.13285)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/unisdf-unifying-neural-representations-for)) |
| 12.21 | SpecNeRF: Gaussian Directional Encoding for Specular Reflections ([:x:](https://arxiv.org/abs/2312.13102)), ([:book:](https://browse.arxiv.org/pdf/2312.13102.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.13102.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.13102)), ([:house:](https://huggingface.co/papers/2312.13102)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/specnerf-gaussian-directional-encoding-for)) |
| 12.21 | Repaint123: Fast and High-quality One Image to 3D Generation with Progressive Controllable 2D Repainting ([:x:](https://arxiv.org/abs/2312.13271)), ([:book:](https://browse.arxiv.org/pdf/2312.13271.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.15504.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.13271)), ([:house:](https://huggingface.co/papers/2312.13271)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/repaint123-fast-and-high-quality-one-image-to)), ([:octocat:](https://github.com/junwuzhang19/repaint123)![GitHub Repo stars](https://img.shields.io/github/stars/junwuzhang19/repaint123?style=social))  |
| 12.21 | Neural feels with neural fields: Visuo-tactile perception for in-hand manipulation ([:x:](https://arxiv.org/abs/2312.13469)), ([:book:](https://browse.arxiv.org/pdf/2312.13469.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.13469.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.13469)), ([:house:](https://huggingface.co/papers/2312.13469)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/neural-feels-with-neural-fields-visuo-tactile)) |
| 12.21 | DreamTuner: Single Image is Enough for Subject-Driven Generation ([:x:](https://arxiv.org/abs/2312.13691)), ([:book:](https://browse.arxiv.org/pdf/2312.13691.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.13691.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.13691)), ([:house:](https://huggingface.co/papers/2312.13691)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dreamtuner-single-image-is-enough-for-subject)) |
| 12.21 | TinySAM: Pushing the Envelope for Efficient Segment Anything Model ([:x:](https://arxiv.org/abs/2312.13789)), ([:book:](https://browse.arxiv.org/pdf/2312.13789.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.13789.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.13789)), ([:house:](https://huggingface.co/papers/2312.13789)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tinysam-pushing-the-envelope-for-efficient)), ([:octocat:](https://github.com/xinghaochen/tinysam)![GitHub Repo stars](https://img.shields.io/github/stars/xinghaochen/tinysam?style=social))  |
| 12.21 | Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models ([:x:](https://arxiv.org/abs/2312.13763)), ([:book:](https://browse.arxiv.org/pdf/2312.13763.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.13763.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.13763)), ([:house:](https://huggingface.co/papers/2312.13763)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/align-your-gaussians-text-to-4d-with-dynamic)) |
| 12.21 | ShowRoom3D: Text to High-Quality 3D Room Generation Using 3D Priors ([:x:](https://arxiv.org/abs/2312.13324)), ([:book:](https://browse.arxiv.org/pdf/2312.13324.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.13324.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.13324)), ([:house:](https://huggingface.co/papers/2312.13324)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/showroom3d-text-to-high-quality-3d-room)) |
| 12.21 | AppAgent: Multimodal Agents as Smartphone Users ([:x:](https://arxiv.org/abs/2312.13771)), ([:book:](https://browse.arxiv.org/pdf/2312.13771.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.13771.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.13771)), ([:house:](https://huggingface.co/papers/2312.13771)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/appagent-multimodal-agents-as-smartphone)), ([SS](https://www.semanticscholar.org/paper/AppAgent%3A-Multimodal-Agents-as-Smartphone-Users-Yang-Liu/53cb57c8ca1ce950b0588d481ae399296acb8f5b)) |
| 12.21 | DREAM-Talk: Diffusion-based Realistic Emotional Audio-driven Method for Single Image Talking Face Generation ([:x:](https://arxiv.org/abs/2312.13578)), ([:book:](https://browse.arxiv.org/pdf/2312.13578.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.13578.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.13578)), ([:house:](https://huggingface.co/papers/2312.13578)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dream-talk-diffusion-based-realistic)) |
| 12.21 | Time is Encoded in the Weights of Finetuned Language Models ([:x:](https://arxiv.org/abs/2312.13401)), ([:book:](https://browse.arxiv.org/pdf/2312.13401.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.13401.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.13401)), ([:house:](https://huggingface.co/papers/2312.13401)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/time-is-encoded-in-the-weights-of-finetuned)) |
| 12.21 | The alpha version of Midjourney V6 is open for testing ([tweet](https://twitter.com/ciguleva/status/1737719062418112542)) |
| 12.21 | InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks ([:x:](https://arxiv.org/abs/2312.14238)), ([:book:](https://browse.arxiv.org/pdf/2312.14238.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.14238.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.14238)), ([:house:](https://huggingface.co/papers/2312.14238)), ([HTML](https://browse.arxiv.org/html/2412.14238v3)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/internvl-scaling-up-vision-foundation-models)), ([:octocat:](https://github.com/opengvlab/internvl)![GitHub Repo stars](https://img.shields.io/github/stars/opengvlab/internvl?style=social))  |
| 12.20 | RadEdit: stress-testing biomedical vision models via diffusion image editing ([:x:](https://arxiv.org/abs/2312.12865)), ([:book:](https://browse.arxiv.org/pdf/2312.12865.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.12865.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.12865)), ([:house:](https://huggingface.co/papers/2312.12865)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/radedit-stress-testing-biomedical-vision)) |
| 12.20 | Autonomous chemical research with large language models (Nature [https://doi.org/10.1038/s41586-023-06792-0](https://www.nature.com/articles/s41586-023-06792-0)) |
| 12.20 | Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model ([:x:](https://arxiv.org/abs/2312.12423)), ([:book:](https://browse.arxiv.org/pdf/2312.12423.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.12423.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.12423)), ([:house:](https://huggingface.co/papers/2312.12423)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/jack-of-all-tasks-master-of-many-designing)) |
| 12.20 | InstructVideo: Instructing Video Diffusion Models with Human Feedback ([:x:](https://arxiv.org/abs/2312.12490)), ([:book:](https://browse.arxiv.org/pdf/2312.12490.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.12490.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.12490)), ([:house:](https://huggingface.co/papers/2312.12490)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/instructvideo-instructing-video-diffusion)) |
| 12.20 | Cached Transformers: Improving Transformers with Differentiable Memory Cache ([:x:](https://arxiv.org/abs/2312.12742)), ([:book:](https://browse.arxiv.org/pdf/2312.12742.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.12742.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.12742)), ([:house:](https://huggingface.co/papers/2312.12742)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/cached-transformers-improving-transformers)) |
| 12.20 | Adaptive Guidance: Training-free Acceleration of Conditional Diffusion Models ([:x:](https://arxiv.org/abs/2312.12487)), ([:book:](https://browse.arxiv.org/pdf/2312.12487.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.12487.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.12487)), ([:house:](https://huggingface.co/papers/2312.12487)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/adaptive-guidance-training-free-acceleration)) |
| 12.20 | Mini-GPTs: Efficient Large Language Models through Contextual Pruning ([:x:](https://arxiv.org/abs/2312.12682)), ([:book:](https://browse.arxiv.org/pdf/2312.12682.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.12682.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.12682)), ([:house:](https://huggingface.co/papers/2312.12682)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mini-gpts-efficient-large-language-models)) |
| 12.20 | Model-Based Control with Sparse Neural Dynamics ([:x:](https://arxiv.org/abs/2312.12791)), ([:book:](https://browse.arxiv.org/pdf/2312.12791.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.12791.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.12791)), ([:house:](https://huggingface.co/papers/2312.12791)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/model-based-control-with-sparse-neural-1)) |
| 12.20 | RadEdit: stress-testing biomedical vision models via diffusion image editing ([:x:](https://arxiv.org/abs/2312.12865)), ([:book:](https://browse.arxiv.org/pdf/2312.12865.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.12865.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.12865)), ([:house:](https://huggingface.co/papers/2312.12865)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/radedit-stress-testing-biomedical-vision)) |
| 12.20 | StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation ([:x:](https://arxiv.org/abs/2312.12491)), ([:book:](https://browse.arxiv.org/pdf/2312.12491.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.12491.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.12491)), ([:house:](https://huggingface.co/papers/2312.12491)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/streamdiffusion-a-pipeline-level-solution-for)), ([:octocat:](https://github.com/cumulo-autumn/streamdiffusion)![GitHub Repo stars](https://img.shields.io/github/stars/cumulo-autumn/streamdiffusion?style=social))  |
| 12.20 | Unlocking Pre-trained Image Backbones for Semantic Image Synthesis ([:x:](https://arxiv.org/abs/2312.13314)), ([:book:](https://browse.arxiv.org/pdf/2312.13314.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.13314.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.13314)), ([:house:](https://huggingface.co/papers/2312.13314)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/unlocking-pre-trained-image-backbones-for)) |
| 12.20 | Fairy: Fast Parallelized Instruction-Guided Video-to-Video Synthesis ([:x:](https://arxiv.org/abs/2312.13834)), ([:book:](https://browse.arxiv.org/pdf/2312.13834.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.13834.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.13834)), ([:house:](https://huggingface.co/papers/2312.13834)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fairy-fast-parallelized-instruction-guided)) |
| 12.20 | Stable Video Diffusion Now Available on Stability AI Developer Platform API ([blog](https://stability.ai/news/introducing-stable-video-diffusion-api)) |
| 12.20 | How to Use ChatGPT to Set Transformative Goals for 2024Use ChatGPT to simplify -- and enhance -- your goal-setting proces ([news](https://www.inc.com/suzanne-lucas/how-to-use-chatgpt-to-set-transformative-goals-for-2024.html)) |
| 12.20 | A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise ([:x:](https://arxiv.org/abs/2312.12436)), ([:book:](https://browse.arxiv.org/pdf/2312.12436.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.12436.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.12436)), ([:house:](https://huggingface.co/papers/2312.12436)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-challenger-to-gpt-4v-early-explorations-of)), ([SS](https://www.semanticscholar.org/paper/A-Challenger-to-GPT-4V-Early-Explorations-of-Gemini-Fu-Zhang/9bd4ce663905d5153c90fd999d56e4b808f3fc57)) |
| 12.20 | Tracking Any Object Amodally ([:x:](https://arxiv.org/abs/2312.12433)), ([:book:](https://browse.arxiv.org/pdf/2312.12433.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.12433.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.12433)), ([:house:](https://huggingface.co/papers/2312.12433)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tracking-any-object-amodally)), ([:octocat:](https://github.com/WesleyHsieh0806/TAO-Amodal)![GitHub Repo stars](https://img.shields.io/github/stars/WesleyHsieh0806/TAO-Amodal?style=social))  |
| 12.19 | Efficient LLM inference solution on Intel GPU ([:x:](https://arxiv.org/abs/2401.05391)), ([:book:](https://browse.arxiv.org/pdf/2401.05391.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.05391.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.05391)), ([:house:](https://huggingface.co/papers/2401.05391)), ([HTML](https://browse.arxiv.org/html/2401.05391v1)), ([:eight_spoked_asterisk:]()) |
| 12.19 | 3D-LFM: Lifting Foundation Model ([:x:](https://arxiv.org/abs/2312.11894)), ([:book:](https://browse.arxiv.org/pdf/2312.11894.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.11894.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.11894)), ([:house:](https://huggingface.co/papers/2312.11894)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/3d-lfm-lifting-foundation-model)), ([:octocat:](https://github.com/mosamdabhi/3dlfm)![GitHub Repo stars](https://img.shields.io/github/stars/mosamdabhi/3dlfm?style=social))  |
| 12.19 | HAAR: Text-Conditioned Generative Model of 3D Strand-based Human Hairstyles ([:x:](https://arxiv.org/abs/2312.11666)), ([:book:](https://browse.arxiv.org/pdf/2312.11666.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.11666.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.11666)), ([:house:](https://huggingface.co/papers/2312.11666)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/haar-text-conditioned-generative-model-of-3d)) |
| 12.19 | MixRT: Mixed Neural Representations For Real-Time NeRF Rendering ([:x:](https://arxiv.org/abs/2312.11841)), ([:book:](https://browse.arxiv.org/pdf/2312.11841.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.11841.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.11841)), ([:house:](https://huggingface.co/papers/2312.11841)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mixrt-mixed-neural-representations-for-real)) |
| 12.19 | Text-Conditioned Resampler For Long Form Video Understanding ([:x:](https://arxiv.org/abs/2312.11897)), ([:book:](https://browse.arxiv.org/pdf/2312.11897.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.11897.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.11897)), ([:house:](https://huggingface.co/papers/2312.11897)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/text-conditioned-resampler-for-long-form)) |
| 12.19 | TIP: Text-Driven Image Processing with Semantic and Restoration Instructions ([:x:](https://arxiv.org/abs/2312.11595)), ([:book:](https://browse.arxiv.org/pdf/2312.11595.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.11595.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.11595)), ([:house:](https://huggingface.co/papers/2312.11595)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tip-text-driven-image-processing-with)) |
| 12.19 | SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing ([:x:](https://arxiv.org/abs/2312.11392)), ([:book:](https://browse.arxiv.org/pdf/2312.11392.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.11392.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.11392)), ([:house:](https://huggingface.co/papers/2312.11392)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/scedit-efficient-and-controllable-image)) |
| 12.19 | G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model ([:x:](https://arxiv.org/abs/2312.11370)), ([:book:](https://browse.arxiv.org/pdf/2312.11370.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.11370.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.11370)), ([:house:](https://huggingface.co/papers/2312.11370)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/g-llava-solving-geometric-problem-with-multi)), ([:octocat:](https://github.com/pipilurj/g-llava)![GitHub Repo stars](https://img.shields.io/github/stars/pipilurj/g-llava?style=social))  |
| 12.19 | GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning ([:x:](https://arxiv.org/abs/2312.11461)), ([:book:](https://browse.arxiv.org/pdf/2312.11461.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.11461.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.11461)), ([:house:](https://huggingface.co/papers/2312.11461)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gavatar-animatable-3d-gaussian-avatars-with)) |
| 12.19 | MAG-Edit: Localized Image Editing in Complex Scenarios via Mask-Based Attention-Adjusted Guidance ([:x:](https://arxiv.org/abs/2312.11396)), ([:book:](https://browse.arxiv.org/pdf/2312.11396.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.11396.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.11396)), ([:house:](https://huggingface.co/papers/2312.11396)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mag-edit-localized-image-editing-in-complex)) |
| 12.19 | Cascade Speculative Drafting for Even Faster LLM Inference ([:x:](https://arxiv.org/abs/2312.11462)), ([:book:](https://browse.arxiv.org/pdf/2312.11462.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.11462.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.11462)), ([:house:](https://huggingface.co/papers/2312.11462)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/cascade-speculative-drafting-for-even-faster)) |
| 12.19 | VolumeDiffusion: Flexible Text-to-3D Generation with Efficient Volumetric Encoder ([:x:](https://arxiv.org/abs/2312.11459)), ([:book:](https://browse.arxiv.org/pdf/2312.11459.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.11459.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.11459)), ([:house:](https://huggingface.co/papers/2312.11459)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/volumediffusion-flexible-text-to-3d)), ([:octocat:](https://github.com/tzco/volumediffusion)![GitHub Repo stars](https://img.shields.io/github/stars/tzco/volumediffusion?style=social))  |
| 12.19 | MaskINT: Video Editing via Interpolative Non-autoregressive Masked Transformers ([:x:](https://arxiv.org/abs/2312.12468)), ([:book:](https://browse.arxiv.org/pdf/2312.12468.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.12468.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.12468)), ([:house:](https://huggingface.co/papers/2312.12468)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/maskint-video-editing-via-interpolative-non)) |
| 12.19 | Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment ([:x:](https://arxiv.org/abs/2312.12148)), ([:book:](https://browse.arxiv.org/pdf/2312.12148.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.12148.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.12148)), ([:house:](https://huggingface.co/papers/2312.12148)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/parameter-efficient-fine-tuning-methods-for)), ([SS](https://www.semanticscholar.org/paper/Parameter-Efficient-Fine-Tuning-Methods-for-Models%3A-Xu-Xie/83de13bd492b9e72c314e308f0d77014154a6a74)) |
| 12.19 | Designing Guiding Principles for NLP for Healthcare: A Case Study of Maternal Health ([:x:](https://arxiv.org/abs/2312.11803)), ([:book:](https://browse.arxiv.org/pdf/2312.11803.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.11803.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.11803)), ([:house:](https://huggingface.co/papers/2312.11803)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/designing-guiding-principles-for-nlp-for)), ([SS](https://www.semanticscholar.org/paper/Designing-Guiding-Principles-for-NLP-for-A-Case-of-Antoniak-Naik/54a8b99357ee491ed2a2da2fef3d65983f97cb96)) |
| 12.19 | These scientists aren’t using ChatGPT — here’s why (Nature [doi: https://doi.org/10.1038/d41586-023-04071-6](https://www.nature.com/articles/d41586-023-04071-6)) |
| 12.19 | Gemini: A Family of Highly Capable Multimodal Models ([:x:](https://arxiv.org/abs/2312.11805)), ([:book:](https://browse.arxiv.org/pdf/2312.11805.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.11805.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.11805)), ([:house:](https://huggingface.co/papers/2312.11805)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gemini-a-family-of-highly-capable-multimodal-1)), ([SS](https://www.semanticscholar.org/paper/Gemini%3A-A-Family-of-Highly-Capable-Multimodal-Anil-Borgeaud/f79114e2572f095265179831d19de2eb54174415)) |
| 12.19 | Urban Generative Intelligence (UGI): A Foundational Platform for Agents in Embodied City Environment ([:x:](https://arxiv.org/abs/2312.11813)), ([:book:](https://browse.arxiv.org/pdf/2312.11813.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.11813.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.11813)), ([:house:](https://huggingface.co/papers/2312.11813)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/urban-generative-intelligence-ugi-a)) |
| 12.18 | A Comprehensive Survey of Attack Techniques, Implementation, and Mitigation Strategies in Large Language Models ([:x:](https://arxiv.org/abs/2312.10982)), ([:book:](https://browse.arxiv.org/pdf/2312.10982.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10982.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10982)), ([:house:](https://huggingface.co/papers/2312.10982)), ([:eight_spoked_asterisk:]()), ([SS](https://www.semanticscholar.org/paper/A-Comprehensive-Survey-of-Attack-Techniques%2C-and-in-Esmradi-Yip/952b2e2aadeec56ee4cffbe23f9f70bcfcb14b53)) |
| 12.18 | Retrieval-Augmented Generation for Large Language Models: A Survey ([:x:](https://arxiv.org/abs/2312.10997)), ([:book:](https://browse.arxiv.org/pdf/2312.10997.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10997.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10997)), ([:house:](https://huggingface.co/papers/2312.10997)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/retrieval-augmented-generation-for-large)), ([:octocat:](https://github.com/tongji-kgllm/rag-survey)![GitHub Repo stars](https://img.shields.io/github/stars/tongji-kgllm/rag-survey?style=social)), ([SS](https://www.semanticscholar.org/paper/Retrieval-Augmented-Generation-for-Large-Language-A-Gao-Xiong/46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5))  |
| 12.18 | 2023, year of open LLMs ([blog](https://huggingface.co/blog/2023-in-llms)) |
| 12.18 | From Google Gemini to OpenAI Q* (Q-Star): A Survey of Reshaping the Generative Artificial Intelligence (AI) Research Landscape ([:x:](https://arxiv.org/abs/2312.10868)), ([:book:](https://browse.arxiv.org/pdf/2312.10868.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10868.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10868)), ([:house:](https://huggingface.co/papers/2312.10868)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/from-google-gemini-to-openai-q-q-star-a)), ([SS](https://www.semanticscholar.org/paper/From-Google-Gemini-to-OpenAI-Q*-(Q-Star)%3A-A-Survey-McIntosh-Susnjak/a1abf4d8bad5694621e4d8cd09e41c80cdbba318)) |
| 12.18 | OpenAI Preparedness ([blog](https://openai.com/safety/preparedness)) - ([framework](https://cdn.openai.com/openai-preparedness-framework-beta.pdf)) |
| 12.18 | An In-depth Look at Gemini's Language Abilities ([:x:](https://arxiv.org/abs/2312.11444)), ([:book:](https://browse.arxiv.org/pdf/2312.11444.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.11444.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.11444)), ([:house:](https://huggingface.co/papers/2312.11444)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/an-in-depth-look-at-gemini-s-language)), ([SS](https://www.semanticscholar.org/paper/An-In-depth-Look-at-Gemini's-Language-Abilities-Akter-Yu/b9ea4ea333eeb8875c3e23637a8dce99038be95c)) | 
| 12.18 | M3DBench: Let's Instruct Large Models with Multi-modal 3D Prompts ([:x:](https://arxiv.org/abs/2312.10763)), ([:book:](https://browse.arxiv.org/pdf/2312.10763.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10763.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10763)), ([:house:](https://huggingface.co/papers/2312.10763)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/m3dbench-let-s-instruct-large-models-with)), ([:octocat:](https://github.com/OpenM3D/M3DBench)![GitHub Repo stars](https://img.shields.io/github/stars/OpenM3D/M3DBench?style=social))  |
| 12.18 | MagicScroll: Nontypical Aspect-Ratio Image Generation for Visual Storytelling via Multi-Layered Semantic-Aware Denoising ([:x:](https://arxiv.org/abs/2312.10899)), ([:book:](https://browse.arxiv.org/pdf/2312.10899.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10899.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10899)), ([:house:](https://huggingface.co/papers/2312.10899)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/magicscroll-nontypical-aspect-ratio-image)) |
| 12.18 | Your Student is Better Than Expected: Adaptive Teacher-Student Collaboration for Text-Conditional Diffusion Models ([:x:](https://arxiv.org/abs/2312.10835)), ([:book:](https://browse.arxiv.org/pdf/2312.10835.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10835.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10835)), ([:house:](https://huggingface.co/papers/2312.10835)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/your-student-is-better-than-expected-adaptive)), ([:octocat:](https://github.com/yandex-research/adaptive-diffusion)![GitHub Repo stars](https://img.shields.io/github/stars/yandex-research/adaptive-diffusion?style=social))  |
| 12.17 | VecFusion: Vector Font Generation with Diffusion ([:x:](https://arxiv.org/abs/2312.10540)), ([:book:](https://browse.arxiv.org/pdf/2312.10540.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10540.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10540)), ([:house:](https://huggingface.co/papers/2312.10540)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vecfusion-vector-font-generation-with)) |
| 12.17 | Silkie: Preference Distillation for Large Visual Language Models ([:x:](https://arxiv.org/abs/2312.10665)), ([:book:](https://browse.arxiv.org/pdf/2312.10665.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10665.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10665)), ([:house:](https://huggingface.co/papers/2312.10665)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/silkie-preference-distillation-for-large)) |
| 12.17 | StarVector: Generating Scalable Vector Graphics Code from Images ([:x:](https://arxiv.org/abs/2312.11556)), ([:book:](https://browse.arxiv.org/pdf/2312.11556.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.11556.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.11556)), ([:house:](https://huggingface.co/papers/2312.11556)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/starvector-generating-scalable-vector)) |
| 12.17 | Paloma: A Benchmark for Evaluating Language Model Fit ([:x:](https://arxiv.org/abs/2312.10523)), ([:book:](https://browse.arxiv.org/pdf/2312.10523.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10523.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10523)), ([:house:](https://huggingface.co/papers/2312.10523)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/paloma-a-benchmark-for-evaluating-language)), ([SS](https://www.semanticscholar.org/paper/Paloma%3A-A-Benchmark-for-Evaluating-Language-Model-Magnusson-Bhagia/1a3f7e23ef8f0bf06d0efa0dc174e4e361226ead)) |
| 12.17 | A Survey of Reasoning with Foundation Models ([:x:](https://arxiv.org/abs/2312.11562)), ([:book:](https://browse.arxiv.org/pdf/2312.11562.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.11562.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.11562)), ([:house:](https://huggingface.co/papers/2312.11562)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-survey-of-reasoning-with-foundation-models)), ([:octocat:](https://github.com/reasoning-survey/awesome-reasoning-foundation-models)![GitHub Repo stars](https://img.shields.io/github/stars/reasoning-survey/awesome-reasoning-foundation-models?style=social)), ([SS](https://www.semanticscholar.org/paper/A-Survey-of-Reasoning-with-Foundation-Models-Sun-Zheng/db1f4a2edc2e18908b556f54bee6dc501e7e5108))  |
| 12.17 | VidToMe: Video Token Merging for Zero-Shot Video Editing ([:x:](https://arxiv.org/abs/2312.10656)), ([:book:](https://browse.arxiv.org/pdf/2312.10656.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10656.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10656)), ([:house:](https://huggingface.co/papers/2312.10656)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vidtome-video-token-merging-for-zero-shot)) |
| 12.16 | Rich Human Feedback for Text-to-Image Generation ([:x:](https://arxiv.org/abs/2312.10240)), ([:book:](https://browse.arxiv.org/pdf/2312.10240.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10240.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10240)), ([:house:](https://huggingface.co/papers/2312.10240)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rich-human-feedback-for-text-to-image)) |
| 12.16 | ProTIP: Progressive Tool Retrieval Improves Planning ([:x:](https://arxiv.org/abs/2312.10332)), ([:book:](https://browse.arxiv.org/pdf/2312.10332.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10332.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10332)), ([:house:](https://huggingface.co/papers/2312.10332)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/protip-progressive-tool-retrieval-improves)) |
| 12.16 | Amphion: An Open-Source Audio, Music and Speech Generation Toolkit ([:x:](https://arxiv.org/abs/2312.09911)), ([:book:](https://browse.arxiv.org/pdf/2312.09911.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.09911.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.09911)), ([:house:](https://huggingface.co/papers/2312.09911)), ([:eight_spoked_asterisk:](https://cs.paperswithcode.com/paper/amphion-an-open-source-audio-music-and-speech)), ([:octocat:](https://github.com/open-mmlab/amphion)![GitHub Repo stars](https://img.shields.io/github/stars/open-mmlab/amphion?style=social))  |
| 12.16 | ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent ([:x:](https://arxiv.org/abs/2312.10003)), ([:book:](https://browse.arxiv.org/pdf/2312.10003.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10003.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10003)), ([:house:](https://huggingface.co/papers/2312.10003)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rest-meets-react-self-improvement-for-multi)) |
| 12.16 | Faithful Persona-based Conversational Dataset Generation with Large Language Models ([:x:](https://arxiv.org/abs/2312.10007)), ([:book:](https://browse.arxiv.org/pdf/2312.10007.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10007.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10007)), ([:house:](https://huggingface.co/papers/2312.10007)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/faithful-persona-based-conversational-dataset)), ([:octocat:](https://github.com/google-research-datasets/Synthetic-Persona-Chat)![GitHub Repo stars](https://img.shields.io/github/stars/google-research-datasets/Synthetic-Persona-Chat?style=social))  |
| 12.16 | SlimmeRF: Slimmable Radiance Fields ([:x:](https://arxiv.org/abs/2312.10034)), ([:book:](https://browse.arxiv.org/pdf/2312.10034.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10034.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10034)), ([:house:](https://huggingface.co/papers/2312.10034)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/slimmerf-slimmable-radiance-fields)), ([:octocat:](https://github.com/shiran-yuan/slimmerf)![GitHub Repo stars](https://img.shields.io/github/stars/shiran-yuan/slimmerf?style=social))  |
| 12.16 | Customize-It-3D: High-Quality 3D Creation from A Single Image Using Subject-Specific Knowledge Prior ([:x:](https://arxiv.org/abs/2312.11535)), ([:book:](https://browse.arxiv.org/pdf/2312.11535.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.11535.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.11535)), ([:house:](https://huggingface.co/papers/2312.11535)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/customize-it-3d-high-quality-3d-creation-from)) |
| 12.16 | Topic-VQ-VAE: Leveraging Latent Codebooks for Flexible Topic-Guided Document Generation ([:x:](https://arxiv.org/abs/2312.11532)), ([:book:](https://browse.arxiv.org/pdf/2312.11532.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.11532.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.11532)), ([:house:](https://huggingface.co/papers/2312.11532)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/topic-vq-vae-leveraging-latent-codebooks-for)) |
| 12.16 | FastSR-NeRF: Improving NeRF Efficiency on Consumer Devices with A Simple Super-Resolution Pipeline ([:x:](https://arxiv.org/abs/2312.11537)), ([:book:](https://browse.arxiv.org/pdf/2312.11537.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.11537.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.11537)), ([:house:](https://huggingface.co/papers/2312.11537)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fastsr-nerf-improving-nerf-efficiency-on)) |
| 12.16 | PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU ([:x:](https://arxiv.org/abs/2312.12456)), ([:book:](https://browse.arxiv.org/pdf/2312.12456.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.12456.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.12456)), ([:house:](https://huggingface.co/papers/2312.12456)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/powerinfer-fast-large-language-model-serving)), ([:octocat:](https://github.com/sjtu-ipads/powerinfer)![GitHub Repo stars](https://img.shields.io/github/stars/sjtu-ipads/powerinfer?style=social))  |
| 12.16 | Challenges with unsupervised LLM knowledge discovery ([:x:](https://arxiv.org/abs/2312.10029)), ([:book:](https://browse.arxiv.org/pdf/2312.10029.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10029.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10029)), ([:house:](https://huggingface.co/papers/2312.10029)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/challenges-with-unsupervised-llm-knowledge)) |
| 12.16 | Catwalk: A Unified Language Model Evaluation Framework for Many Datasets ([:x:](https://arxiv.org/abs/2312.10253)), ([:book:](https://browse.arxiv.org/pdf/2312.10253.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10253.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10253)), ([:house:](https://huggingface.co/papers/2312.10253)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/catwalk-a-unified-language-model-evaluation)), ([:octocat:](https://github.com/allenai/catwalk)![GitHub Repo stars](https://img.shields.io/github/stars/allenai/catwalk?style=social)), ([SS](https://www.semanticscholar.org/paper/Catwalk%3A-A-Unified-Language-Model-Evaluation-for-Groeneveld-Awadalla/4318e4ab5e2f5e2a50469aa043fe66c0744370a4))  |
| 12.16 | Point Transformer V3: Simpler, Faster, Stronger ([:x:](https://arxiv.org/abs/2312.10035)), ([:book:](https://browse.arxiv.org/pdf/2312.10035.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10035.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10035)), ([:house:](https://huggingface.co/papers/2312.10035)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/point-transformer-v3-simpler-faster-stronger)), ([:octocat:](https://github.com/pointcept/pointtransformerv3)![GitHub Repo stars](https://img.shields.io/github/stars/pointcept/pointtransformerv3?style=social))  |
| 12.15 | Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision ([:x:](https://arxiv.org/abs/2312.09390)), ([:book:](https://browse.arxiv.org/pdf/2312.09390.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.09390.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.09390)), ([:house:](https://huggingface.co/papers/2312.09390)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/weak-to-strong-generalization-eliciting)) |
| 12.15 | Generative Artificial Intelligence and the Creative Economy Staff Report: Perspectives and Takeaways ([report](https://www.ftc.gov/reports/generative-artificial-intelligence-creative-economy-staff-report-perspectives-takeaways)), ([PDF](https://www.ftc.gov/system/files/ftc_gov/pdf/12-15-2023AICEStaffReport.pdf)) |
| 12.15 | Weight subcloning: direct initialization of transformers using larger pretrained ones ([:x:](https://arxiv.org/abs/2312.09299)), ([:book:](https://browse.arxiv.org/pdf/2312.09299.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.09299.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.09299)), ([:house:](https://huggingface.co/papers/2312.09299)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/weight-subcloning-direct-initialization-of)) |
| 12.15 | Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models ([:x:](https://arxiv.org/abs/2312.09608)), ([:book:](https://browse.arxiv.org/pdf/2312.09608.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.09608.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.09608)), ([:house:](https://huggingface.co/papers/2312.09608)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/faster-diffusion-rethinking-the-role-of-unet)), ([:octocat:](https://github.com/hutaihang/faster-diffusion)![GitHub Repo stars](https://img.shields.io/github/stars/hutaihang/faster-diffusion?style=social))  |
| 12.15 | Self-Evaluation Improves Selective Generation in Large Language Models ([:x:](https://arxiv.org/abs/2312.09300)), ([:book:](https://browse.arxiv.org/pdf/2312.09300.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.09300.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.09300)), ([:house:](https://huggingface.co/papers/2312.09300)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/self-evaluation-improves-selective-generation)) |
| 12.15 | Extending Context Window of Large Language Models via Semantic Compression ([:x:](https://arxiv.org/abs/2312.09571)), ([:book:](https://browse.arxiv.org/pdf/2312.09571.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.09571.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.09571)), ([:house:](https://huggingface.co/papers/2312.09571)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/extending-context-window-of-large-language-1)) |
| 12.15 | Stable Score Distillation for High-Quality 3D Generation ([:x:](https://arxiv.org/abs/2312.09305)), ([:book:](https://browse.arxiv.org/pdf/2312.09305.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.09305.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.09305)), ([:house:](https://huggingface.co/papers/2312.09305)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/stable-score-distillation-for-high-quality-3d)) |
| 12.15 | Towards the Unification of Generative and Discriminative Visual Foundation Model: A Survey ([:x:](https://arxiv.org/abs/2312.10163)), ([:book:](https://browse.arxiv.org/pdf/2312.10163.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10163.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10163)), ([:house:](https://huggingface.co/papers/2312.10163)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-the-unification-of-generative-and)) |
| 12.15 | MobileSAMv2: Faster Segment Anything to Everything ([:x:](https://arxiv.org/abs/2312.09579)), ([:book:](https://browse.arxiv.org/pdf/2312.09579.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.09579.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.09579)), ([:house:](https://huggingface.co/papers/2312.09579)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mobilesamv2-faster-segment-anything-to)), ([:octocat:](https://github.com/chaoningzhang/mobilesam)![GitHub Repo stars](https://img.shields.io/github/stars/chaoningzhang/mobilesam?style=social))  |
| 12.15 | DreamTalk: When Expressive Talking Head Generation Meets Diffusion Probabilistic Models ([:x:](https://arxiv.org/abs/2312.09767)), ([:book:](https://browse.arxiv.org/pdf/2312.09767.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.09767.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.09767)), ([:house:](https://huggingface.co/papers/2312.09767)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dreamtalk-when-expressive-talking-head)) |
| 12.15 | Towards the Unification of Generative and Discriminative Visual Foundation Model: A Survey ([:x:](https://arxiv.org/abs/2312.10163)), ([:book:](https://browse.arxiv.org/pdf/2312.10163.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10163.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10163)), ([:house:](https://huggingface.co/papers/2312.10163)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-the-unification-of-generative-and)) |
| 12.15 | LoRAMoE: Revolutionizing Mixture of Experts for Maintaining World Knowledge in Language Model Alignment ([:x:](https://arxiv.org/abs/2312.09979)), ([:book:](https://browse.arxiv.org/pdf/2312.09979.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.09979.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.09979)), ([:house:](https://huggingface.co/papers/2312.09979)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-art-of-balancing-revolutionizing-mixture)), ([SS](https://www.semanticscholar.org/paper/LoRAMoE%3A-Revolutionizing-Mixture-of-Experts-for-in-Dou-Zhou/5fffc5f56c67740603c68473dea50ce059cbb78f)) |
| 12.15 | TinyGSM: achieving >80% on GSM8k with small language models ([:x:](https://arxiv.org/abs/2312.09241)), ([:book:](https://browse.arxiv.org/pdf/2312.09241.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.09241.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.09241)), ([:house:](https://huggingface.co/papers/2312.09241)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tinygsm-achieving-80-on-gsm8k-with-small)) |
| 12.15 | VideoLCM: Video Latent Consistency Model ([:x:](https://arxiv.org/abs/2312.09109)), ([:book:](https://browse.arxiv.org/pdf/2312.09109.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.09109.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.09109)), ([:house:](https://huggingface.co/papers/2312.09109)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/videolcm-video-latent-consistency-model)) |
| 12.15 | LIME: Localized Image Editing via Attention Regularization in Diffusion Models ([:x:](https://arxiv.org/abs/2312.09256)), ([:book:](https://browse.arxiv.org/pdf/2312.09256.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.09256.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.09256)), ([:house:](https://huggingface.co/papers/2312.09256)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lime-localized-image-editing-via-attention)) |
| 12.15 | Mosaic-SDF for 3D Generative Models ([:x:](https://arxiv.org/abs/2312.09222)), ([:book:](https://browse.arxiv.org/pdf/2312.09222.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.09222.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.09222)), ([:house:](https://huggingface.co/papers/2312.09222)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mosaic-sdf-for-3d-generative-models)) |
| 12.15 | FineControlNet: Fine-level Text Control for Image Generation with Spatially Aligned Text Control Injection ([:x:](https://arxiv.org/abs/2312.09252)), ([:book:](https://browse.arxiv.org/pdf/2312.09252.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.09252.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.09252)), ([:house:](https://huggingface.co/papers/2312.09252)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/finecontrolnet-fine-level-text-control-for)) |
| 12.15 | VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation ([:x:](https://arxiv.org/abs/2312.09251)), ([:book:](https://browse.arxiv.org/pdf/2312.09251.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.09251.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.09251)), ([:house:](https://huggingface.co/papers/2312.09251)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vl-gpt-a-generative-pre-trained-transformer)), ([:octocat:](https://github.com/ailab-cvc/vl-gpt)![GitHub Repo stars](https://img.shields.io/github/stars/ailab-cvc/vl-gpt?style=social))  |
| 12.15 | Pixel Aligned Language Models ([:x:](https://arxiv.org/abs/2312.09237)), ([:book:](https://browse.arxiv.org/pdf/2312.09237.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.09237.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.09237)), ([:house:](https://huggingface.co/papers/2312.09237)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pixel-aligned-language-models)) |
| 12.15 | General Object Foundation Model for Images and Videos at Scale ([:x:](https://arxiv.org/abs/2312.09158)), ([:book:](https://browse.arxiv.org/pdf/2312.09158.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.09158.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.09158)), ([:house:](https://huggingface.co/papers/2312.09158)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/general-object-foundation-model-for-images)), ([:octocat:](https://github.com/FoundationVision/GLEE)![GitHub Repo stars](https://img.shields.io/github/stars/FoundationVision/GLEE?style=social))  |
| 12.15 | Holodeck: Language Guided Generation of 3D Embodied AI Environments ([:x:](https://arxiv.org/abs/2312.09067)), ([:book:](https://browse.arxiv.org/pdf/2312.09067.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.09067.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.09067)), ([:house:](https://huggingface.co/papers/2312.09067)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/holodeck-language-guided-generation-of-3d)) |
| 12.14 | CERN for AGI: A Theoretical Framework for Autonomous Simulation-Based Artificial Intelligence Testing and Alignment ([:x:](https://arxiv.org/abs/2312.09402)), ([:book:](https://browse.arxiv.org/pdf/2312.09402.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.09402.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.09402)), ([:house:](https://huggingface.co/papers/2312.09402)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/cern-for-agi-a-theoretical-framework-for)), ([SS](https://www.semanticscholar.org/paper/CERN-for-AGI%3A-A-Theoretical-Framework-for-Testing-Boji%C4%87-Cinelli/0cef9b572565c93366b3228b0812e9c0069d4f18)) |
| 12.14 | ChatSOS LLM-based knowledge QA system for safety engineering	([:x:](https://arxiv.org/abs/2312.08629)), ([:book:](https://browse.arxiv.org/pdf/2312.08629.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.08629.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.08629)), ([:house:](https://huggingface.co/papers/2312.08629)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chatsos-llm-based-knowledge-q-a-system-for)) |
| 12.14 | Influence of Prompting Strategies on Segment Anything Model (SAM) for Short-axis Cardiac MRI segmentation ([:x:](https://arxiv.org/abs/2312.08932)), ([:book:](https://browse.arxiv.org/pdf/2312.08932.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.08932.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.08932)), ([:house:](https://huggingface.co/papers/2312.08932)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/influence-of-prompting-strategies-on-segment)) |
| 12.14 | ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks ([:x:](https://arxiv.org/abs/2312.08583)), ([:book:](https://browse.arxiv.org/pdf/2312.08583.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.08583.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.08583)), ([:house:](https://huggingface.co/papers/2312.08583)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/zeroquant-4-2-redefining-llms-quantization)), ([:octocat:](https://github.com/microsoft/DeepSpeed)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/DeepSpeed?style=social))  |
| 12.14 | Vision-Language Models as a Source of Rewards ([:x:](https://arxiv.org/abs/2312.09187)), ([:book:](https://browse.arxiv.org/pdf/2312.09187.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.09187.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.09187)), ([:house:](https://huggingface.co/papers/2312.09187)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vision-language-models-as-a-source-of-rewards)) |
| 12.14 | StemGen: A music generation model that listens ([:x:](https://arxiv.org/abs/2312.08723)), ([:book:](https://browse.arxiv.org/pdf/2312.08723.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.08723.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.08723)), ([:house:](https://huggingface.co/papers/2312.08723)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/stemgen-a-music-generation-model-that-listens)) |
| 12.14 | CogAgent: A Visual Language Model for GUI Agents ([:x:](https://arxiv.org/abs/2312.08914)), ([:book:](https://browse.arxiv.org/pdf/2312.08914.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.08914.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.08914)), ([:house:](https://huggingface.co/papers/2312.08914)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/cogagent-a-visual-language-model-for-gui)), ([:octocat:](https://github.com/thudm/cogvlm)![GitHub Repo stars](https://img.shields.io/github/stars/thudm/cogvlm?style=social))  |
| 12.14 | A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style Models on Dense Captions ([:x:](https://arxiv.org/abs/2312.08578)), ([:book:](https://browse.arxiv.org/pdf/2312.08578.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.08578.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.08578)), ([:house:](https://huggingface.co/papers/2312.08578)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-picture-is-worth-more-than-77-text-tokens)), ([:octocat:](https://github.com/facebookresearch/dci)![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/dci?style=social))  |
| 12.14 | Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention ([:x:](https://arxiv.org/abs/2312.08618)), ([:book:](https://browse.arxiv.org/pdf/2312.08618.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.08618.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.08618)), ([:house:](https://huggingface.co/papers/2312.08618)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/zebra-extending-context-window-with-layerwise)) |
| 12.14 | Distributed Inference and Fine-tuning of Large Language Models Over The Internet ([:x:](https://arxiv.org/abs/2312.08361)), ([:book:](https://browse.arxiv.org/pdf/2312.08361.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.08361.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.08361)), ([:house:](https://huggingface.co/papers/2312.08361)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/distributed-inference-and-fine-tuning-of-1)) |
| 12.14 | FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects ([:x:](https://arxiv.org/abs/2312.08344)), ([:book:](https://browse.arxiv.org/pdf/2312.08344.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.08344.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.08344)), ([:house:](https://huggingface.co/papers/2312.08344)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/foundationpose-unified-6d-pose-estimation-and)) |
| 12.14 | TigerBot: An Open Multilingual Multitask LLM ([:x:](https://arxiv.org/abs/2312.08688)), ([:book:](https://browse.arxiv.org/pdf/2312.08688.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.08688.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.08688)), ([:house:](https://huggingface.co/papers/2312.08688)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tigerbot-an-open-multilingual-multitask-llm)) |
| 12.14 | OpenAI - Prompt engineering ([guide](https://platform.openai.com/docs/guides/prompt-engineering)) |
| 12.14 | OpenAI - Weak-to-strong generalization ([blog](https://openai.com/research/weak-to-strong-generalization)) |
| 12.14 | China releases first AI large language model for ancient book research ([news](https://www.ecns.cn/news/sci-tech/2023-12-14/detail-ihcvumcq4268645.shtml)) |
| 12.14 | [Imagen 2](https://deepmind.google/technologies/imagen-2/) on Vertex AI is now generally available (Google [blog](https://cloud.google.com/blog/products/ai-machine-learning/imagen-2-on-vertex-ai-is-now-generally-available?hl=en)) |
| 12.13 | JAMA NETWORK OPEN PUBLISHES CRITERIA FOR MANUSCRIPTS REPORTING CLINICAL USE OF AI ([news](https://advances.massgeneral.org/neuro/journal.aspx?id=2557)) |
| 12.13 | SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained Geometry and Appearance ([:x:](https://arxiv.org/abs/2312.08889)), ([:book:](https://browse.arxiv.org/pdf/2312.08889.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.08889.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.08889)), ([:house:](https://huggingface.co/papers/2312.08889)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/seeavatar-photorealistic-text-to-3d-avatar)) |
| 12.13 | LLM in a flash: Efficient Large Language Model Inference with Limited Memory ([:x:](https://arxiv.org/abs/2312.11514)), ([:book:](https://browse.arxiv.org/pdf/2312.11514.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.11514.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.11514)), ([:house:](https://huggingface.co/papers/2312.11514)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llm-in-a-flash-efficient-large-language-model)) |
| 12.13 | PromptBench: A Unified Library for Evaluation of Large Language Models ([:x:](https://arxiv.org/abs/2312.07910)), ([:book:](https://browse.arxiv.org/pdf/2312.07910.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.07910.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.07910)), ([:house:](https://huggingface.co/papers/2312.07910)), ([:eight_spoked_asterisk:](nified-library-for-evaluation)), ([:octocat:](https://github.com/microsoft/promptbench)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/promptbench?style=social))  |
| 12.13 | SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention ([:x:](https://arxiv.org/abs/2312.07987)), ([:book:](https://browse.arxiv.org/pdf/2312.07987.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.07987.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.07987)), ([:house:](https://huggingface.co/papers/2312.07987)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/switchhead-accelerating-transformers-with)), ([:octocat:](https://github.com/robertcsordas/moe_attention)![GitHub Repo stars](https://img.shields.io/github/stars/robertcsordas/moe_attention?style=social))  |
| 12.13 | CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor ([:x:](https://arxiv.org/abs/2312.07661)), ([:book:](https://browse.arxiv.org/pdf/2312.07661.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.07661.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.07661)), ([:house:](https://huggingface.co/papers/2312.07661)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/clip-as-rnn-segment-countless-visual-concepts)) |
| 12.13 | Clockwork Diffusion: Efficient Generation With Model-Step Distillation ([:x:](https://arxiv.org/abs/2312.08128)), ([:book:](https://browse.arxiv.org/pdf/2312.08128.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.08128.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.08128)), ([:house:](https://huggingface.co/papers/2312.08128)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/clockwork-diffusion-efficient-generation-with)) |
| 12.13 | Foundation Models in Robotics: Applications, Challenges, and the Future ([:x:](https://arxiv.org/abs/2312.07843)), ([:book:](https://browse.arxiv.org/pdf/2312.07843.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.07843.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.07843)), ([:house:](https://huggingface.co/papers/2312.07843)), ([:eight_spoked_asterisk:]()) |
| 12.13 | The Rise of ‘Small Language Models’ and Reinforcement Learning ([news](https://www.theinformation.com/articles/the-rise-of-small-language-models-and-reinforcement-learning)) |
| 12.13 | Mistral AI Picks ‘Mixture of Experts’ Model to Challenge GPT 3.5 ([news](https://decrypt.co/209540/mistral-ai-picks-mixture-of-experts-model-to-challenge-gpt-3-5)) |
| 12.13 | OpenAI’s chief scientist helped to create ChatGPT — while worrying about AI safety (Nature [news](https://www.nature.com/articles/d41586-023-03925-3)) |
| 12.13 | Nature’s Ten people (and one non-human) who helped shape science in 2023 ([news](https://www.nature.com/immersive/d41586-023-03919-1/index.html)) |
| 12.13 | OpenAI and Axel Springer strike unprecedented deal to offer news in ChatGPT ([news](https://www.cnbc.com/2023/12/13/openai-and-axel-springer-strike-unprecedented-deal-to-offer-news-in-chatgpt.html)) |
| 12.13 | ChatGPT and science: the AI system was a force in 2023 — for good and bad (Nature [doi: https://doi.org/10.1038/d41586-023-03930-6](https://www.nature.com/articles/d41586-023-03930-6)) |
| 12.13 | PEEKABOO: Interactive Video Generation via Masked-Diffusion ([:x:](https://arxiv.org/abs/2312.07509)), ([:book:](https://browse.arxiv.org/pdf/2312.07509.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.07509.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.07509)), ([:house:](https://huggingface.co/papers/2312.07509)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/peekaboo-interactive-video-generation-via)) |
| 12.13 | How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary Investigation ([:x:](https://arxiv.org/abs/2312.07424)), ([:book:](https://browse.arxiv.org/pdf/2312.07424.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.07424.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.07424)), ([:house:](https://huggingface.co/papers/2312.07424)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/how-well-does-gpt-4v-ision-adapt-to)) |
| 12.13 | Interfacing Foundation Models' Embeddings ([:x:](https://arxiv.org/abs/2312.07532)), ([:book:](https://browse.arxiv.org/pdf/2312.07532.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.07532.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.07532)), ([:house:](https://huggingface.co/papers/2312.07532)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/interfacing-foundation-models-embeddings)), ([:octocat:](https://github.com/ux-decoder/find)![GitHub Repo stars](https://img.shields.io/github/stars/ux-decoder/find?style=social))  |
| 12.13 | FreeInit: Bridging Initialization Gap in Video Diffusion Models ([:x:](https://arxiv.org/abs/2312.07537)), ([:book:](https://browse.arxiv.org/pdf/2312.07537.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.07537.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.07537)), ([:house:](https://huggingface.co/papers/2312.07537)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/freeinit-bridging-initialization-gap-in-video)), ([:octocat:](https://github.com/tianxingwu/freeinit)![GitHub Repo stars](https://img.shields.io/github/stars/tianxingwu/freeinit?style=social))  |
| 12.13 | FreeControl: Training-Free Spatial Control of Any Text-to-Image Diffusion Model with Any Condition ([:x:](https://arxiv.org/abs/2312.07536)), ([:book:](https://browse.arxiv.org/pdf/2312.07536.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.07536.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.07536)), ([:house:](https://huggingface.co/papers/2312.07536)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/freecontrol-training-free-spatial-control-of)) |
| 12.13 | DiffMorpher: Unleashing the Capability of Diffusion Models for Image Morphing ([project](https://kevin-thu.github.io/DiffMorpher_page/)), ([:x:](https://arxiv.org/abs/2312.07409)), ([:book:](https://browse.arxiv.org/pdf/2312.07409.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.07409.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.07409)), ([:house:](https://huggingface.co/papers/2312.07409)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/diffmorpher-unleashing-the-capability-of)), ([:octocat:](https://github.com/Kevin-thu/DiffMorpher)![GitHub Repo stars](https://img.shields.io/github/stars/Kevin-thu/DiffMorpher?style=social)), ([demo](https://huggingface.co/spaces/Kevin-thu/DiffMorpher))  |
| 12.13 | VILA: On Pre-training for Visual Language Models ([:x:](https://arxiv.org/abs/2312.07533)), ([:book:](https://browse.arxiv.org/pdf/2312.07533.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.07533.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.07533)), ([:house:](https://huggingface.co/papers/2312.07533)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vila-on-pre-training-for-visual-language)) |
| 12.13 | A Survey of Text Watermarking in the Era of Large Language Models ([:x:](https://arxiv.org/abs/2312.07913)), ([:book:](https://browse.arxiv.org/pdf/2312.07913.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.07913.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.07913)), ([:house:](https://huggingface.co/papers/2312.07913)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-survey-of-text-watermarking-in-the-era-of)), ([SS](https://www.semanticscholar.org/paper/A-Survey-of-Text-Watermarking-in-the-Era-of-Large-Liu-Pan/3686311263da911309233e93a271f44f2a1f1c3a)) |
| 12.12 | promptbase - All things prompt engineering ([:octocat:](https://github.com/microsoft/promptbase)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/promptbase?style=social)) |
| 12.12 | Steering at the Frontier: Extending the Power of Prompting (Microsoft [blog](https://www.microsoft.com/en-us/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/)) |
| 12.12 | LLMEval: A Preliminary Study on How to Evaluate Large Language Models ([:x:](https://arxiv.org/abs/2312.07398)), ([:book:](https://browse.arxiv.org/pdf/2312.07398.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.07398.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.07398)), ([:house:](https://huggingface.co/papers/2312.07398)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llmeval-a-preliminary-study-on-how-to)) |
| 12.12 | Domain Prompt Learning with Quaternion Networks ([:x:](https://arxiv.org/abs/2312.08878)), ([:book:](https://browse.arxiv.org/pdf/2312.08878.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.08878.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.08878)), ([:house:](https://huggingface.co/papers/2312.08878)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/domain-prompt-learning-with-quaternion)) |
| 12.12 | SM70: A Large Language Model for Medical Devices ([:x:](https://arxiv.org/abs/2312.06974)), ([:book:](https://browse.arxiv.org/pdf/2312.06974.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.06974.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.06974)), ([:house:](https://huggingface.co/papers/2312.06974)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sm70-a-large-language-model-for-medical)) |
| 12.12 | Efficient Few-Shot Clinical Task Adaptation with Large Language Models ([:x:](https://arxiv.org/abs/2312.07125)), ([:book:](https://browse.arxiv.org/pdf/2312.07125.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.07125.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.07125)), ([:house:](https://huggingface.co/papers/2312.07125)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/efficient-few-shot-clinical-task-adaptation)) |
| 12.12 | Mathematical Language Models: A Survey ([:x:](https://arxiv.org/abs/2312.07622)), ([:book:](https://browse.arxiv.org/pdf/2312.07622.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.07622.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.07622)), ([:house:](https://huggingface.co/papers/2312.07622)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mathematical-language-models-a-survey)) |
| 12.12 | Phi-2: The surprising power of small language models (Microsoft [blog](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)) |
| 12.12 | Microsoft debuts 2.7B-parameter Phi-2 model that outperforms many larger language models ([news](https://siliconangle.com/2023/12/12/microsoft-debuts-2-7b-parameter-phi-2-model-outperforms-many-larger-language-models/)) |
| 12.12 | Google’s New AI, Gemini, Beats ChatGPT In 30 Of 32 Test Categories (Forbes [news](https://www.forbes.com/sites/chriswestfall/2023/12/12/googles-new-ai-gemini-beats-chatgpt-in-30-of-32-test-categories/?sh=73956e3b6c80)) |
| 12.12 | From Text to Motion: Grounding GPT-4 in a Humanoid Robot "Alter3" ([:x:](https://arxiv.org/abs/2312.06571)), ([:book:](https://browse.arxiv.org/pdf/2312.06571.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.06571.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.06571)), ([:house:](https://huggingface.co/papers/2312.06571)), ([:eight_spoked_asterisk:]()) |
| 12.12 | LLM360: Towards Fully Transparent Open-Source LLMs ([:x:](https://arxiv.org/abs/2312.06550)), ([:book:](https://browse.arxiv.org/pdf/2312.06550.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.06550.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.06550)), ([:house:](https://huggingface.co/papers/2312.06550)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llm360-towards-fully-transparent-open-source)), ([:octocat:](https://github.com/llm360/analysis360)![GitHub Repo stars](https://img.shields.io/github/stars/llm360/analysis360?style=social))  |
| 12.12 | Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior ([:x:](https://arxiv.org/abs/2312.06655)), ([:book:](https://browse.arxiv.org/pdf/2312.06655.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.06655.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.06655)), ([:house:](https://huggingface.co/papers/2312.06655)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sherpa3d-boosting-high-fidelity-text-to-3d)), ([:octocat:](https://github.com/liuff19/Sherpa3D)![GitHub Repo stars](https://img.shields.io/github/stars/liuff19/Sherpa3D?style=social))  |
| 12.12 | Photorealistic Video Generation with Diffusion Models ([:x:](https://arxiv.org/abs/2312.06662)), ([:book:](https://browse.arxiv.org/pdf/2312.06662.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.06662.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.06662)), ([:house:](https://huggingface.co/papers/2312.06662)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/photorealistic-video-generation-with)) |
| 12.12 | "I Want It That Way": Enabling Interactive Decision Support Using Large Language Models and Constraint Programming ([:x:](https://arxiv.org/abs/2312.06908)), ([:book:](https://browse.arxiv.org/pdf/2312.06908.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.06908.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.06908)), ([:house:](https://huggingface.co/papers/2312.06908)), ([:eight_spoked_asterisk:]()) |
| 12.12 | Fast Training of Diffusion Transformer with Extreme Masking for 3D Point Clouds Generation ([:x:](https://arxiv.org/abs/2312.07231)), ([:book:](https://browse.arxiv.org/pdf/2312.07231.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.07231.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.07231)), ([:house:](https://huggingface.co/papers/2312.07231)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fast-training-of-diffusion-transformer-with)) |
| 12.12 | Rethinking Compression: Reduced Order Modelling of Latent Features in Large Language Models ([:x:](https://arxiv.org/abs/2312.07046)), ([:book:](https://browse.arxiv.org/pdf/2312.07046.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.07046.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.07046)), ([:house:](https://huggingface.co/papers/2312.07046)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rethinking-compression-reduced-order)), ([:octocat:](https://github.com/transmuteai/trailmet)![GitHub Repo stars](https://img.shields.io/github/stars/transmuteai/trailmet?style=social))  |
| 12.12 | Honeybee: Locality-enhanced Projector for Multimodal LLM ([:x:](https://arxiv.org/abs/2312.06742)), ([:book:](https://browse.arxiv.org/pdf/2312.06742.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.06742.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.06742)), ([:house:](https://huggingface.co/papers/2312.06742)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/honeybee-locality-enhanced-projector-for)), ([:octocat:](https://github.com/kakaobrain/honeybee)![GitHub Repo stars](https://img.shields.io/github/stars/kakaobrain/honeybee?style=social))  |
| 12.12 | COLMAP-Free 3D Gaussian Splatting ([:x:](https://arxiv.org/abs/2312.07504)), ([:book:](https://browse.arxiv.org/pdf/2312.07504.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.07504.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.07504)), ([:house:](https://huggingface.co/papers/2312.07504)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/colmap-free-3d-gaussian-splatting)) |
| 12.12 | Alignment for Honesty ([:x:](https://arxiv.org/abs/2312.07000)), ([:book:](https://browse.arxiv.org/pdf/2312.07000.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.07000.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.07000)), ([:house:](https://huggingface.co/papers/2312.07000)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/alignment-for-honesty)), ([:octocat:](https://github.com/gair-nlp/alignment-for-honesty)![GitHub Repo stars](https://img.shields.io/github/stars/gair-nlp/alignment-for-honesty?style=social))  |
| 12.12 | CCM: Adding Conditional Controls to Text-to-Image Consistency Models ([:x:](https://arxiv.org/abs/2312.06971)), ([:book:](https://browse.arxiv.org/pdf/2312.06971.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.06971.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.06971)), ([:house:](https://huggingface.co/papers/2312.06971)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ccm-adding-conditional-controls-to-text-to)) |
| 12.11 | Privacy Issues in Large Language Models: A Survey ([:x:](https://arxiv.org/abs/2312.06717)), ([:book:](https://browse.arxiv.org/pdf/2312.06717.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.06717.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.06717)), ([:house:](https://huggingface.co/papers/2312.06717)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/privacy-issues-in-large-language-models-a)), ([:octocat:](https://github.com/safr-ml-lab/survey-llm)![GitHub Repo stars](https://img.shields.io/github/stars/safr-ml-lab/survey-llm?style=social)), ([SS](https://www.semanticscholar.org/paper/Privacy-Issues-in-Large-Language-Models%3A-A-Survey-Neel-Chang/6900a4cff871055213077e1c846cdf70b77c91a7))  |
| 12.11 | Why We Support and Encourage the Use of Large Language Models in NEJM AI Submissions ([paper](https://ai.nejm.org/doi/full/10.1056/AIe2300128)) |
| 12.11 | Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models ([:x:](https://arxiv.org/abs/2312.06109)), ([:book:](https://browse.arxiv.org/pdf/2312.06109.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.06109.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.06109)), ([:house:](https://huggingface.co/papers/2312.06109)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vary-scaling-up-the-vision-vocabulary-for)) |
| 12.11 | Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models ([:x:](https://arxiv.org/abs/2312.06585)), ([:book:](https://browse.arxiv.org/pdf/2312.06585.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.06585.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.06585)), ([:house:](https://huggingface.co/papers/2312.06585)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/beyond-human-data-scaling-self-training-for)) |
| 12.11 | A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges ([:x:](https://arxiv.org/abs/2311.05112)), ([:book:](https://browse.arxiv.org/pdf/2311.05112.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.05112.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.05112)), ([:house:](https://huggingface.co/papers/2311.05112)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-survey-of-large-language-models-in-medicine)), ([:octocat:](https://github.com/ai-in-health/medllmspracticalguide)![GitHub Repo stars](https://img.shields.io/github/stars/ai-in-health/medllmspracticalguide?style=social))  |
| 12.11 | Artificial Intelligence vs Clinician Performance in Estimating Probabilities of Diagnoses Before and After Testing (JAMA [doi:10.1001/jamanetworkopen.2023.47075](https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2812737)) |
| 12.11 | Evaluation of Large Language Models for Decision Making in Autonomous Driving ([:x:](https://arxiv.org/abs/2312.06351)), ([:book:](https://browse.arxiv.org/pdf/2312.06351.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.06351.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.06351)), ([:house:](https://huggingface.co/papers/2312.06351)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evaluation-of-large-language-models-for-1)) |
| 12.11 | Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes ([:x:](https://arxiv.org/abs/2312.06353)), ([:book:](https://browse.arxiv.org/pdf/2312.06353.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.06353.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.06353)), ([:house:](https://huggingface.co/papers/2312.06353)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/federated-full-parameter-tuning-of-billion)) |
| 12.10 | Context Tuning for Retrieval Augmented Generation ([:x:](https://arxiv.org/abs/2312.05708)), ([:book:](https://browse.arxiv.org/pdf/2312.05708.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.05708.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.05708)), ([:house:](https://huggingface.co/papers/2312.05708)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/context-tuning-for-retrieval-augmented)) |
| 12.9 | Using Captum to Explain Generative Language Models ([:x:](https://arxiv.org/abs/2312.05491)), ([:book:](https://browse.arxiv.org/pdf/2312.05491.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.05491.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.05491)), ([:house:](https://huggingface.co/papers/2312.05491)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/using-captum-to-explain-generative-language)) |
| 12.9 | Efficient Quantization Strategies for Latent Diffusion Models ([:x:](https://arxiv.org/abs/2312.05431)), ([:book:](https://browse.arxiv.org/pdf/2312.05431.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.05431.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.05431)), ([:house:](https://huggingface.co/papers/2312.05431)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/efficient-quantization-strategies-for-latent)) |
| 12.9 | Steering Llama 2 via Contrastive Activation Addition ([:x:](https://arxiv.org/abs/2312.06681)), ([:book:](https://browse.arxiv.org/pdf/2312.06681.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.06681.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.06681)), ([:house:](https://huggingface.co/papers/2312.06681)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/steering-llama-2-via-contrastive-activation)) |
| 12.9 | Artificial intelligence act: Council and Parliament strike a deal on the first rules for AI in the world ([press](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/)) |
| 12.9 | Google’s best Gemini AI demo video was fabricated ([news](https://arstechnica.com/information-technology/2023/12/google-admits-it-fudged-a-gemini-ai-demo-video-which-critics-say-misled-viewers/)) |
| 12.9 | DreaMoving: A Human Dance Video Generation Framework based on Diffusion Models ([:x:](https://arxiv.org/abs/2312.05107)), ([:book:](https://browse.arxiv.org/pdf/2312.05107.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.05107.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.05107)), ([:house:](https://huggingface.co/papers/2312.05107)), ([:eight_spoked_asterisk:]()) |
| 12.9 | PathFinder: Guided Search over Multi-Step Reasoning Paths ([:x:](https://arxiv.org/abs/2312.05180)), ([:book:](https://browse.arxiv.org/pdf/2312.05180.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.05180.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.05180)), ([:house:](https://huggingface.co/papers/2312.05180)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pathfinder-guided-search-over-multi-step)) |
| 12.8 | Perspectives on the State and Future of Deep Learning -- 2023 ([:x:](https://arxiv.org/abs/2312.09323)), ([:book:](https://browse.arxiv.org/pdf/2312.09323.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.09323.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.09323)), ([:house:](https://huggingface.co/papers/2312.09323)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/perspectives-on-the-state-and-future-of-deep)) |
| 12.8 | Are We Testing or Being Tested? Exploring the Practical Applications of Large Language Models in Software Testing ([:x:](https://arxiv.org/abs/2312.04860)), ([:book:](https://browse.arxiv.org/pdf/2312.04860.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04860.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04860)), ([:house:](https://huggingface.co/papers/2312.04860)), ([:eight_spoked_asterisk:]()), ([SS](https://www.semanticscholar.org/paper/Are-We-Testing-or-Being-Tested-Exploring-the-of-in-Santos-Santos/9fe9b361e954554d38d209205db8ba83139c1e9e)) |
| 12.8 | Assessing LLMs for Moral Value Pluralism ([:x:](https://arxiv.org/abs/2312.10075)), ([:book:](https://browse.arxiv.org/pdf/2312.10075.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10075.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10075)), ([:house:](https://huggingface.co/papers/2312.10075)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/assessing-llms-for-moral-value-pluralism)), ([SS](https://www.semanticscholar.org/paper/Assessing-LLMs-for-Moral-Value-Pluralism-Benkler-Mosaphir/5204ea886dd9391fdea6975c36e8c2305c9813d1)) |
| 12.8 | Large-scale Training of Foundation Models for Wearable Biosignals ([:x:](https://arxiv.org/abs/2312.05409)), ([:book:](https://browse.arxiv.org/pdf/2312.05409.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.05409.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.05409)), ([:house:](https://huggingface.co/papers/2312.05409)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-scale-training-of-foundation-models-for)) |
| 12.8 | Ophtha-LLaMA2: A Large Language Model for Ophthalmology ([:x:](https://arxiv.org/abs/2312.04906)), ([:book:](https://browse.arxiv.org/pdf/2312.04906.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04906.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04906)), ([:house:](https://huggingface.co/papers/2312.04906)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ophtha-llama2-a-large-language-model-for)) |
| 12.8 | MVDD: Multi-View Depth Diffusion Models ([:x:](https://arxiv.org/abs/2312.04875)), ([:book:](https://browse.arxiv.org/pdf/2312.04875.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04875.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04875)), ([:house:](https://huggingface.co/papers/2312.04875)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mvdd-multi-view-depth-diffusion-models)) |
| 12.8 | ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations ([:x:](https://arxiv.org/abs/2312.04655)), ([:book:](https://browse.arxiv.org/pdf/2312.04655.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04655.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04655)), ([:house:](https://huggingface.co/papers/2312.04655)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/eclipse-a-resource-efficient-text-to-image)) |
| 12.8 | Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models ([:x:](https://arxiv.org/abs/2312.04724)), ([:book:](https://browse.arxiv.org/pdf/2312.04724.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04724.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04724)), ([:house:](https://huggingface.co/papers/2312.04724)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/purple-llama-cyberseceval-a-secure-coding)) |
| 12.8 | SparQ Attention: Bandwidth-Efficient LLM Inference ([:x:](https://arxiv.org/abs/2312.04985)), ([:book:](https://browse.arxiv.org/pdf/2312.04985.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04985.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04985)), ([:house:](https://huggingface.co/papers/2312.04985)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sparq-attention-bandwidth-efficient-llm)) |
| 12.8 | GPT4 paper assistant: A daily ArXiv scanner ([:octocat:](https://github.com/tatsu-lab/gpt_paper_assistant)![GitHub Repo stars](https://img.shields.io/github/stars/tatsu-lab/gpt_paper_assistant?style=social)), ([demo](https://tatsu-lab.github.io/gpt_paper_assistant/)) |
| 12.8 | EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism ([:x:](https://arxiv.org/abs/2312.04916)), ([:book:](https://browse.arxiv.org/pdf/2312.04916.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04916.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04916)), ([:house:](https://huggingface.co/papers/2312.04916)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ee-llm-large-scale-training-and-inference-of)) |
| 12.8 | Customizing Motion in Text-to-Video Diffusion Models ([:x:](https://arxiv.org/abs/2312.04966)), ([:book:](https://browse.arxiv.org/pdf/2312.04966.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04966.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04966)), ([:house:](https://huggingface.co/papers/2312.04966)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/customizing-motion-in-text-to-video-diffusion)) |
| 12.8 | HyperDreamer: Hyper-Realistic 3D Content Generation and Editing from a Single Image ([:x:](https://arxiv.org/abs/2312.04543)), ([:book:](https://browse.arxiv.org/pdf/2312.04543.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04543.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04543)), ([:house:](https://huggingface.co/papers/2312.04543)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hyperdreamer-hyper-realistic-3d-content)) |
| 12.8 | PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding ([:x:](https://arxiv.org/abs/2312.04461)), ([:book:](https://browse.arxiv.org/pdf/2312.04461.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04461.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04461)), ([:house:](https://huggingface.co/papers/2312.04461)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/photomaker-customizing-realistic-human-photos)) |
| 12.8 | Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models ([:x:](https://arxiv.org/abs/2312.04410)), ([:book:](https://browse.arxiv.org/pdf/2312.04410.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04410.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04410)), ([:house:](https://huggingface.co/papers/2312.04410)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/smooth-diffusion-crafting-smooth-latent)), ([:octocat:](https://github.com/shi-labs/smooth-diffusion)![GitHub Repo stars](https://img.shields.io/github/stars/shi-labs/smooth-diffusion?style=social))  |
| 12.8 | GenTron: Delving Deep into Diffusion Transformers for Image and Video Generation ([:x:](https://arxiv.org/abs/2312.04557)), ([:book:](https://browse.arxiv.org/pdf/2312.04557.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04557.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04557)), ([:house:](https://huggingface.co/papers/2312.04557)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gentron-delving-deep-into-diffusion)) |
| 12.8 | NeRFiller: Completing Scenes via Generative 3D Inpainting ([:x:](https://arxiv.org/abs/2312.04560)), ([:book:](https://browse.arxiv.org/pdf/2312.04560.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04560.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04560)), ([:house:](https://huggingface.co/papers/2312.04560)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/nerfiller-completing-scenes-via-generative-3d)) |
| 12.8 | Large Language Models for Mathematicians ([:x:](https://arxiv.org/abs/2312.04556)), ([:book:](https://browse.arxiv.org/pdf/2312.04556.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04556.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04556)), ([:house:](https://huggingface.co/papers/2312.04556)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-for-mathematicians)) |
| 12.8 | Gen2Det: Generate to Detect ([:x:](https://arxiv.org/abs/2312.04566)), ([:book:](https://browse.arxiv.org/pdf/2312.04566.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04566.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04566)), ([:house:](https://huggingface.co/papers/2312.04566)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gen2det-generate-to-detect)) |
| 12.8 | DreamVideo: Composing Your Dream Videos with Customized Subject and Motion ([:x:](https://arxiv.org/abs/2312.04433)), ([:book:](https://browse.arxiv.org/pdf/2312.04433.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04433.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04433)), ([:house:](https://huggingface.co/papers/2312.04433)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dreamvideo-composing-your-dream-videos-with)) |
| 12.8 | Scaling Laws of Synthetic Images for Model Training ... for Now ([:x:](https://arxiv.org/abs/2312.04567)), ([:book:](https://browse.arxiv.org/pdf/2312.04567.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04567.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04567)), ([:house:](https://huggingface.co/papers/2312.04567)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/scaling-laws-of-synthetic-images-for-model)), ([:octocat:](https://github.com/google-research/syn-rep-learn)![GitHub Repo stars](https://img.shields.io/github/stars/google-research/syn-rep-learn?style=social))  |
| 12.8 | Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation ([:x:](https://arxiv.org/abs/2312.04483)), ([:book:](https://browse.arxiv.org/pdf/2312.04483.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04483.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04483)), ([:house:](https://huggingface.co/papers/2312.04483)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hierarchical-spatio-temporal-decoupling-for)) |
| 12.8 | Generating Illustrated Instructions ([:x:](https://arxiv.org/abs/2312.04552)), ([:book:](https://browse.arxiv.org/pdf/2312.04552.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04552.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04552)), ([:house:](https://huggingface.co/papers/2312.04552)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/generating-illustrated-instructions)) |
| 12.8 | Efficient Monotonic Multihead Attention ([:x:](https://arxiv.org/abs/2312.04515)), ([:book:](https://browse.arxiv.org/pdf/2312.04515.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04515.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04515)), ([:house:](https://huggingface.co/papers/2312.04515)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/efficient-monotonic-multihead-attention)) |
| 12.8 | Seamless: Multilingual Expressive and Streaming Speech Translation ([:x:](https://arxiv.org/abs/2312.05187)), ([:book:](https://browse.arxiv.org/pdf/2312.05187.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.05187.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.05187)), ([:house:](https://huggingface.co/papers/2312.05187)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/seamless-multilingual-expressive-and)), ([:octocat:](https://github.com/facebookresearch/seamless_communication)![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/seamless_communication?style=social))  |
| 12.7 | Performance of Large Language Models on a Neurology Board–Style Examination (JAMA [doi:10.1001/jamanetworkopen.2023.46721](https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2812620)) |
| 12.7 | LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning ([:x:](https://arxiv.org/abs/2312.03849)), ([:book:](https://browse.arxiv.org/pdf/2312.03849.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03849.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03849)), ([:house:](https://huggingface.co/papers/2312.03849)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lego-learning-egocentric-action-frame)) |
| 12.7 | Beyond Surface: Probing LLaMA Across Scales and Layers ([:x:](https://arxiv.org/abs/2312.04333)), ([:book:](https://browse.arxiv.org/pdf/2312.04333.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04333.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04333)), ([:house:](https://huggingface.co/papers/2312.04333)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/beyond-surface-probing-llama-across-scales)), ([:octocat:](https://github.com/nuochenpku/llama_analysis)![GitHub Repo stars](https://img.shields.io/github/stars/nuochenpku/llama_analysis?style=social))  |
| 12.7 | Pearl: A Production-ready Reinforcement Learning Agent ([:x:](https://arxiv.org/abs/2312.03814)), ([:book:](https://browse.arxiv.org/pdf/2312.03814.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03814.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03814)), ([:house:](https://huggingface.co/papers/2312.03814)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pearl-a-production-ready-reinforcement)), ([:octocat:](https://github.com/facebookresearch/pearl)![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/pearl?style=social))  |
| 12.7 | Controllable Human-Object Interaction Synthesis ([:x:](https://arxiv.org/abs/2312.03913)), ([:book:](https://browse.arxiv.org/pdf/2312.03913.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03913.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03913)), ([:house:](https://huggingface.co/papers/2312.03913)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/controllable-human-object-interaction)) |
| 12.7 | Alpha-CLIP: A CLIP Model Focusing on Wherever You Want ([:x:](https://arxiv.org/abs/2312.03818)), ([:book:](https://browse.arxiv.org/pdf/2312.03818.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03818.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03818)), ([:house:](https://huggingface.co/papers/2312.03818)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/alpha-clip-a-clip-model-focusing-on-wherever)), ([:octocat:](https://github.com/sunzey/alphaclip)![GitHub Repo stars](https://img.shields.io/github/stars/sunzey/alphaclip?style=social))  |
| 12.7 | Text-to-3D Generation with Bidirectional Diffusion using both 2D and 3D priors ([:x:](https://arxiv.org/abs/2312.04963)), ([:book:](https://browse.arxiv.org/pdf/2312.04963.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04963.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04963)), ([:house:](https://huggingface.co/papers/2312.04963)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/text-to-3d-generation-with-bidirectional)) |
| 12.7 | Announcing Purple Llama: Towards open trust and safety in the new world of generative AI ([blog](https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai/)) |
| 12.7 | Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations ([paper](https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/)), ([:x:](https://arxiv.org/abs/2312.06674)), ([:book:](https://browse.arxiv.org/pdf/2312.06674.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.06674.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.06674)), ([:house:](https://huggingface.co/papers/2312.06674)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llama-guard-llm-based-input-output-safeguard)) |
| 12.7 | Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on Prompt Engineering Strategies ([:x:](https://arxiv.org/abs/2312.04344)), ([:book:](https://browse.arxiv.org/pdf/2312.04344.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04344.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04344)), ([:house:](https://huggingface.co/papers/2312.04344)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/enhancing-medical-task-performance-in-gpt-4v)) |
| 12.7 | GPT-4V with Emotion: A Zero-shot Benchmark for Multimodal Emotion Understanding ([:x:](https://arxiv.org/abs/2312.04293)), ([:book:](https://browse.arxiv.org/pdf/2312.04293.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04293.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04293)), ([:house:](https://huggingface.co/papers/2312.04293)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gpt-4v-with-emotion-a-zero-shot-benchmark-for)) |
| 12.7 | Chain of Code: Reasoning with a Language Model-Augmented Code Emulator ([:x:](https://arxiv.org/abs/2312.04474)), ([:book:](https://browse.arxiv.org/pdf/2312.04474.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.04474.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.04474)), ([:house:](https://huggingface.co/papers/2312.04474)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chain-of-code-reasoning-with-a-language-model)) |
| 12.7 | OneLLM: One Framework to Align All Modalities with Language ([:x:](https://arxiv.org/abs/2312.03700)), ([:book:](https://browse.arxiv.org/pdf/2312.03700.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03700.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03700)), ([:house:](https://huggingface.co/papers/2312.03700)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/onellm-one-framework-to-align-all-modalities)), ([:octocat:](https://github.com/csuhan/onellm)![GitHub Repo stars](https://img.shields.io/github/stars/csuhan/onellm?style=social))  |
| 12.7 | Relightable Gaussian Codec Avatars ([:x:](https://arxiv.org/abs/2312.03704)), ([:book:](https://browse.arxiv.org/pdf/2312.03704.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03704.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03704)), ([:house:](https://huggingface.co/papers/2312.03704)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/relightable-gaussian-codec-avatars)) |
| 12.7 | MotionCtrl: A Unified and Flexible Motion Controller for Video Generation ([:x:](https://arxiv.org/abs/2312.03641)), ([:book:](https://browse.arxiv.org/pdf/2312.03641.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03641.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03641)), ([:house:](https://huggingface.co/papers/2312.03641)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/motionctrl-a-unified-and-flexible-motion)) |
| 12.7 | Context Diffusion: In-Context Aware Image Generation ([:x:](https://arxiv.org/abs/2312.03584)), ([:book:](https://browse.arxiv.org/pdf/2312.03584.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03584.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03584)), ([:house:](https://huggingface.co/papers/2312.03584)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/context-diffusion-in-context-aware-image)) |
| 12.7 | DreamComposer: Controllable 3D Object Generation via Multi-View Conditions ([:x:](https://arxiv.org/abs/2312.03611)), ([:book:](https://browse.arxiv.org/pdf/2312.03611.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03611.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03611)), ([:house:](https://huggingface.co/papers/2312.03611)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dreamcomposer-controllable-3d-object)) |
| 12.7 | Self-conditioned Image Generation via Generating Representations ([:x:](https://arxiv.org/abs/2312.03701)), ([:book:](https://browse.arxiv.org/pdf/2312.03701.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03701.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03701)), ([:house:](https://huggingface.co/papers/2312.03701)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/self-conditioned-image-generation-via)), ([:octocat:](https://github.com/LTH14/rcg)![GitHub Repo stars](https://img.shields.io/github/stars/LTH14/rcg?style=social)) |
| 12.7 | Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia ([:x:](https://arxiv.org/abs/2312.03664)), ([:book:](https://browse.arxiv.org/pdf/2312.03664.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03664.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03664)), ([:house:](https://huggingface.co/papers/2312.03664)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/generative-agent-based-modeling-with-actions)), ([:octocat:](https://github.com/google-deepmind/concordia)![GitHub Repo stars](https://img.shields.io/github/stars/google-deepmind/concordia?style=social))  |
| 12.7 | Multimodal Data and Resource Efficient Device-Directed Speech Detection with Large Foundation Models ([:x:](https://arxiv.org/abs/2312.03632)), ([:book:](https://browse.arxiv.org/pdf/2312.03632.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03632.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03632)), ([:house:](https://huggingface.co/papers/2312.03632)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/multimodal-data-and-resource-efficient-device)) |
| 12.7 | Language-Informed Visual Concept Learning ([:x:](https://arxiv.org/abs/2312.03587)), ([:book:](https://browse.arxiv.org/pdf/2312.03587.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03587.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03587)), ([:house:](https://huggingface.co/papers/2312.03587)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/language-informed-visual-concept-learning)) |
| 12.6 | Assessing the Usability of GutGPT: A Simulation Study of an AI Clinical Decision Support System for Gastrointestinal Bleeding Risk ([:x:](https://arxiv.org/abs/2312.10072)), ([:book:](https://browse.arxiv.org/pdf/2312.10072.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10072.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10072)), ([:house:](https://huggingface.co/papers/2312.10072)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/assessing-the-usability-of-gutgpt-a)), ([SS](https://www.semanticscholar.org/paper/Assessing-the-Usability-of-GutGPT%3A-A-Simulation-of-Chan-You/d03810c0a5efa889c8c095b2be4766eecae7ccc2)) |
| 12.6 | LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models ([:x:](https://arxiv.org/abs/2312.02949)), ([:book:](https://browse.arxiv.org/pdf/2312.02949.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02949.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02949)), ([:house:](https://huggingface.co/papers/2312.02949)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llava-grounding-grounded-visual-chat-with)), ([:octocat:](https://github.com/ux-decoder/llava-grounding)![GitHub Repo stars](https://img.shields.io/github/stars/ux-decoder/llava-grounding?style=social)) |
| 12.6 | Alchemist: Parametric Control of Material Properties with Diffusion Models ([:x:](https://arxiv.org/abs/2312.02970)), ([:book:](https://browse.arxiv.org/pdf/2312.02970.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02970.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02970)), ([:house:](https://huggingface.co/papers/2312.02970)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/alchemist-parametric-control-of-material)) |
| 12.6 | GPT4Point: A Unified Framework for Point-Language Understanding and Generation ([:x:](https://arxiv.org/abs/2312.02980)), ([:book:](https://browse.arxiv.org/pdf/2312.02980.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02980.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02980)), ([:house:](https://huggingface.co/papers/2312.02980)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gpt4point-a-unified-framework-for-point)) |
| 12.6 | WhisBERT: Multimodal Text-Audio Language Modeling on 100M Words ([:x:](https://arxiv.org/abs/2312.02931)), ([:book:](https://browse.arxiv.org/pdf/2312.02931.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02931.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02931)), ([:house:](https://huggingface.co/papers/2312.02931)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/whisbert-multimodal-text-audio-language)) |
| 12.6 | Cache Me if You Can: Accelerating Diffusion Models through Block Caching ([:x:](https://arxiv.org/abs/2312.03209)), ([:book:](https://browse.arxiv.org/pdf/2312.03209.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03209.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03209)), ([:house:](https://huggingface.co/papers/2312.03209)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/cache-me-if-you-can-accelerating-diffusion)) |
| 12.6 | HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian Splatting ([:x:](https://arxiv.org/abs/2312.03461)), ([:book:](https://browse.arxiv.org/pdf/2312.03461.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03461.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03461)), ([:house:](https://huggingface.co/papers/2312.03461)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hifi4g-high-fidelity-human-performance)) |
| 12.6 | LooseControl: Lifting ControlNet for Generalized Depth Conditioning ([:x:](https://arxiv.org/abs/2312.03079)), ([:book:](https://browse.arxiv.org/pdf/2312.03079.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03079.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03079)), ([:house:](https://huggingface.co/papers/2312.03079)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/loosecontrol-lifting-controlnet-for)) |
| 12.6 | MagicStick: Controllable Video Editing via Control Handle Transformations ([:x:](https://arxiv.org/abs/2312.03047)), ([:book:](https://browse.arxiv.org/pdf/2312.03047.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03047.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03047)), ([:house:](https://huggingface.co/papers/2312.03047)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/magicstick-controllable-video-editing-via)), ([:octocat:](https://github.com/mayuelala/magicstick)![GitHub Repo stars](https://img.shields.io/github/stars/mayuelala/magicstick?style=social))  |
| 12.6 | HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces ([:x:](https://arxiv.org/abs/2312.03160)), ([:book:](https://browse.arxiv.org/pdf/2312.03160.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03160.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03160)), ([:house:](https://huggingface.co/papers/2312.03160)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hybridnerf-efficient-neural-rendering-via)) |
| 12.6 | Kandinsky 3.0 Technical Report ([:x:](https://arxiv.org/abs/2312.03511)), ([:book:](https://browse.arxiv.org/pdf/2312.03511.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03511.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03511)), ([:house:](https://huggingface.co/papers/2312.03511)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/kandinsky-3-0-technical-report)), ([:octocat:](https://github.com/ai-forever/movqgan)![GitHub Repo stars](https://img.shields.io/github/stars/ai-forever/movqgan?style=social))  |
| 12.6 | Schrodinger Bridges Beat Diffusion Models on Text-to-Speech Synthesis ([:x:](https://arxiv.org/abs/2312.03491)), ([:book:](https://browse.arxiv.org/pdf/2312.03491.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03491.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03491)), ([:house:](https://huggingface.co/papers/2312.03491)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/schrodinger-bridges-beat-diffusion-models-on)) |
| 12.6 | AnimateZero: Video Diffusion Models are Zero-Shot Image Animators ([:x:](https://arxiv.org/abs/2312.03793)), ([:book:](https://browse.arxiv.org/pdf/2312.03793.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03793.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03793)), ([:house:](https://huggingface.co/papers/2312.03793)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/animatezero-video-diffusion-models-are-zero)), ([:octocat:](https://github.com/vvictoryuki/animatezero)![GitHub Repo stars](https://img.shields.io/github/stars/vvictoryuki/animatezero?style=social))  |
| 12.6 | EU Artificial Intelligence act: potential implications for healthcare AI ([blog](https://setterwalls.se/artikel/eu-artificial-intelligence-act-potential-implications-for-healthcare-ai/)) |
| 12.6 | DiffusionSat: A Generative Foundation Model for Satellite Imagery ([:x:](https://arxiv.org/abs/2312.03606)), ([:book:](https://browse.arxiv.org/pdf/2312.03606.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03606.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03606)), ([:house:](https://huggingface.co/papers/2312.03606)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/diffusionsat-a-generative-foundation-model)) |
| 12.6 | Open-sourced Data Ecosystem in Autonomous Driving: the Present and Future ([:x:](https://arxiv.org/abs/2312.03408)), ([:book:](https://browse.arxiv.org/pdf/2312.03408.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03408.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03408)), ([:house:](https://huggingface.co/papers/2312.03408)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/open-sourced-data-ecosystem-in-autonomous)), ([:octocat:](https://github.com/opendrivelab/driveagi)![GitHub Repo stars](https://img.shields.io/github/stars/opendrivelab/driveagi?style=social))  |
| 12.6 | Gemini: A Family of Highly Capable Multimodal Models ([PDF](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf)) |
| 12.6 | Google - AlphaCode 2 Technical Report ([PDF](https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf)) |
| 12.6 | Google -Introducing Gemini: our largest and most capable AI model ([blog](https://blog.google/technology/ai/google-gemini-ai/)), (Hands-on with Gemini: Interacting with multimodal AI - [youtube](https://www.youtube.com/watch?v=UIZAiXYceBI)) | 
| 12.6 | Google - Learn more about Gemini, our most capable AI model ([blog](https://blog.google/technology/ai/gemini-collection/)), (Welcome to the Gemini era - [youtube](https://www.youtube.com/watch?v=jV1vkHv4zq8)) |
| 12.6 | Pixel 8 Pro — the first smartphone with AI built in — is now running Gemini Nano, plus more AI updates coming to the Pixel portfolio ([blog](https://blog.google/products/pixel/pixel-feature-drop-december-2023/)) |
| 12.6 | Early LLM-based Tools for Enterprise Information Workers Likely Provide Meaningful Boosts to Productivity ([paper](https://www.microsoft.com/en-us/research/publication/early-llm-based-tools-for-enterprise-information-workers-likely-provide-meaningful-boosts-to-productivity/)), ([pdf](https://www.microsoft.com/en-us/research/uploads/prod/2023/12/AI-and-Productivity-Report-First-Edition.pdf)) |
| 12.6 | Describing Differences in Image Sets with Natural Language ([:x:](https://arxiv.org/abs/2312.02974)), ([:book:](https://browse.arxiv.org/pdf/2312.02974.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02974.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02974)), ([:house:](https://huggingface.co/papers/2312.02974)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/describing-differences-in-image-sets-with)), ([:octocat:](https://github.com/understanding-visual-datasets/visdiff)![GitHub Repo stars](https://img.shields.io/github/stars/understanding-visual-datasets/visdiff?style=social))  |
| 12.6 | LivePhoto: Real Image Animation with Text-guided Motion Control ([:x:](https://arxiv.org/abs/2312.02928)), ([:book:](https://browse.arxiv.org/pdf/2312.02928.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02928.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02928)), ([:house:](https://huggingface.co/papers/2312.02928)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/livephoto-real-image-animation-with-text)) |
| 12.6 | Rank-without-GPT: Building GPT-Independent Listwise Rerankers on Open-Source Large Language Models ([:x:](https://arxiv.org/abs/2312.02969)), ([:book:](https://browse.arxiv.org/pdf/2312.02969.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02969.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02969)), ([:house:](https://huggingface.co/papers/2312.02969)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rank-without-gpt-building-gpt-independent)) |
| 12.6 | Fine-grained Controllable Video Generation via Object Appearance and Context ([:x:](https://arxiv.org/abs/2312.02919)), ([:book:](https://browse.arxiv.org/pdf/2312.02919.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02919.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02919)), ([:house:](https://huggingface.co/papers/2312.02919)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fine-grained-controllable-video-generation)) |
| 12.6 | MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human Captures ([:x:](https://arxiv.org/abs/2312.02963)), ([:book:](https://browse.arxiv.org/pdf/2312.02963.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02963.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02963)), ([:house:](https://huggingface.co/papers/2312.02963)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mvhumannet-a-large-scale-dataset-of-multi)) |
| 12.6 | ReconFusion: 3D Reconstruction with Diffusion Priors ([:x:](https://arxiv.org/abs/2312.02981)), ([:book:](https://browse.arxiv.org/pdf/2312.02981.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02981.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02981)), ([:house:](https://huggingface.co/papers/2312.02981)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/reconfusion-3d-reconstruction-with-diffusion)) |
| 12.5 | Breast Ultrasound Report Generation using LangChain ([:x:](https://arxiv.org/abs/2312.03013)), ([:book:](https://browse.arxiv.org/pdf/2312.03013.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03013.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03013)), ([:house:](https://huggingface.co/papers/2312.03013)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/breast-ultrasound-report-generation-using)) |
| 12.5 | Generating Fine-Grained Human Motions Using ChatGPT-Refined Descriptions ([:x:](https://arxiv.org/abs/2312.02772)), ([:book:](https://browse.arxiv.org/pdf/2312.02772.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02772.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02772)), ([:house:](https://huggingface.co/papers/2312.02772)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/generating-fine-grained-human-motions-using)) |
| 12.5 | Orthogonal Adaptation for Modular Customization of Diffusion Models ([:x:](https://arxiv.org/abs/2312.02432)), ([:book:](https://browse.arxiv.org/pdf/2312.02432.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02432.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02432)), ([:house:](https://huggingface.co/papers/2312.02432)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/orthogonal-adaptation-for-modular)) |
| 12.5 | FaceStudio: Put Your Face Everywhere in Seconds ([:x:](https://arxiv.org/abs/2312.02663)), ([:book:](https://browse.arxiv.org/pdf/2312.02663.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02663.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02663)), ([:house:](https://huggingface.co/papers/2312.02663)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/facestudio-put-your-face-everywhere-in)) |
| 12.5 | Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians ([:x:](https://arxiv.org/abs/2312.03029)), ([:book:](https://browse.arxiv.org/pdf/2312.03029.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03029.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03029)), ([:house:](https://huggingface.co/papers/2312.03029)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gaussian-head-avatar-ultra-high-fidelity-head)), ([:octocat:](https://github.com/yuelangx/gaussian-head-avatar)![GitHub Repo stars](https://img.shields.io/github/stars/yuelangx/gaussian-head-avatar?style=social))  |
| 12.5 | BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models ([:x:](https://arxiv.org/abs/2312.02896)), ([:book:](https://browse.arxiv.org/pdf/2312.02896.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02896.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02896)), ([:house:](https://huggingface.co/papers/2312.02896)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/benchlmm-benchmarking-cross-style-visual)), ([:octocat:](https://github.com/AIFEG/BenchLMM)![GitHub Repo stars](https://img.shields.io/github/stars/AIFEG/BenchLMM?style=social))  |
| 12.5 | Foundation Models for Weather and Climate Data Understanding: A Comprehensive Survey ([:x:](https://arxiv.org/abs/2312.03014)), ([:book:](https://browse.arxiv.org/pdf/2312.03014.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.03014.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.03014)), ([:house:](https://huggingface.co/papers/2312.03014)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/foundation-models-for-weather-and-climate)) |
| 12.5 | Creative Agents: Empowering Agents with Imagination for Creative Tasks ([:x:](https://arxiv.org/abs/2312.02519)), ([:book:](https://browse.arxiv.org/pdf/2312.02519.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02519.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02519)), ([:house:](https://huggingface.co/papers/2312.02519)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/creative-agents-empowering-agents-with)), ([:octocat:](https://github.com/pku-rl/creative-agents)![GitHub Repo stars](https://img.shields.io/github/stars/pku-rl/creative-agents?style=social))  |
| 12.5 | Large Language Models on Graphs: A Comprehensive Survey ([:x:](https://arxiv.org/abs/2312.02783)), ([:book:](https://browse.arxiv.org/pdf/2312.02783.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02783.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02783)), ([:house:](https://huggingface.co/papers/2312.02783)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-on-graphs-a)), ([:octocat:](https://github.com/petergriffinjin/awesome-language-model-on-graphs)![GitHub Repo stars](https://img.shields.io/github/stars/petergriffinjin/awesome-language-model-on-graphs?style=social))  |
| 12.5 | Magicoder: Source Code Is All You Need ([:x:](https://arxiv.org/abs/2312.02120)), ([:book:](https://browse.arxiv.org/pdf/2312.02120.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02120.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02120)), ([:house:](https://huggingface.co/papers/2312.02120)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/magicoder-source-code-is-all-you-need)), ([:octocat:](https://github.com/ise-uiuc/magicoder)![GitHub Repo stars](https://img.shields.io/github/stars/ise-uiuc/magicoder?style=social))  |
| 12.5 | Llamafile  - Distribute and run LLMs with a single file ([:octocat:](https://github.com/Mozilla-Ocho/llamafile#binary-instructions)![GitHub Repo stars](https://img.shields.io/github/stars/Mozilla-Ocho/llamafile#binary-instructions?style=social)) |
| 12.5 | LLM Visualization ([demo](https://bbycroft.net/llm)) |
| 12.5 | Analyzing and Improving the Training Dynamics of Diffusion Models ([:x:](https://arxiv.org/abs/2312.02696)), ([:book:](https://browse.arxiv.org/pdf/2312.02696.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02696.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02696)), ([:house:](https://huggingface.co/papers/2312.02696)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/analyzing-and-improving-the-training-dynamics)) |
| 12.4 | Competition-Level Problems are Effective LLM Evaluators ([:x:](https://arxiv.org/abs/2312.02143)), ([:book:](https://browse.arxiv.org/pdf/2312.02143.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02143.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02143)), ([:house:](https://huggingface.co/papers/2312.02143)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/competition-level-problems-are-effective)) |
| 12.4 | A collection of principles for guiding and evaluating large language models ([:x:](https://arxiv.org/abs/2312.10059)), ([:book:](https://browse.arxiv.org/pdf/2312.10059.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.10059.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.10059)), ([:house:](https://huggingface.co/papers/2312.10059)), ([:eight_spoked_asterisk:]()), ([SS](https://www.semanticscholar.org/paper/A-collection-of-principles-for-guiding-and-large-Hebenstreit-Praas/ed37dbbd05c59ce0564544f32cbe47a2af4eb73f)) |
| 12.4 | MedXChat: Bridging CXR Modalities with a Unified Multimodal Large Model ([:x:](https://arxiv.org/abs/2312.02233)), ([:book:](https://browse.arxiv.org/pdf/2312.02233.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02233.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02233)), ([:house:](https://huggingface.co/papers/2312.02233)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/medxchat-bridging-cxr-modalities-with-a)) |
| 12.4 | Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks ([:x:](https://arxiv.org/abs/2312.02366)), ([:book:](https://browse.arxiv.org/pdf/2312.02366.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02366.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02366)), ([:house:](https://huggingface.co/papers/2312.02366)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-general-purpose-vision-foundation)), ([:octocat:](https://github.com/mohammedsb/dinov2formedical)![GitHub Repo stars](https://img.shields.io/github/stars/mohammedsb/dinov2formedical?style=social))  |
| 12.4 | Aligning and Prompting Everything All at Once for Universal Visual Perception ([:x:](https://arxiv.org/abs/2312.02153)), ([:book:](https://browse.arxiv.org/pdf/2312.02153.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02153.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02153)), ([:house:](https://huggingface.co/papers/2312.02153)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/aligning-and-prompting-everything-all-at-once)), ([:octocat:](https://github.com/shenyunhang/ape)![GitHub Repo stars](https://img.shields.io/github/stars/shenyunhang/ape?style=social))  |
| 12.4 | Hulk: A Universal Knowledge Translator for Human-Centric Tasks ([:x:](https://arxiv.org/abs/2312.01697)), ([:book:](https://browse.arxiv.org/pdf/2312.01697.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.01697.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.01697)), ([:house:](https://huggingface.co/papers/2312.01697)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hulk-a-universal-knowledge-translator-for)), ([:octocat:](https://github.com/opengvlab/humanbench)![GitHub Repo stars](https://img.shields.io/github/stars/opengvlab/humanbench?style=social))  |
| 12.4 | [AI Alliance](https://thealliance.ai/) Launches as an International Community of Leading Technology Developers, Researchers, and Adopters Collaborating Together to Advance Open, Safe, Responsible AI (Meta [blog](https://ai.meta.com/blog/ai-alliance/)) | 
| 12.4 | Style Aligned Image Generation via Shared Attention ([project](https://style-aligned-gen.github.io/)), ([:x:](https://arxiv.org/abs/2312.02133)), ([:book:](https://browse.arxiv.org/pdf/2312.02133.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02133.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02133)), ([:house:](https://huggingface.co/papers/2312.02133)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/style-aligned-image-generation-via-shared)), ([:octocat:](https://github.com/google/style-aligned/)![GitHub Repo stars](https://img.shields.io/github/stars/google/style-aligned/?style=social))  |
| 12.4 | Data Management For Large Language Models: A Survey ([:x:](https://arxiv.org/abs/2312.01700)), ([:book:](https://browse.arxiv.org/pdf/2312.01700.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.01700.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.01700)), ([:house:](https://huggingface.co/papers/2312.01700)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/data-management-for-large-language-models-a)) |
| 12.4 | Merlin:Empowering Multimodal LLMs with Foresight Minds ([:x:](https://arxiv.org/abs/2312.00589)), ([:book:](https://browse.arxiv.org/pdf/2312.00589.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.00589.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.00589)), ([:house:](https://huggingface.co/papers/2312.00589)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/merlin-empowering-multimodal-llms-with)) |
| 12.4 | X-Adapter: Adding Universal Compatibility of Plugins for Upgraded Diffusion Model ([:x:](https://arxiv.org/abs/2312.02238)), ([:book:](https://browse.arxiv.org/pdf/2312.02238.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02238.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02238)), ([:house:](https://huggingface.co/papers/2312.02238)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/x-adapter-adding-universal-compatibility-of)) |
| 12.3 | Effectively Fine-tune to Improve Large Multimodal Models for Radiology Report Generation ([:x:](https://arxiv.org/abs/2312.01504)), ([:book:](https://browse.arxiv.org/pdf/2312.01504.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.01504.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.01504)), ([:house:](https://huggingface.co/papers/2312.01504)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/effectively-fine-tune-to-improve-large)) |
| 12.3 | DragVideo: Interactive Drag-style Video Editing ([:x:](https://arxiv.org/abs/2312.02216)), ([:book:](https://browse.arxiv.org/pdf/2312.02216.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02216.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02216)), ([:house:](https://huggingface.co/papers/2312.02216)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dragvideo-interactive-drag-style-video)), ([:octocat:](https://github.com/rickyskywalker/dragvideo-official)![GitHub Repo stars](https://img.shields.io/github/stars/rickyskywalker/dragvideo-official?style=social))  |
| 12.3 | ImageDream: Image-Prompt Multi-view Diffusion for 3D Generation ([:x:](https://arxiv.org/abs/2312.02201)), ([:book:](https://browse.arxiv.org/pdf/2312.02201.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02201.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02201)), ([:house:](https://huggingface.co/papers/2312.02201)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/imagedream-image-prompt-multi-view-diffusion)) |
| 12.3 | US artificial intelligence leader OpenAI applies for GPT-6, GPT-7 trademarks in China ([news](https://www.scmp.com/tech/big-tech/article/3243678/us-artificial-intelligence-leader-openai-applies-gpt-6-gpt-7-trademarks-china)) |
| 12.3 | Axiomatic Preference Modeling for Longform Question Answering ([:x:](https://arxiv.org/abs/2312.02206)), ([:book:](https://browse.arxiv.org/pdf/2312.02206.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02206.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02206)), ([:house:](https://huggingface.co/papers/2312.02206)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/axiomatic-preference-modeling-for-longform)) |
| 12.2 | StableDreamer: Taming Noisy Score Distillation Sampling for Text-to-3D ([:x:](https://arxiv.org/abs/2312.02189)), ([:book:](https://browse.arxiv.org/pdf/2312.02189.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02189.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02189)), ([:house:](https://huggingface.co/papers/2312.02189)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/stabledreamer-taming-noisy-score-distillation)) |
| 12.2 | Medical AI Tools Can Make Dangerous Mistakes. Can the Government Help Prevent Them? (WSJ [news](https://www.wsj.com/tech/ai/medical-ai-tools-can-make-dangerous-mistakes-can-the-government-help-prevent-them-b7cd8b35)) - ([archive](https://archive.is/lyxRv)) |
| 12.2 | Segment and Caption Anything ([:x:](https://arxiv.org/abs/2312.00869)), ([:book:](https://browse.arxiv.org/pdf/2312.00869.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.00869.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.00869)), ([:house:](https://huggingface.co/papers/2312.00869)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/segment-and-caption-anything)) |
| 12.2 | SeaLLMs -- Large Language Models for Southeast Asia ([:x:](https://arxiv.org/abs/2312.00738)), ([:book:](https://browse.arxiv.org/pdf/2312.00738.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.00738.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.00738)), ([:house:](https://huggingface.co/papers/2312.00738)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/seallms-large-language-models-for-southeast)), ([:octocat:](https://github.com/damo-nlp-sg/seallms)![GitHub Repo stars](https://img.shields.io/github/stars/damo-nlp-sg/seallms?style=social))  |
| 12.2 | VideoBooth: Diffusion-based Video Generation with Image Prompts ([:x:](https://arxiv.org/abs/2312.00777)), ([:book:](https://browse.arxiv.org/pdf/2312.00777.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.00777.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.00777)), ([:house:](https://huggingface.co/papers/2312.00777)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/videobooth-diffusion-based-video-generation)) |
| 12.2 | Mamba: Linear-Time Sequence Modeling with Selective State Spaces ([:x:](https://arxiv.org/abs/2312.00752)), ([:book:](https://browse.arxiv.org/pdf/2312.00752.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.00752.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.00752)), ([:house:](https://huggingface.co/papers/2312.00752)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mamba-linear-time-sequence-modeling-with)) |
| 12.2 | Beyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses ([:x:](https://arxiv.org/abs/2312.00763)), ([:book:](https://browse.arxiv.org/pdf/2312.00763.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.00763.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.00763)), ([:house:](https://huggingface.co/papers/2312.00763)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/beyond-chatbots-explorellm-for-structured)) |
| 12.1 | Rethinking FID: Towards a Better Evaluation Metric for Image Generation ([:x:](https://arxiv.org/abs/2401.09603)), ([:book:](https://browse.arxiv.org/pdf/2401.09603.pdf)), ([:paperclip:](https://arxiv.org/pdf/2401.09603.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2401.09603)), ([:house:](https://huggingface.co/papers/2401.09603)), ([HTML](https://browse.arxiv.org/html/2401.09603v1)), ([:eight_spoked_asterisk:]()) |
| 12.1 | Grounding Everything: Emerging Localization Properties in Vision-Language Transformers ([:x:](https://arxiv.org/abs/2312.00878)), ([:book:](https://browse.arxiv.org/pdf/2312.00878.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.00878.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.00878)), ([:house:](https://huggingface.co/papers/2312.00878)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/grounding-everything-emerging-localization)), ([:octocat:](https://github.com/walbouss/gem)![GitHub Repo stars](https://img.shields.io/github/stars/walbouss/gem?style=social))  |
| 12.1 | The Efficiency Spectrum of Large Language Models: An Algorithmic Survey ([:x:](https://arxiv.org/abs/2312.00678)), ([:book:](https://browse.arxiv.org/pdf/2312.00678.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.00678.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.00678)), ([:house:](https://huggingface.co/papers/2312.00678)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-efficiency-spectrum-of-large-language)), ([:octocat:](https://github.com/tding1/efficient-llm-survey)![GitHub Repo stars](https://img.shields.io/github/stars/tding1/efficient-llm-survey?style=social)) |
| 12.1 | An open letter to ChatGPT on its first birthday (CNN [news](https://edition.cnn.com/2023/12/01/media/chatgpt-first-birthday-open-letter/index.html)) |
| 12.1 | Explanatory Argument Extraction of Correct Answers in Resident Medical Exams ([:x:](https://arxiv.org/abs/2312.00567)), ([:book:](https://browse.arxiv.org/pdf/2312.00567.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.00567.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.00567)), ([:house:](https://huggingface.co/papers/2312.00567)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/explanatory-argument-extraction-of-correct)) |
| 12.1 | GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs ([:x:](https://arxiv.org/abs/2312.00093)), ([:book:](https://browse.arxiv.org/pdf/2312.00093.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.00093.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.00093)), ([:house:](https://huggingface.co/papers/2312.00093)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/graphdreamer-compositional-3d-scene-synthesis)) |
| 12.1 | StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter ([:x:](https://arxiv.org/abs/2312.00330)), ([:book:](https://browse.arxiv.org/pdf/2312.00330.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.00330.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.00330)), ([:house:](https://huggingface.co/papers/2312.00330)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/stylecrafter-enhancing-stylized-text-to-video)), ([:octocat:](https://github.com/GongyeLiu/StyleCrafter)![GitHub Repo stars](https://img.shields.io/github/stars/GongyeLiu/StyleCrafter?style=social))  |
| 12.1 | Dolphins: Multimodal Language Model for Driving ([:x:](https://arxiv.org/abs/2312.00438)), ([:book:](https://browse.arxiv.org/pdf/2312.00438.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.00438.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.00438)), ([:house:](https://huggingface.co/papers/2312.00438)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dolphins-multimodal-language-model-for)) |
| 12.1 | DREAM: Diffusion Rectification and Estimation-Adaptive Models ([:x:](https://arxiv.org/abs/2312.00210)), ([:book:](https://browse.arxiv.org/pdf/2312.00210.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.00210.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.00210)), ([:house:](https://huggingface.co/papers/2312.00210)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dream-diffusion-rectification-and-estimation)) |
| 12.1 | Instruction-tuning Aligns LLMs to the Human Brain ([:x:](https://arxiv.org/abs/2312.00575)), ([:book:](https://browse.arxiv.org/pdf/2312.00575.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.00575.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.00575)), ([:house:](https://huggingface.co/papers/2312.00575)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/instruction-tuning-aligns-llms-to-the-human)) |
| 12.1 | Text-Guided 3D Face Synthesis -- From Generation to Editing ([:x:](https://arxiv.org/abs/2312.00375)), ([:book:](https://browse.arxiv.org/pdf/2312.00375.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.00375.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.00375)), ([:house:](https://huggingface.co/papers/2312.00375)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/text-guided-3d-face-synthesis-from-generation)) |
| 12.1 | FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting ([:x:](https://arxiv.org/abs/2312.00451)), ([:book:](https://browse.arxiv.org/pdf/2312.00451.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.00451.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.00451)), ([:house:](https://huggingface.co/papers/2312.00451)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fsgs-real-time-few-shot-view-synthesis-using)) |
| 12.1 | Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering ([:x:](https://arxiv.org/abs/2312.00109)), ([:book:](https://browse.arxiv.org/pdf/2312.00109.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.00109.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.00109)), ([:house:](https://huggingface.co/papers/2312.00109)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/scaffold-gs-structured-3d-gaussians-for-view)) |
| 12.1 | PyNeRF: Pyramidal Neural Radiance Fields ([:x:](https://arxiv.org/abs/2312.00252)), ([:book:](https://browse.arxiv.org/pdf/2312.00252.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.00252.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.00252)), ([:house:](https://huggingface.co/papers/2312.00252)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pynerf-pyramidal-neural-radiance-fields-1)) |
| 11.30 | Tech predictions for 2024 and beyond ([blog](https://www.allthingsdistributed.com/2023/11/tech-predictions-for-2024-and-beyond.html)) |
| 11.30 | Meta - Audiobox: Generating audio from voice and natural language prompts ([blog](https://ai.meta.com/blog/audiobox-generating-audio-voice-natural-language-prompts)) |
| 11.30 | Will Generative Artificial Intelligence Deliver on Its Promise in Health Care? (JAMA [doi:10.1001/jama.2023.25054](https://jamanetwork.com/journals/jama/article-abstract/2812615)) |
| 11.30 | Generative AI could revolutionize health care — but not if control is ceded to big tech (Nature [doi: https://doi.org/10.1038/d41586-023-03803-y](https://www.nature.com/articles/d41586-023-03803-y)) |
| 11.30 | ChatGPT one year on: who is using it, how and why? (Nature [https://doi.org/10.1038/d41586-023-03798-6](https://www.nature.com/articles/d41586-023-03798-6)) |
| 11.30 | RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance ([:x:](https://arxiv.org/abs/2311.18681)), ([:book:](https://browse.arxiv.org/pdf/2311.18681.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.18681.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.18681)), ([:house:](https://huggingface.co/papers/2311.18681)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/radialog-a-large-vision-language-model-for)), ([:octocat:](https://github.com/chantalmp/radialog)![GitHub Repo stars](https://img.shields.io/github/stars/chantalmp/radialog?style=social))  |
| 11.30 | X-Dreamer: Creating High-quality 3D Content by Bridging the Domain Gap Between Text-to-2D and Text-to-3D Generation ([:x:](https://arxiv.org/abs/2312.00085)), ([:book:](https://browse.arxiv.org/pdf/2312.00085.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.00085.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.00085)), ([:house:](https://huggingface.co/papers/2312.00085)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/x-dreamer-creating-high-quality-3d-content-by)) |
| 11.30 | MoMask: Generative Masked Modeling of 3D Human Motions ([:x:](https://arxiv.org/abs/2312.00063)), ([:book:](https://browse.arxiv.org/pdf/2312.00063.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.00063.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.00063)), ([:house:](https://huggingface.co/papers/2312.00063)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/momask-generative-masked-modeling-of-3d-human)) |
| 11.30 | HiFi Tuner: High-Fidelity Subject-Driven Fine-Tuning for Diffusion Models ([:x:](https://arxiv.org/abs/2312.00079)), ([:book:](https://browse.arxiv.org/pdf/2312.00079.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.00079.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.00079)), ([:house:](https://huggingface.co/papers/2312.00079)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hifi-tuner-high-fidelity-subject-driven-fine)) |
| 11.30 | Six ways large language models are changing healthcare (Nature medicine [https://doi.org/10.1038/s41591-023-02700-1](https://www.nature.com/articles/s41591-023-02700-1)) |
| 11.30 | Towards Accurate Differential Diagnosis with Large Language Models ([:x:](https://arxiv.org/abs/2312.00164)), ([:book:](https://browse.arxiv.org/pdf/2312.00164.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.00164.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.00164)), ([:house:](https://huggingface.co/papers/2312.00164)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-accurate-differential-diagnosis-with)) |
| 11.30 | Generative AI could revolutionize health care — but not if control is ceded to big tech (Nature [doi: https://doi.org/10.1038/d41586-023-03803-y](https://www.nature.com/articles/d41586-023-03803-y)) |
| 11.30 | Discover, download, and run local LLMs - ([LM Studio](https://lmstudio.ai/)) |
| 11.30 | A timeline of Sam Altman’s firing from OpenAI — and the fallout ([news](https://techcrunch.com/2023/11/29/a-timeline-of-sam-altmans-firing-from-openai-and-the-fallout/)) |
| 11.30 | Synthetic data: Anthropic’s CAI, from fine-tuning to pretraining, OpenAI’s Superalignment, tips, types, and open examples ([blog](https://www.interconnects.ai/p/llm-synthetic-data)) |
| 11.29 | Deepfakes, Misinformation, and Disinformation in the Era of Frontier AI, Generative AI, and Large AI Models ([:x:](https://arxiv.org/abs/2311.17394)), ([:book:](https://browse.arxiv.org/pdf/2311.17394.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.17394.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.17394)), ([:house:](https://huggingface.co/papers/2311.17394)), ([:eight_spoked_asterisk:]()) |
| 11.29 | Are we going MAD? Benchmarking Multi-Agent Debate between Language Models for Medical Q&A ([:x:](https://arxiv.org/abs/2311.17371)), ([:book:](https://browse.arxiv.org/pdf/2311.17371.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.17371.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.17371)), ([:house:](https://huggingface.co/papers/2311.17371)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/are-we-going-mad-benchmarking-multi-agent)) |
| 11.29 | Welcome to a new world of work with Amazon Q - ([tweet](https://twitter.com/awscloud/status/1729574944336400481)), ([blog](https://aws.amazon.com/ko/q/)) |
| 11.29 | Scaling deep learning for materials discovery (Nature [https://doi.org/10.1038/s41586-023-06735-9](https://www.nature.com/articles/s41586-023-06735-9)) |
| 11.29 | Millions of new materials discovered with deep learning (Google DeepMind [blog](https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/)) |
| 11.29 | OpenAI [Cookbook](https://cookbook.openai.com/) |
| 11.29 | Announcing ElevenLabs Grants! ([tweet](https://twitter.com/elevenlabsio/status/1729570805388853701)), ([site](https://elevenlabs.io/grants)) |
| 11.29 | SDXL Turbo: A real-time text-to-image generation model ([tweet](https://twitter.com/StabilityAI/status/1729589510155948074)), ([news](https://stability.ai/news/stability-ai-sdxl-turbo)) |
| 11.29 |Training Chain-of-Thought via Latent-Variable Inference ([:x:](https://arxiv.org/abs/2312.02179)), ([:book:](https://browse.arxiv.org/pdf/2312.02179.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.02179.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.02179)), ([:house:](https://huggingface.co/papers/2312.02179)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/training-chain-of-thought-via-latent-variable-1)) |
| 11.28 | The Falcon Series of Open Language Models ([:x:](https://arxiv.org/abs/2311.16867)), ([:book:](https://browse.arxiv.org/pdf/2311.16867.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.16867.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.16867)), ([:house:](https://huggingface.co/papers/2311.16867)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-falcon-series-of-open-language-models)) |
| 11.28 | AMA issues new principles for AI development, deployment & use ([press](https://www.ama-assn.org/press-center/press-releases/ama-issues-new-principles-ai-development-deployment-use)), ([PDF](https://www.ama-assn.org/system/files/ama-ai-principles.pdf)) |
| 11.28 | Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation ([project](https://humanaigc.github.io/animate-anyone/)), ([:x:](https://arxiv.org/abs/2311.17117)), ([:book:](https://browse.arxiv.org/pdf/2311.17117.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.17117.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.17117)), ([:house:](https://huggingface.co/papers/2311.17117)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/animate-anyone-consistent-and-controllable)), ([:octocat:](https://github.com/HumanAIGC/AnimateAnyone)![GitHub Repo stars](https://img.shields.io/github/stars/HumanAIGC/AnimateAnyone?style=social))  |
| 11.28 | Power Hungry Processing: Watts Driving the Cost of AI Deployment? ([:x:](https://arxiv.org/abs/2311.16863)), ([:book:](https://browse.arxiv.org/pdf/2311.16863.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.16863.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.16863)), ([:house:](https://huggingface.co/papers/2311.16863)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/power-hungry-processing-watts-driving-the)) |
| 11.28 | Graph Prompt Learning: A Comprehensive Survey and Beyond ([:x:](https://arxiv.org/abs/2311.16534)), ([:book:](https://browse.arxiv.org/pdf/2311.16534.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.16534.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.16534)), ([:house:](https://huggingface.co/papers/2311.16534)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/graph-prompt-learning-a-comprehensive-survey)), ([:octocat:](https://github.com/sheldonresearch/ProG)![GitHub Repo stars](https://img.shields.io/github/stars/sheldonresearch/ProG?style=social))  |
| 11.28 | ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up? ([:x:](https://arxiv.org/abs/2311.16989)), ([:book:](https://browse.arxiv.org/pdf/2311.16989.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.16989.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.16989)), ([:house:](https://huggingface.co/papers/2311.16989)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chatgpt-s-one-year-anniversary-are-open)) |
| 11.28 | SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models ([project](https://guoyww.github.io/projects/SparseCtrl/)), ([:x:](https://arxiv.org/abs/2311.16933)), ([:book:](https://browse.arxiv.org/pdf/2311.16933.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.16933.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.16933)), ([:house:](https://huggingface.co/papers/2311.16933)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sparsectrl-adding-sparse-controls-to-text-to)) |
| 11.28 | Surf-D: High-Quality Surface Generation for Arbitrary Topologies using Diffusion Models ([project](https://yzmblog.github.io/projects/SurfD/)), ([:x:](https://arxiv.org/abs/2311.17050)), ([:book:](https://browse.arxiv.org/pdf/2311.17050.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.17050.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.17050)), ([:house:](https://huggingface.co/papers/2311.17050)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/surf-d-high-quality-surface-generation-for)) |
| 11.28 | Introducing Pika 1.0, the idea-to-video platform that brings your creativity to life ([tweet](https://twitter.com/pika_labs/status/1729510078959497562)), ([site](https://pika.art/login)) |
| 11.28 | Adversarial Diffusion Distillation ([:x:](https://arxiv.org/abs/2311.17042)), ([:book:](https://browse.arxiv.org/pdf/2311.17042.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.17042.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.17042)), ([:house:](https://huggingface.co/papers/2311.17042)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/adversarial-diffusion-distillation)), ([:octocat:](https://github.com/stability-ai/generative-models)![GitHub Repo stars](https://img.shields.io/github/stars/stability-ai/generative-models?style=social))  |
| 11.28 | MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI ([project](https://mmmu-benchmark.github.io/)), ([:x:](https://arxiv.org/abs/2311.16502)), ([:book:](https://browse.arxiv.org/pdf/2311.15504.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.16502.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.16502)), ([:house:](https://huggingface.co/papers/2311.16502)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mmmu-a-massive-multi-discipline-multimodal)), ([:octocat:](https://github.com/MMMU-Benchmark/MMMU)![GitHub Repo stars](https://img.shields.io/github/stars/MMMU-Benchmark/MMMU?style=social)), ([Dataset](https://huggingface.co/datasets/MMMU/MMMU)) |
| 11.28 | LEDITS++: Limitless Image Editing using Text-to-Image Models ([:x:](https://arxiv.org/abs/2311.16711)), ([:book:](https://browse.arxiv.org/pdf/2311.16711.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.16711.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.16711)), ([:house:](https://huggingface.co/papers/2311.16711)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ledits-limitless-image-editing-using-text-to)) |
| 11.28 | MEDITRON-70B: Scaling Medical Pretraining for Large Language Models ([:x:](https://arxiv.org/abs/2311.16079)), ([:book:](https://browse.arxiv.org/pdf/2311.16079.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.16079.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.16079)), ([:house:](https://huggingface.co/papers/2311.16079)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/meditron-70b-scaling-medical-pretraining-for)), ([:octocat:](https://github.com/epfllm/meditron)![GitHub Repo stars](https://img.shields.io/github/stars/epfllm/meditron?style=social))  |
| 11.28 | Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine ([:x:](https://arxiv.org/abs/2311.16452)), ([:book:](https://browse.arxiv.org/pdf/2311.16452.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.16452.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.16452)), ([:house:](https://huggingface.co/papers/2311.16452)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/can-generalist-foundation-models-outcompete)) |
| 11.28 | Effective prompting for Large Multimodal Models like GPT-4 Vision or LLaVA. 🔥 ([:octocat:](https://github.com/roboflow/multimodal-maestro)![GitHub Repo stars](https://img.shields.io/github/stars/roboflow/multimodal-maestro?style=social)) 
| 11.28 | The Power of Prompting ([blog](https://www.microsoft.com/en-us/research/blog/the-power-of-prompting/)) |
| 11.27 | Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models ([:x:](https://arxiv.org/abs/2311.16103)), ([:book:](https://browse.arxiv.org/pdf/2311.16103.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.16103.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.16103)), ([:house:](https://huggingface.co/papers/2311.16103)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/video-bench-a-comprehensive-benchmark-and)), ([:octocat:](https://github.com/pku-yuangroup/video-bench)![GitHub Repo stars](https://img.shields.io/github/stars/pku-yuangroup/video-bench?style=social))  |
| 11.27 | WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large Language Models ([:x:](https://arxiv.org/abs/2311.15930)), ([:book:](https://browse.arxiv.org/pdf/2311.15930.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.15930.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.15930)), ([:house:](https://huggingface.co/papers/2311.15930)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/worldsense-a-synthetic-benchmark-for-grounded)) |
| 11.27 | MeshGPT: Generating Triangle Meshes with Decoder-Only Transformers ([project](https://nihalsid.github.io/mesh-gpt/)), ([:x:](https://arxiv.org/abs/2312.15475)), ([:book:](https://browse.arxiv.org/pdf/2312.15475.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.15475.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.15475)), ([:house:](https://huggingface.co/papers/2312.15475)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/meshgpt-generating-triangle-meshes-with)) |
| 11.27 | RO-LLaMA: Generalist LLM for Radiation Oncology via Noise Augmentation and Consistency Regularization ([:x:](https://arxiv.org/abs/2311.15876)), ([:book:](https://browse.arxiv.org/pdf/2311.15876.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.15876.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.15876)), ([:house:](https://huggingface.co/papers/2311.15876)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ro-llama-generalist-llm-for-radiation)) |
| 11.27 | Applications of Large Scale Foundation Models for Autonomous Driving ([:x:](https://arxiv.org/abs/2311.12144)), ([:book:](https://browse.arxiv.org/pdf/2311.12144.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.12144.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.12144)), ([:house:](https://huggingface.co/papers/2311.12144)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/applications-of-large-scale-foundation-models)) |
| 11.27 | Building the Future of Responsible AI: A Reference Architecture for Designing Large Language Model based Agents ([:x:](https://arxiv.org/abs/2311.13148)), ([:book:](https://browse.arxiv.org/pdf/2311.13148.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.13148.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.13148)), ([:house:](https://huggingface.co/papers/2311.13148)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/building-the-future-of-responsible-ai-a)) |
| 11.27 | ChatGPT’s One-Year Anniversary: Generative AI’s Breakout Year ([blog](https://www.globalxetfs.com/chatgpts-one-year-anniversary-generative-ais-breakout-year/)) |
| 11.27 | GPT-4’s potential in shaping the future of radiology ([tweet](https://twitter.com/gdb/status/1729483568827744673)), ([blog](https://www.microsoft.com/en-us/research/blog/gpt-4s-potential-in-shaping-the-future-of-radiology/)) |
| 11.27 | Automatic Hallucination detection with SelfCheckGPT NLI ([blog](https://huggingface.co/blog/dhuynh95/automatic-hallucination-detection)) |
| 11.25 | Walking a Tightrope -- Evaluating Large Language Models in High-Risk Domains ([:x:](https://arxiv.org/abs/2311.14966)), ([:book:](https://browse.arxiv.org/pdf/2311.14966.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.14966.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.14966)), ([:house:](https://huggingface.co/papers/2311.14966)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/walking-a-tightrope-evaluating-large-language)) |
| 11.25 | LLM-Assisted Code Cleaning For Training Accurate Code Generators ([:x:](https://arxiv.org/abs/2311.14904)), ([:book:](https://browse.arxiv.org/pdf/2311.14904.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.14904.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.14904)), ([:house:](https://huggingface.co/papers/2311.14904)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llm-assisted-code-cleaning-for-training)) |
| 11.23 | MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model ([project](https://showlab.github.io/magicanimate/)), ([:x:](https://arxiv.org/abs/2311.16498)), ([:book:](https://browse.arxiv.org/pdf/2311.16498.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.16498.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.16498)), ([:house:](https://huggingface.co/papers/2311.16498)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/magicanimate-temporally-consistent-human)), ([:octocat:](https://github.com/magic-research/magic-animate)![GitHub Repo stars](https://img.shields.io/github/stars/magic-research/magic-animate?style=social)), ([demo](https://huggingface.co/spaces/zcxu-eric/magicanimate)) |
| 11.23 | Challenges of Large Language Models for Mental Health Counseling ([:x:](https://arxiv.org/abs/2311.13857)), ([:book:](https://browse.arxiv.org/pdf/2311.13857.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.13857.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.13857)), ([:house:](https://huggingface.co/papers/2311.13857)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/challenges-of-large-language-models-for)) |
| 11.23 | MLLM-Bench, Evaluating Multi-modal LLMs using GPT-4V ([:x:](https://arxiv.org/abs/2311.13951)), ([:book:](https://browse.arxiv.org/pdf/2311.13951.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.13951.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.13951)), ([:house:](https://huggingface.co/papers/2311.13951)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mllm-bench-evaluating-multi-modal-llms-using)), ([:octocat:](https://github.com/freedomintelligence/mllm-bench)![GitHub Repo stars](https://img.shields.io/github/stars/freedomintelligence/mllm-bench?style=social))  |
| 11.23 | ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs ([:x:](https://arxiv.org/abs/2311.13600)), ([:book:](https://browse.arxiv.org/pdf/2311.13600.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.13600.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.13600)), ([:house:](https://huggingface.co/papers/2311.13600)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ziplora-any-subject-in-any-style-by)) |
| 11.23 | Visual In-Context Prompting ([:x:](https://arxiv.org/abs/2311.13601)), ([:book:](https://browse.arxiv.org/pdf/2311.13601.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.13601.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.13601)), ([:house:](https://huggingface.co/papers/2311.13601)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/visual-in-context-prompting)), ([:octocat:](https://github.com/ux-decoder/dinov)![GitHub Repo stars](https://img.shields.io/github/stars/ux-decoder/dinov?style=social))  |
| 11.22 | Positional Description Matters for Transformers Arithmetic ([:x:](https://arxiv.org/abs/2311.14737)), ([:book:](https://browse.arxiv.org/pdf/2311.14737.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.14737.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.14737)), ([:house:](https://huggingface.co/papers/2311.14737)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/positional-description-matters-for)) |
| 11.22 | Enhancing Summarization Performance through Transformer-Based Prompt Engineering in Automated Medical Reporting ([:x:](https://arxiv.org/abs/2311.13274)), ([:book:](https://browse.arxiv.org/pdf/2311.13274.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.13274.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.13274)), ([:house:](https://huggingface.co/papers/2311.13274)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/enhancing-summarization-performance-through)) |
| 11.22 | OpenAI chaos: A timeline of firings, interim CEOs, re-hirings and other twists ([blog](https://www.axios.com/2023/11/22/openai-microsoft-sam-altman-ceo-chaos-timeline)) |
| 11.22 | Here's a timeline of the OpenAI saga with CEO Sam Altman (mashable [news](https://mashable.com/article/openai-sam-altman-saga-timeline)) |
| 11.22 | A timeline of Sam Altman's firing and dramatic return to OpenAI (Reuters [news](https://www.reuters.com/technology/openai-ouster-microsoft-ai-research-ceo-sam-altmans-tumultuous-weekend-2023-11-20/)) |
| 11.22 | Sam Altman to return as CEO of OpenAI ([news](https://www.theguardian.com/technology/2023/nov/22/sam-altman-openai-ceo-return-board-chatgpt)) |
| 11.22 | DiffusionMat: Alpha Matting as Sequential Refinement Learning ([project](https://cnnlstm.github.io/DiffusionMat)), ([:x:](https://arxiv.org/abs/2311.13535)), ([:book:](https://browse.arxiv.org/pdf/2311.13535.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.13535.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.13535)), ([:house:](https://huggingface.co/papers/2311.13535)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/diffusionmat-alpha-matting-as-sequential)) |
| 11.22 | GAIA: a benchmark for General AI Assistants ([:x:](https://arxiv.org/abs/2311.12983)), ([:book:](https://browse.arxiv.org/pdf/2311.12983.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.12983.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.12983)), ([:house:](https://huggingface.co/papers/2311.12983)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gaia-a-benchmark-for-general-ai-assistants)) |
| 11.22 | FusionFrames: Efficient Architectural Aspects for Text-to-Video Generation Pipeline ([:x:](https://arxiv.org/abs/2311.13073)), ([:book:](https://browse.arxiv.org/pdf/2311.13073.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.13073.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.13073)), ([:house:](https://huggingface.co/papers/2311.13073)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fusionframes-efficient-architectural-aspects)), ([:octocat:](https://github.com/ai-forever/kandinskyvideo)![GitHub Repo stars](https://img.shields.io/github/stars/ai-forever/kandinskyvideo?style=social))  |
| 11.22 | LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes ([:x:](https://arxiv.org/abs/2311.13384)), ([:book:](https://browse.arxiv.org/pdf/2311.13384.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.13384.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.13384)), ([:house:](https://huggingface.co/papers/2311.13384)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/luciddreamer-domain-free-generation-of-3d)) |
| 11.22 | Diffusion Model Alignment Using Direct Preference Optimization ([:x:](https://arxiv.org/abs/2311.12908)), ([:book:](https://browse.arxiv.org/pdf/2311.12908.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.12908.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.12908)), ([:house:](https://huggingface.co/papers/2311.12908)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/diffusion-model-alignment-using-direct)) |
| 11.22 | Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model ([:x:](https://arxiv.org/abs/2311.13231)), ([:book:](https://browse.arxiv.org/pdf/2311.13231.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.13231.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.13231)), ([:house:](https://huggingface.co/papers/2311.13231)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/using-human-feedback-to-fine-tune-diffusion)), ([:octocat:](https://github.com/yk7333/d3po)![GitHub Repo stars](https://img.shields.io/github/stars/yk7333/d3po?style=social))  |
| 11.22 | PG-Video-LLaVA: Pixel Grounding Large Video-Language Models ([:x:](https://arxiv.org/abs/2311.13435)), ([:book:](https://browse.arxiv.org/pdf/2311.13435.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.13435.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.13435)), ([:house:](https://huggingface.co/papers/2311.13435)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pg-video-llava-pixel-grounding-large-video)), ([:octocat:](https://github.com/mbzuai-oryx/video-llava)![GitHub Repo stars](https://img.shields.io/github/stars/mbzuai-oryx/video-llava?style=social))  |
| 11.22 | Diffusion360: Seamless 360 Degree Panoramic Image Generation based on Diffusion Models ([:x:](https://arxiv.org/abs/2311.13141)), ([:book:](https://browse.arxiv.org/pdf/2311.13141.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.13141.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.13141)), ([:house:](https://huggingface.co/papers/2311.13141)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/diffusion360-seamless-360-degree-panoramic)), ([:octocat:](https://github.com/archerfmy/sd-t2i-360panoimage)![GitHub Repo stars](https://img.shields.io/github/stars/archerfmy/sd-t2i-360panoimage?style=social))  |
| 11.22 | SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering ([:x:](https://arxiv.org/abs/2311.12775)), ([:book:](https://browse.arxiv.org/pdf/2311.12775.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.12775.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.12775)), ([:house:](https://huggingface.co/papers/2311.12775)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sugar-surface-aligned-gaussian-splatting-for)) |
| 11.22 | ChatGPT generates fake data set to support scientific hypothesis (Nature [doi: https://doi.org/10.1038/d41586-023-03635-w](https://www.nature.com/articles/d41586-023-03635-w)), ([PDF](https://www.nature.com/articles/d41586-023-03635-w.pdf)) |
| 11.21 | Prompting Frameworks for Large Language Models: A Survey ([:x:](https://arxiv.org/abs/2311.12785)), ([:book:](https://browse.arxiv.org/pdf/2311.12785.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.12785.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.12785)), ([:house:](https://huggingface.co/papers/2311.12785)), ([:eight_spoked_asterisk:](https://cs.paperswithcode.com/paper/prompting-frameworks-for-large-language)), ([:octocat:](https://github.com/lxx0628/prompting-framework-survey)![GitHub Repo stars](https://img.shields.io/github/stars/lxx0628/prompting-framework-survey?style=social)), ([SS](https://www.semanticscholar.org/paper/Prompting-Frameworks-for-Large-Language-Models%3A-A-Liu-Wang/5eb5cad49de8a1f386f3dd127de2046eaa27ea5c))  |
| 11.21 | It’s Time For ‘Nutrition Labels’ In Artificial Intelligence (Forbes [news](https://www.forbes.com/sites/greglicholai/2023/11/21/its-time-for-nutrition-labels-in-artificial-intelligence/)) |
| 11.21 | From Classification to Clinical Insights: Towards Analyzing and Reasoning About Mobile and Behavioral Health Data With Large Language Models ([:x:](https://arxiv.org/abs/2311.13063)), ([:book:](https://browse.arxiv.org/pdf/2311.13063.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.13063.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.13063)), ([:house:](https://huggingface.co/papers/2311.13063)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/from-classification-to-clinical-insights)) |
| 11.21 | ALPHA: AnomaLous Physiological Health Assessment Using Large Language Models ([:x:](https://arxiv.org/abs/2311.12524)), ([:book:](https://browse.arxiv.org/pdf/2311.12524.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.12524.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.12524)), ([:house:](https://huggingface.co/papers/2311.12524)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/alpha-anomalous-physiological-health)), ([:octocat:](https://github.com/mcjacktang/llm-healthassistant)![GitHub Repo stars](https://img.shields.io/github/stars/mcjacktang/llm-healthassistant?style=social))  |
| 11.21 | HierSpeech++: Bridging the Gap between Semantic and Acoustic Representation of Speech by Hierarchical Variational Inference for Zero-shot Speech Synthesis ([:x:](https://arxiv.org/abs/2311.12454)), ([:book:](https://browse.arxiv.org/pdf/2311.12454.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.12454.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.12454)), ([:house:](https://huggingface.co/papers/2311.12454)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hierspeech-bridging-the-gap-between-semantic)), ([:octocat:](https://github.com/sh-lee-prml/hierspeechpp)![GitHub Repo stars](https://img.shields.io/github/stars/sh-lee-prml/hierspeechpp?style=social))  |
| 11.21 | PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics ([:x:](https://arxiv.org/abs/2311.12198)), ([:book:](https://browse.arxiv.org/pdf/2311.12198.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.12198.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.12198)), ([:house:](https://huggingface.co/papers/2311.12198)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/physgaussian-physics-integrated-3d-gaussians)) |
| 11.21 | NeuroPrompts: An Adaptive Framework to Optimize Prompts for Text-to-Image Generation ([:x:](https://arxiv.org/abs/2311.12229)), ([:book:](https://browse.arxiv.org/pdf/2311.12229.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.12229.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.12229)), ([:house:](https://huggingface.co/papers/2311.12229)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/neuroprompts-an-adaptive-framework-to)) |
| 11.21 | Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models ([:x:](https://arxiv.org/abs/2311.12092)), ([:book:](https://browse.arxiv.org/pdf/2311.12092.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.12092.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.12092)), ([:house:](https://huggingface.co/papers/2311.12092)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/concept-sliders-lora-adaptors-for-precise)), ([:octocat:](https://github.com/rohitgandikota/sliders)![GitHub Repo stars](https://img.shields.io/github/stars/rohitgandikota/sliders?style=social))  |
| 11.21 | PF-LRM: Pose-Free Large Reconstruction Model for Joint Pose and Shape Prediction ([:x:](https://arxiv.org/abs/2311.12024)), ([:book:](https://browse.arxiv.org/pdf/2311.12024.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.12024.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.12024)), ([:house:](https://huggingface.co/papers/2311.12024)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pf-lrm-pose-free-large-reconstruction-model)) |
| 11.21 | GPT4Motion: Scripting Physical Motions in Text-to-Video Generation via Blender-Oriented GPT Planning ([:x:](https://arxiv.org/abs/2311.12631)), ([:book:](https://browse.arxiv.org/pdf/2311.12631.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.12631.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.12631)), ([:house:](https://huggingface.co/papers/2311.12631)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gpt4motion-scripting-physical-motions-in-text)) |
| 11.21 | Accuracy of ChatGPT, Google Bard, and Microsoft Bing for Simplifying Radiology Reports (RSNA [ https://doi.org/10.1148/radiol.232561](https://pubs.rsna.org/doi/10.1148/radiol.232561)), ([PDF](https://pubs.rsna.org/doi/epdf/10.1148/radiol.232561)) |
| 11.21 | System 2 Attention (is something you might need too) ([:x:](https://arxiv.org/abs/2311.11829)), ([:book:](https://browse.arxiv.org/pdf/2311.11829.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.11829.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.11829)), ([:house:](https://huggingface.co/papers/2311.11829)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/system-2-attention-is-something-you-might)) |
| 11.21 | GPQA: A Graduate-Level Google-Proof Q&A Benchmark ([:x:](https://arxiv.org/abs/2311.12022)), ([:book:](https://browse.arxiv.org/pdf/2311.12022.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.12022.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.12022)), ([:house:](https://huggingface.co/papers/2311.12022)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gpqa-a-graduate-level-google-proof-q-a)), ([:octocat:](https://github.com/idavidrein/gpqa)![GitHub Repo stars](https://img.shields.io/github/stars/idavidrein/gpqa?style=social))  |
| 11.21 | GPT-4V(ision) for Robotics: Multimodal Task Planning from Human Demonstration ([:x:](https://arxiv.org/abs/2311.12015)), ([:book:](https://browse.arxiv.org/pdf/2311.12015.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.12015.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.12015)), ([:house:](https://huggingface.co/papers/2311.12015)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gpt-4v-ision-for-robotics-multimodal-task)) |
| 11.20 | Assessing Prompt Injection Risks in 200+ Custom GPTs ([:x:](https://arxiv.org/abs/2311.11538)), ([:book:](https://browse.arxiv.org/pdf/2311.11538.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.11538.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.11538)), ([:house:](https://huggingface.co/papers/2311.11538)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/assessing-prompt-injection-risks-in-200)) |
| 11.20 | Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey ([:x:](https://arxiv.org/abs/2311.12351)), ([:book:](https://browse.arxiv.org/pdf/2311.12351.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.12351.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.12351)), ([:house:](https://huggingface.co/papers/2311.12351)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/advancing-transformer-architecture-in-long)), ([:octocat:](https://github.com/strivin0311/long-llms-learning)![GitHub Repo stars](https://img.shields.io/github/stars/strivin0311/long-llms-learning?style=social))  |
| 11.20 | Sam Altman to Join Microsoft Following OpenAI Ouster (WSJ [news](https://www.wsj.com/tech/ai/openai-leadership-hangs-in-balance-as-sam-altmans-counte-rebellion-gains-steam-47276fa8)) |
| 11.20 | MultiLoRA: Democratizing LoRA for Better Multi-Task Learning ([:x:](https://arxiv.org/abs/2311.11501)), ([:book:](https://browse.arxiv.org/pdf/2311.11501.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.11501.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.11501)), ([:house:](https://huggingface.co/papers/2311.11501)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/multilora-democratizing-lora-for-better-multi)) |
| 11.19 | Meta Prompting for AGI Systems ([:x:](https://arxiv.org/abs/2311.11482)), ([:book:](https://browse.arxiv.org/pdf/2311.11482.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.11482.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.11482)), ([:house:](https://huggingface.co/papers/2311.11482)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/meta-prompting-for-agi-systems)), ([:octocat:](https://github.com/meta-prompting/meta-prompting)![GitHub Repo stars](https://img.shields.io/github/stars/meta-prompting/meta-prompting?style=social))  |
| 11.19 | M^{2}UGen: Multi-modal Music Understanding and Generation with the Power of Large Language Models ([:x:](https://arxiv.org/abs/2311.11255)), ([:book:](https://browse.arxiv.org/pdf/2311.11255.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.11255.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.11255)), ([:house:](https://huggingface.co/papers/2311.11255)), ([:eight_spoked_asterisk:]()) |
| 11.19 | LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching ([:x:](https://arxiv.org/abs/2311.11284)), ([:book:](https://browse.arxiv.org/pdf/2311.11284.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.11284.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.11284)), ([:house:](https://huggingface.co/papers/2311.11284)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/luciddreamer-towards-high-fidelity-text-to-3d)), ([:octocat:](https://github.com/envision-research/luciddreamer)![GitHub Repo stars](https://img.shields.io/github/stars/envision-research/luciddreamer?style=social))  |
| 11.19 | AutoStory: Generating Diverse Storytelling Images with Minimal Human Effort ([:x:](https://arxiv.org/abs/2311.11243)), ([:book:](https://browse.arxiv.org/pdf/2311.11243.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.11243.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.11243)), ([:house:](https://huggingface.co/papers/2311.11243)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/autostory-generating-diverse-storytelling)) |
| 11.19 | TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems ([:x:](https://arxiv.org/abs/2311.11315)), ([:book:](https://browse.arxiv.org/pdf/2311.11315.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.11315.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.11315)), ([:house:](https://huggingface.co/papers/2311.11315)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tptu-v2-boosting-task-planning-and-tool-usage)) |
| 11.18 | Designing Interpretable ML System to Enhance Trustworthy AI in Healthcare: A Systematic Review of the Last Decade to A Proposed Robust Framework ([:x:](https://arxiv.org/abs/2311.11055)), ([:book:](https://browse.arxiv.org/pdf/2311.11055.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.11055.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.11055)), ([:house:](https://huggingface.co/papers/2311.11055)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/designing-interpretable-ml-system-to-enhance)) |
| 11.18 | MagicDance: Realistic Human Dance Video Generation with Motions & Facial Expressions Transfer ([:x:](https://arxiv.org/abs/2311.12052)), ([:book:](https://browse.arxiv.org/pdf/2311.12052.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.12052.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.12052)), ([:house:](https://huggingface.co/papers/2311.12052)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/magicdance-realistic-human-dance-video)), ([:octocat:](https://github.com/boese0601/magicdance)![GitHub Repo stars](https://img.shields.io/github/stars/boese0601/magicdance?style=social))  |
| 11.18 | Make Pixels Dance: High-Dynamic Video Generation ([:x:](https://arxiv.org/abs/2311.10982)), ([:book:](https://browse.arxiv.org/pdf/2311.10982.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10982.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10982)), ([:house:](https://huggingface.co/papers/2311.10982)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/make-pixels-dance-high-dynamic-video)) |
| 11.18 | Orca 2: Teaching Small Language Models How to Reason ([:x:](https://arxiv.org/abs/2311.11045)), ([:book:](https://browse.arxiv.org/pdf/2311.11045.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.11045.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.11045)), ([:house:](https://huggingface.co/papers/2311.11045)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/orca-2-teaching-small-language-models-how-to)), ([:octocat:](https://github.com/zhaoolee/garss)![GitHub Repo stars](https://img.shields.io/github/stars/zhaoolee/garss?style=social))  |
| 11.18 | Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers ([:x:](https://arxiv.org/abs/2311.10642)), ([:book:](https://browse.arxiv.org/pdf/2311.10642.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10642.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10642)), ([:house:](https://huggingface.co/papers/2311.10642)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rethinking-attention-exploring-shallow-feed)) |
| 11.18 | Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning ([:x:](https://arxiv.org/abs/2311.10709)), ([:book:](https://browse.arxiv.org/pdf/2311.10709.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10709.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10709)), ([:house:](https://huggingface.co/papers/2311.10709)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/emu-video-factorizing-text-to-video)) |
| 11.18 | Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2 ([:x:](https://arxiv.org/abs/2311.10702)), ([:book:](https://browse.arxiv.org/pdf/2311.10702.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10702.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10702)), ([:house:](https://huggingface.co/papers/2311.10702)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/camels-in-a-changing-climate-enhancing-lm)) |
| 11.18 | SelfEval: Leveraging the discriminative nature of generative models for evaluation ([:x:](https://arxiv.org/abs/2311.10708)), ([:book:](https://browse.arxiv.org/pdf/2311.10708.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10708.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10708)), ([:house:](https://huggingface.co/papers/2311.10708)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/selfeval-leveraging-the-discriminative-nature)) |
| 11.18 | Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning ([:x:](https://arxiv.org/abs/2311.11077)), ([:book:](https://browse.arxiv.org/pdf/2311.11077.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.11077.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.11077)), ([:house:](https://huggingface.co/papers/2311.11077)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/adapters-a-unified-library-for-parameter)), ([:octocat:](https://github.com/Adapter-Hub/adapter-transformers)![GitHub Repo stars](https://img.shields.io/github/stars/Adapter-Hub/adapter-transformers?style=social))  |
| 11.18 | Distilling and Retrieving Generalizable Knowledge for Robot Manipulation via Language Corrections ([:x:](https://arxiv.org/abs/2311.10678)), ([:book:](https://browse.arxiv.org/pdf/2311.10678.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10678.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10678)), ([:house:](https://huggingface.co/papers/2311.10678)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/distilling-and-retrieving-generalizable)) |
| 11.17 | Leveraging Large Language Models for Decision Support in Personalized Oncology (JAMA [doi:10.1001/jamanetworkopen.2023.43689](https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2812097)) |
| 11.17 | PEFT-MedAware: Large Language Model for Medical Awareness ([:x:](https://arxiv.org/abs/2311.10697)), ([:book:](https://browse.arxiv.org/pdf/2311.10697.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10697.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10697)), ([:house:](https://huggingface.co/papers/2311.10697)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/peft-medaware-large-language-model-for)) |
| 11.17 | OpenAI’s Sam Altman exits as CEO because ‘board no longer has confidence’ in his ability to lead (CNBC [news](https://www.cnbc.com/2023/11/17/sam-altman-leaves-openai-mira-murati-appointed-interim-boss.html)) |
| 11.17 | Testing Language Model Agents Safely in the Wild ([:x:](https://arxiv.org/abs/2311.10538)), ([:book:](https://browse.arxiv.org/pdf/2311.10538.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10538.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10538)), ([:house:](https://huggingface.co/papers/2311.10538)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/testing-language-model-agents-safely-in-the)) |
| 11.17 | Text-to-Sticker: Style Tailoring Latent Diffusion Models for Human Expression ([:x:](https://arxiv.org/abs/2311.10794)), ([:book:](https://browse.arxiv.org/pdf/2311.10794.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10794.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10794)), ([:house:](https://huggingface.co/papers/2311.10794)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/text-to-sticker-style-tailoring-latent)) |
| 11.17 | The Chosen One: Consistent Characters in Text-to-Image Diffusion Models ([:x:](https://arxiv.org/abs/2311.10093)), ([:book:](https://browse.arxiv.org/pdf/2311.10093.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10093.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10093)), ([:house:](https://huggingface.co/papers/2311.10093)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-chosen-one-consistent-characters-in-text)) |
| 11.17 | Adaptive Shells for Efficient Neural Radiance Field Rendering ([:x:](https://arxiv.org/abs/2311.10091)), ([:book:](https://browse.arxiv.org/pdf/2311.10091.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10091.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10091)), ([:house:](https://huggingface.co/papers/2311.10091)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/adaptive-shells-for-efficient-neural-radiance)) |
| 11.17 | JaxMARL: Multi-Agent RL Environments in JAX ([:x:](https://arxiv.org/abs/2311.10090)), ([:book:](https://browse.arxiv.org/pdf/2311.10090.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10090.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10090)), ([:house:](https://huggingface.co/papers/2311.10090)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/jaxmarl-multi-agent-rl-environments-in-jax)), ([:octocat:](https://github.com/flairox/jaxmarl )![GitHub Repo stars](https://img.shields.io/github/stars/flairox/jaxmarl ?style=social))  | 
| 11.16 | By the Numbers: Tracking The AI Executive Order (HAI [news](https://hai.stanford.edu/news/numbers-tracking-ai-executive-order)) |
| 11.16 | Change to policy on the use of generative AI and large language models (SCience [blog](https://www.science.org/content/blog-post/change-policy-use-generative-ai-and-large-language-models)) |
| 11.16 | ML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks ([:x:](https://arxiv.org/abs/2311.09835)), ([:book:](https://browse.arxiv.org/pdf/2311.09835.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.09835.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.09835)), ([:house:](https://huggingface.co/papers/2311.09835)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ml-bench-large-language-models-leverage-open)) |
| 11.16 | MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning ([:x:](https://arxiv.org/abs/2311.10537)), ([:book:](https://browse.arxiv.org/pdf/2311.10537.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10537.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10537)), ([:house:](https://huggingface.co/papers/2311.10537)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/medagents-large-language-models-as)), ([:octocat:](https://github.com/gersteinlab/medagents)![GitHub Repo stars](https://img.shields.io/github/stars/gersteinlab/medagents?style=social))  |
| 11.16 | HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs ([:x:](https://arxiv.org/abs/2311.09774)), ([:book:](https://browse.arxiv.org/pdf/2311.09774.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.09774.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.09774)), ([:house:](https://huggingface.co/papers/2311.09774)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/huatuogpt-ii-one-stage-training-for-medical)), ([:octocat:](https://github.com/freedomintelligence/huatuogpt-ii)![GitHub Repo stars](https://img.shields.io/github/stars/freedomintelligence/huatuogpt-ii?style=social))  |
| 11.16 | Do Physicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation ([:x:](https://arxiv.org/abs/2311.09684)), ([:book:](https://browse.arxiv.org/pdf/2311.09684.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.09684.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.09684)), ([:house:](https://huggingface.co/papers/2311.09684)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/do-physicians-know-how-to-prompt-the-need-for)) |
| 11.16 | AI: The Coming Revolution ([report](https://www.coatue.com/blog/perspective/ai-the-coming-revolution-2023)), ([presentation](https://drive.google.com/file/d/1gQhYT7j6b2wJmrFZHNeQgTiWPyTsjOfX/view)) |
| 11.16 | VideoCon: Robust Video-Language Alignment via Contrast Captions ([:x:](https://arxiv.org/abs/2311.10111)), ([:book:](https://browse.arxiv.org/pdf/2311.10111.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10111.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10111)), ([:house:](https://huggingface.co/papers/2311.10111)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/videocon-robust-video-language-alignment-via)), ([:octocat:](https://github.com/hritikbansal/videocon)![GitHub Repo stars](https://img.shields.io/github/stars/hritikbansal/videocon?style=social))  |
| 11.16 | UnifiedVisionGPT: Streamlining Vision-Oriented AI through Generalized Multimodal Framework ([:x:](https://arxiv.org/abs/2311.10125)), ([:book:](https://browse.arxiv.org/pdf/2311.10125.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10125.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10125)), ([:house:](https://huggingface.co/papers/2311.10125)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/unifiedvisiongpt-streamlining-vision-oriented)), ([:octocat:](https://github.com/lhbuilder/sa-segment-anything)![GitHub Repo stars](https://img.shields.io/github/stars/lhbuilder/sa-segment-anything?style=social))  |
| 11.16 | Exponentially Faster Language Modelling ([:x:](https://arxiv.org/abs/2311.10770)), ([:book:](https://browse.arxiv.org/pdf/2311.10770.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10770.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10770)), ([:house:](https://huggingface.co/papers/2311.10770)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/exponentially-faster-language-modelling)), ([:octocat:](https://github.com/pbelcak/fastbert)![GitHub Repo stars](https://img.shields.io/github/stars/pbelcak/fastbert?style=social))  |
| 11.16 | Memory Augmented Language Models through Mixture of Word Experts ([:x:](https://arxiv.org/abs/2311.10768)), ([:book:](https://browse.arxiv.org/pdf/2311.10768.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10768.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10768)), ([:house:](https://huggingface.co/papers/2311.10768)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/memory-augmented-language-models-through)) |
| 11.16 | ToolTalk: Evaluating Tool-Usage in a Conversational Setting ([:x:](https://arxiv.org/abs/2311.10775)), ([:book:](https://browse.arxiv.org/pdf/2311.10775.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10775.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10775)), ([:house:](https://huggingface.co/papers/2311.10775)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tooltalk-evaluating-tool-usage-in-a)) |
| 11.16 | Video-LLaVA: Learning United Visual Representation by Alignment Before Projection ([:x:](https://arxiv.org/abs/2311.10122)), ([:book:](https://browse.arxiv.org/pdf/2311.10122.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10122.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10122)), ([:house:](https://huggingface.co/papers/2311.10122)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/video-llava-learning-united-visual-1)), ([:octocat:](https://github.com/PKU-YuanGroup/Video-LLaVA)![GitHub Repo stars](https://img.shields.io/github/stars/PKU-YuanGroup/Video-LLaVA?style=social))  |
| 11.16 | MetaDreamer: Efficient Text-to-3D Creation With Disentangling Geometry and Texture ([:x:](https://arxiv.org/abs/2311.10123)), ([:book:](https://browse.arxiv.org/pdf/2311.10123.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10123.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10123)), ([:house:](https://huggingface.co/papers/2311.10123)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/metadreamer-efficient-text-to-3d-creation)) |
| 11.16 | I&S-ViT: An Inclusive & Stable Method for Pushing the Limit of Post-Training ViTs Quantization ([:x:](https://arxiv.org/abs/2311.10126)), ([:book:](https://browse.arxiv.org/pdf/2311.10126.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10126.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10126)), ([:house:](https://huggingface.co/papers/2311.10126)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/i-s-vit-an-inclusive-stable-method-for)) |
| 11.16 | Contrastive Chain-of-Thought Prompting ([:x:](https://arxiv.org/abs/2311.09277)), ([:book:](https://browse.arxiv.org/pdf/2311.09277.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.09277.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.09277)), ([:house:](https://huggingface.co/papers/2311.09277)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/contrastive-chain-of-thought-prompting)), ([:octocat:](https://github.com/damo-nlp-sg/contrastive-cot)![GitHub Repo stars](https://img.shields.io/github/stars/damo-nlp-sg/contrastive-cot?style=social))  |
| 11.16 | Tied-Lora: Enhacing parameter efficiency of LoRA with weight tying ([:x:](https://arxiv.org/abs/2311.09578)), ([:book:](https://browse.arxiv.org/pdf/2311.09578.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.09578.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.09578)), ([:house:](https://huggingface.co/papers/2311.09578)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tied-lora-enhacing-parameter-efficiency-of)) |
| 11.16 | DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model ([:x:](https://arxiv.org/abs/2311.09217)), ([:book:](https://browse.arxiv.org/pdf/2311.09217.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.09217.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.09217)), ([:house:](https://huggingface.co/papers/2311.09217)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dmv3d-denoising-multi-view-diffusion-using-3d)) |
| 11.16 | Single-Image 3D Human Digitization with Shape-Guided Diffusion ([:x:](https://arxiv.org/abs/2311.09221)), ([:book:](https://browse.arxiv.org/pdf/2311.09221.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.09221.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.09221)), ([:house:](https://huggingface.co/papers/2311.09221)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/single-image-3d-human-digitization-with-shape)) |
| 11.16 | GRIM: GRaph-based Interactive narrative visualization for gaMes ([:x:](https://arxiv.org/abs/2311.09213)), ([:book:](https://browse.arxiv.org/pdf/2311.09213.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.09213.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.09213)), ([:house:](https://huggingface.co/papers/2311.09213)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/grim-graph-based-interactive-narrative)) |
| 11.16 | PEARL: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers ([:x:](https://arxiv.org/abs/2311.09180)), ([:book:](https://browse.arxiv.org/pdf/2311.09180.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.09180.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.09180)), ([:house:](https://huggingface.co/papers/2311.09180)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pearl-personalizing-large-language-model)) |
| 11.16 | SiRA: Sparse Mixture of Low Rank Adaptation ([:x:](https://arxiv.org/abs/2311.09179)), ([:book:](https://browse.arxiv.org/pdf/2311.09179.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.09179.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.09179)), ([:house:](https://huggingface.co/papers/2311.09179)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sira-sparse-mixture-of-low-rank-adaptation)) |
| 11.16 | Fusion-Eval: Integrating Evaluators with LLMs ([:x:](https://arxiv.org/abs/2311.09204)), ([:book:](https://browse.arxiv.org/pdf/2311.09204.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.09204.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.09204)), ([:house:](https://huggingface.co/papers/2311.09204)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fusion-eval-integrating-evaluators-with-llms)) |
| 11.15 | Towards Publicly Accountable Frontier LLMs: Building an External Scrutiny Ecosystem under the ASPIRE Framework ([:x:](https://arxiv.org/abs/2311.14711)), ([:book:](https://browse.arxiv.org/pdf/2311.14711.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.14711.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.14711)), ([:house:](https://huggingface.co/papers/2311.14711)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-publicly-accountable-frontier-llms)), ([SS](https://www.semanticscholar.org/paper/Towards-Publicly-Accountable-Frontier-LLMs%3A-an-the-Anderljung-Smith/86c130dee7b3060bc647625b79945927ada43d3e)) |
| 11.15 | How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities ([:x:](https://arxiv.org/abs/2311.09447)), ([:book:](https://browse.arxiv.org/pdf/2311.09447.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.09447.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.09447)), ([:house:](https://huggingface.co/papers/2311.09447)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/how-trustworthy-are-open-source-llms-an)) |
| 11.15 | Towards Publicly Accountable Frontier LLMs: Building an External Scrutiny Ecosystem under the ASPIRE Framework ([:x:](https://arxiv.org/abs/2311.14711)), ([:book:](https://browse.arxiv.org/pdf/2311.14711.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.14711.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.14711)), ([:house:](https://huggingface.co/papers/2311.14711)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-publicly-accountable-frontier-llms)) |
| 11.15 | Can AI solve medical mysteries? It’s worth finding out (WP [news](https://www.washingtonpost.com/opinions/2023/11/15/ai-rare-disease-diagnosis/)), ([archive](https://archive.is/1IULt#selection-429.0-429.55)) |
| 11.15 | UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs ([:x:](https://arxiv.org/abs/2311.09257)), ([:book:](https://browse.arxiv.org/pdf/2311.09257.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.09257.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.09257)), ([:house:](https://huggingface.co/papers/2311.09257)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ufogen-you-forward-once-large-scale-text-to)) |
| 11.15 | Drivable 3D Gaussian Avatars ([:x:](https://arxiv.org/abs/2311.08581)), ([:book:](https://browse.arxiv.org/pdf/2311.08581.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.08581.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.08581)), ([:house:](https://huggingface.co/papers/2311.08581)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/drivable-3d-gaussian-avatars)) |
| 11.15 | EDMSound: Spectrogram Based Diffusion Models for Efficient and High-Quality Audio Synthesis ([:x:](https://arxiv.org/abs/2311.08667)), ([:book:](https://browse.arxiv.org/pdf/2311.08667.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.08667.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.08667)), ([:house:](https://huggingface.co/papers/2311.08667)), ([:eight_spoked_asterisk:]()) |
| 11.15 | UNcommonsense Reasoning: Abductive Reasoning about Uncommon Situations ([:x:](https://arxiv.org/abs/2311.08469)), ([:book:](https://browse.arxiv.org/pdf/2311.08469.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.08469.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.08469)), ([:house:](https://huggingface.co/papers/2311.08469)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/uncommonsense-reasoning-abductive-reasoning)) |
| 11.15 | UT5: Pretraining Non autoregressive T5 with unrolled denoising ([:x:](https://arxiv.org/abs/2311.08552)), ([:book:](https://browse.arxiv.org/pdf/2311.08552.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.08552.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.08552)), ([:house:](https://huggingface.co/papers/2311.08552)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ut5-pretraining-non-autoregressive-t5-with)) |
| 11.15 | Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models ([:x:](https://arxiv.org/abs/2311.08692)), ([:book:](https://browse.arxiv.org/pdf/2311.08692.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.08692.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.08692)), ([:house:](https://huggingface.co/papers/2311.08692)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/routing-to-the-expert-efficient-reward-guided)) |
| 11.15 | Llamas Know What GPTs Don't Show: Surrogate Models for Confidence Estimation ([:x:](https://arxiv.org/abs/2311.08877)), ([:book:](https://browse.arxiv.org/pdf/2311.08877.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.08877.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.08877)), ([:house:](https://huggingface.co/papers/2311.08877)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llamas-know-what-gpts-don-t-show-surrogate)) |
| 11.15 | Thread of Thought Unraveling Chaotic Contexts ([:x:](https://arxiv.org/abs/2311.08734)), ([:book:](https://browse.arxiv.org/pdf/2311.08734.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.08734.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.08734)), ([:house:](https://huggingface.co/papers/2311.08734)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/thread-of-thought-unraveling-chaotic-contexts)) |
| 11.15 | Instant3D: Instant Text-to-3D Generation ([:x:](https://arxiv.org/abs/2311.08403)), ([:book:](https://browse.arxiv.org/pdf/2311.08403.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.08403.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.08403)), ([:house:](https://huggingface.co/papers/2311.08403)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/instant3d-instant-text-to-3d-generation)) |
| 11.15 | Fine-tuning Language Models for Factuality ([:x:](https://arxiv.org/abs/2311.08401)), ([:book:](https://browse.arxiv.org/pdf/2311.08401.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.08401.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.08401)), ([:house:](https://huggingface.co/papers/2311.08401)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fine-tuning-language-models-for-factuality)) |
| 11.15 | Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads to Answers Faster ([:x:](https://arxiv.org/abs/2311.08263)), ([:book:](https://browse.arxiv.org/pdf/2311.08263.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.08263.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.08263)), ([:house:](https://huggingface.co/papers/2311.08263)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fast-chain-of-thought-a-glance-of-future-from)) |
| 11.14 | Extrinsically-Focused Evaluation of Omissions in Medical Summarization ([:x:](https://arxiv.org/abs/2311.08303)), ([:book:](https://browse.arxiv.org/pdf/2311.08303.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.08303.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.08303)), ([:house:](https://huggingface.co/papers/2311.08303)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/extrinsically-focused-evaluation-of-omissions)) |
| 11.14 | Artificial General Intelligence, Existential Risk, and Human Risk Perception ([:x:](https://arxiv.org/abs/2311.08698)), ([:book:](https://browse.arxiv.org/pdf/2311.08698.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.08698.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.08698)), ([:house:](https://huggingface.co/papers/2311.08698)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/artificial-general-intelligence-existential)) |
| 11.14 | Story-to-Motion: Synthesizing Infinite and Controllable Character Animation from Long Text ([:x:](https://arxiv.org/abs/2311.07446)), ([:book:](https://browse.arxiv.org/pdf/2311.07446.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.07446.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.07446)), ([:house:](https://huggingface.co/papers/2311.07446)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/story-to-motion-synthesizing-infinite-and)) |
| 11.14 | One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion ([:x:](https://arxiv.org/abs/2311.07885)), ([:book:](https://browse.arxiv.org/pdf/2311.07885.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.07885.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.07885)), ([:house:](https://huggingface.co/papers/2311.07885)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/one-2-3-45-fast-single-image-to-3d-objects)) |
| 11.14 | A Survey on Language Models for Code ([:x:](https://arxiv.org/abs/2311.07989)), ([:book:](https://browse.arxiv.org/pdf/2311.07989.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.07989.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.07989)), ([:house:](https://huggingface.co/papers/2311.07989)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-survey-on-language-models-for-code)), ([:octocat:](https://github.com/codefuse-ai/awesome-code-llm)![GitHub Repo stars](https://img.shields.io/github/stars/codefuse-ai/awesome-code-llm?style=social))  |
| 11.14 | DiLoCo: Distributed Low-Communication Training of Language Models ([:x:](https://arxiv.org/abs/2311.08105)), ([:book:](https://browse.arxiv.org/pdf/2311.08105.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.08105.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.08105)), ([:house:](https://huggingface.co/papers/2311.08105)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/diloco-distributed-low-communication-training)) |
| 11.14 | Instruction-Following Evaluation for Large Language Models ([:x:](https://arxiv.org/abs/2311.07911)), ([:book:](https://browse.arxiv.org/pdf/2311.07911.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.07911.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.07911)), ([:house:](https://huggingface.co/papers/2311.07911)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/instruction-following-evaluation-for-large)), ([:octocat:](https://github.com/google-research/google-research)![GitHub Repo stars](https://img.shields.io/github/stars/google-research/google-research?style=social))  |
| 11.14 | The ART of LLM Refinement: Ask, Refine, and Trust ([:x:](https://arxiv.org/abs/2311.07961)), ([:book:](https://browse.arxiv.org/pdf/2311.07961.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.07961.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.07961)), ([:house:](https://huggingface.co/papers/2311.07961)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-art-of-llm-refinement-ask-refine-and)) |
| 11.14 | MART: Improving LLM Safety with Multi-round Automatic Red-Teaming ([:x:](https://arxiv.org/abs/2311.07689)), ([:book:](https://browse.arxiv.org/pdf/2311.07689.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.07689.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.07689)), ([:house:](https://huggingface.co/papers/2311.07689)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mart-improving-llm-safety-with-multi-round)) |
| 11.14 | Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models ([:x:](https://arxiv.org/abs/2311.07919)), ([:book:](https://browse.arxiv.org/pdf/2311.07919.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.07919.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.07919)), ([:house:](https://huggingface.co/papers/2311.07919)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/qwen-audio-advancing-universal-audio)), ([:octocat:](https://github.com/qwenlm/qwen-audio)![GitHub Repo stars](https://img.shields.io/github/stars/qwenlm/qwen-audio?style=social))  |
| 11.14 | GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation ([:x:](https://arxiv.org/abs/2311.07562)), ([:book:](https://browse.arxiv.org/pdf/2311.07562.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.07562.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.07562)), ([:house:](https://huggingface.co/papers/2311.07562)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gpt-4v-in-wonderland-large-multimodal-models)), ([:octocat:](https://github.com/zzxslp/mm-navigator)![GitHub Repo stars](https://img.shields.io/github/stars/zzxslp/mm-navigator?style=social))  |
| 11.14 | SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models ([:x:](https://arxiv.org/abs/2311.07575)), ([:book:](https://browse.arxiv.org/pdf/2311.07575.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.07575.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.07575)), ([:house:](https://huggingface.co/papers/2311.07575)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sphinx-the-joint-mixing-of-weights-tasks-and)), ([:octocat:](https://github.com/alpha-vllm/llama2-accessory)![GitHub Repo stars](https://img.shields.io/github/stars/alpha-vllm/llama2-accessory?style=social))  |
| 11.14 | MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks ([:x:](https://arxiv.org/abs/2311.07463)), ([:book:](https://browse.arxiv.org/pdf/2311.07463.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.07463.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.07463)), ([:house:](https://huggingface.co/papers/2311.07463)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/megaverse-benchmarking-large-language-models)) | 
| 11.13 | Applying Large Language Models for Causal Structure Learning in Non Small Cell Lung Cancer ([:x:](https://arxiv.org/abs/2311.07191)), ([:book:](https://browse.arxiv.org/pdf/2311.07191.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.07191.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.07191)), ([:house:](https://huggingface.co/papers/2311.07191)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/applying-large-language-models-for-causal)) |
| 11.13 | The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4 ([:x:](https://arxiv.org/abs/2311.07361)), ([:book:](https://browse.arxiv.org/pdf/2311.07361.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.07361.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.07361)), ([:house:](https://huggingface.co/papers/2311.07361)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-impact-of-large-language-models-on)) |
| 11.13 | To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning ([:x:](https://arxiv.org/abs/2311.07574)), ([:book:](https://browse.arxiv.org/pdf/2311.07574.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.07574.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.07574)), ([:house:](https://huggingface.co/papers/2311.07574)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/to-see-is-to-believe-prompting-gpt-4v-for)), ([:octocat:](https://github.com/x2fd/lvis-instruct4v)![GitHub Repo stars](https://img.shields.io/github/stars/x2fd/lvis-instruct4v?style=social))  |
| 11.13 | Music ControlNet: Multiple Time-varying Controls for Music Generation ([:x:](https://arxiv.org/abs/2311.07069)), ([:book:](https://browse.arxiv.org/pdf/2311.07069.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.07069.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.07069)), ([:house:](https://huggingface.co/papers/2311.07069)), ([:eight_spoked_asterisk:]()) |
| 11.13 | Prompt Engineering a Prompt Engineer ([:x:](https://arxiv.org/abs/2311.05661)), ([:book:](https://browse.arxiv.org/pdf/2311.05661.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.05661.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.05661)), ([:house:](https://huggingface.co/papers/2311.05661)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/prompt-engineering-a-prompt-engineer)) |
| 11.12 | Trusted Source Alignment in Large Language Models ([:x:](https://arxiv.org/abs/2311.06697)), ([:book:](https://browse.arxiv.org/pdf/2311.06697.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.06697.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.06697)), ([:house:](https://huggingface.co/papers/2311.06697)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/trusted-source-alignment-in-large-language)) |
| 11.12 | ChatAnything: Facetime Chat with LLM-Enhanced Personas ([:x:](https://arxiv.org/abs/2311.06772)), ([:book:](https://browse.arxiv.org/pdf/2311.06772.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.06772.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.06772)), ([:house:](https://huggingface.co/papers/2311.06772)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chatanything-facetime-chat-with-llm-enhanced)) |
| 11.12 | Q-Instruct: Improving Low-level Visual Abilities for Multi-modality Foundation Models ([:x:](https://arxiv.org/abs/2311.06783)), ([:book:](https://browse.arxiv.org/pdf/2311.06783.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.06783.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.06783)), ([:house:](https://huggingface.co/papers/2311.06783)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/q-instruct-improving-low-level-visual)), ([:octocat:](https://github.com/Q-Future/Q-Instruct)![GitHub Repo stars](https://img.shields.io/github/stars/Q-Future/Q-Instruct?style=social))  |
| 11.12 | Towards General-Purpose Speech Abilities for Large Language Models Using Unpaired Data ([:x:](https://arxiv.org/abs/2311.06753)), ([:book:](https://browse.arxiv.org/pdf/2311.06753.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.06753.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.06753)), ([:house:](https://huggingface.co/papers/2311.06753)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-general-purpose-speech-abilities-for)) |
| 11.12 | Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer ([:x:](https://arxiv.org/abs/2311.06720)), ([:book:](https://browse.arxiv.org/pdf/2311.06720.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.06720.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.06720)), ([:house:](https://huggingface.co/papers/2311.06720)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/cappy-outperforming-and-boosting-large-multi)) |
| 11.11 | LayoutPrompter: Awaken the Design Ability of Large Language Models ([:x:](https://arxiv.org/abs/2311.06495)), ([:book:](https://browse.arxiv.org/pdf/2311.06495.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.06495.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.06495)), ([:house:](https://huggingface.co/papers/2311.06495)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/layoutprompter-awaken-the-design-ability-of-1)), ([:octocat:](https://github.com/microsoft/layoutgeneration)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/layoutgeneration?style=social))  |
| 11.11 | GOAT: GO to Any Thing ([:x:](https://arxiv.org/abs/2311.06430)), ([:book:](https://browse.arxiv.org/pdf/2311.06430.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.06430.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.06430)), ([:house:](https://huggingface.co/papers/2311.06430)), ([:eight_spoked_asterisk:]()) |
| 11.11 | Language Models can be Logical Solvers ([:x:](https://arxiv.org/abs/2311.06158)), ([:book:](https://browse.arxiv.org/pdf/2311.06158.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.06158.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.06158)), ([:house:](https://huggingface.co/papers/2311.06158)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/language-models-can-be-logical-solvers)) |
| 11.11 | Instant3D: Fast Text-to-3D with Sparse-View Generation and Large Reconstruction Model ([:x:](https://arxiv.org/abs/2311.06214)), ([:book:](https://browse.arxiv.org/pdf/2311.06214.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.06214.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.06214)), ([:house:](https://huggingface.co/papers/2311.06214)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/instant3d-fast-text-to-3d-with-sparse-view)) |
| 11.11 | Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization ([:x:](https://arxiv.org/abs/2311.06243)), ([:book:](https://browse.arxiv.org/pdf/2311.06243.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.06243.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.06243)), ([:house:](https://huggingface.co/papers/2311.06243)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/parameter-efficient-orthogonal-finetuning-via)) |
| 11.11 | A Strengths, Weaknesses, Opportunities, and Threats (SWOT) Analysis of ChatGPT Integration in Nursing Education: A Narrative Review (Cureus [DOI: 10.7759/cureus.48643](https://www.cureus.com/articles/202239-a-strengths-weaknesses-opportunities-and-threats-swot-analysis-of-chatgpt-integration-in-nursing-education-a-narrative-review#!/)) |
| 11.11 | The Impact of Chat Generative Pre-trained Transformer (ChatGPT) on Oncology: Application, Expectations, and Future Prospects (Cureus [DOI: 10.7759/cureus.48670](https://www.cureus.com/articles/203580-the-impact-of-chat-generative-pre-trained-transformer-chatgpt-on-oncology-application-expectations-and-future-prospects#!/)) |
| 11.10 | Holistic Evaluation of GPT-4V for Biomedical Imaging ([:x:](https://arxiv.org/abs/2312.05256)), ([:book:](https://browse.arxiv.org/pdf/2312.05256.pdf)), ([:paperclip:](https://arxiv.org/pdf/2312.05256.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2312.05256)), ([:house:](https://huggingface.co/papers/2312.05256)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/holistic-evaluation-of-gpt-4v-for-biomedical)) |
| 11.10 | How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model ([:x:](https://arxiv.org/abs/2311.07594)), ([:book:](https://browse.arxiv.org/pdf/2311.07594.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.07594.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.07594)), ([:house:](https://huggingface.co/papers/2311.07594)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/how-to-bridge-the-gap-between-modalities-a)) |
| 11.10 | ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences ([:x:](https://arxiv.org/abs/2311.06025)), ([:book:](https://browse.arxiv.org/pdf/2311.06025.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.06025.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.06025)), ([:house:](https://huggingface.co/papers/2311.06025)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chimed-gpt-a-chinese-medical-large-language)), ([:octocat:](https://github.com/synlp/chimed-gpt)![GitHub Repo stars](https://img.shields.io/github/stars/synlp/chimed-gpt?style=social))  |
| 11.10 | JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models ([:x:](https://arxiv.org/abs/2311.05997)), ([:book:](https://browse.arxiv.org/pdf/2311.05997.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.05997.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.05997)), ([:house:](https://huggingface.co/papers/2311.05997)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/jarvis-1-open-world-multi-task-agents-with)) |
| 11.10 | China proposes new regulations for generative AI focusing on data security, evaluation ([news](https://coingeek.com/china-proposes-new-regulations-for-generative-ai-focusing-on-data-security-evaluation/)) - ([生成式人工智能服务安全基本要求](https://www.tc260.org.cn/upload/2023-10-11/1697008495851003865.pdf)) |
| 11.10 | New international consortium formed to create trustworthy and reliable generative AI models for science ([news](https://www.anl.gov/article/new-international-consortium-formed-to-create-trustworthy-and-reliable-generative-ai-models-for)) - ([Trillion Parameter Consortium](https://tpc.dev/)) |
| 11.10 | AI robotics’ ‘GPT moment’ is near (TC [news](https://techcrunch.com/2023/11/10/ai-robotics-gpt-moment-is-near/)) |
| 11.10 | :hearts: ChatGPT in prostate cancer: myth or reality? (Prostate Cancer Prostatic Dis [https://doi.org/10.1038/s41391-023-00750-7](https://www.nature.com/articles/s41391-023-00750-7)) |
| 11.10 | LCM-LoRA: A Universal Stable-Diffusion Acceleration Module ([:x:](https://arxiv.org/abs/2311.05556)), ([:book:](https://browse.arxiv.org/pdf/2311.05556.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.05556.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.05556)), ([:house:](https://huggingface.co/papers/2311.05556)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lcm-lora-a-universal-stable-diffusion)), ([:octocat:](https://github.com/luosiallen/latent-consistency-model)![GitHub Repo stars](https://img.shields.io/github/stars/luosiallen/latent-consistency-model?style=social))  |
| 11.10 | LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents ([:x:](https://arxiv.org/abs/2311.05437)), ([:book:](https://browse.arxiv.org/pdf/2311.05437.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.05437.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.05437)), ([:house:](https://huggingface.co/papers/2311.05437)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llava-plus-learning-to-use-tools-for-creating)) |
| 11.9 | Frontier Language Models are not Robust to Adversarial Arithmetic, or "What do I need to say so you agree 2+2=5? ([:x:](https://arxiv.org/abs/2311.07587)), ([:book:](https://browse.arxiv.org/pdf/2311.07587.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.07587.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.07587)), ([:house:](https://huggingface.co/papers/2311.07587)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/frontier-language-models-are-not-robust-to)) |
| 11.9 | Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure ([:x:](https://arxiv.org/abs/2311.07590)), ([:book:](https://browse.arxiv.org/pdf/2311.07590.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.07590.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.07590)), ([:house:](https://huggingface.co/papers/2311.07590)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/technical-report-large-language-models-can)), ([:octocat:](https://github.com/apolloresearch/insider-trading)![GitHub Repo stars](https://img.shields.io/github/stars/apolloresearch/insider-trading?style=social))  |
| 11.9 | Lumos: Learning Agents with Unified Data, Modular Design, and Open-Source LLMs ([:x:](https://arxiv.org/abs/2311.05657)), ([:book:](https://browse.arxiv.org/pdf/2311.05657.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.05657.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.05657)), ([:house:](https://huggingface.co/papers/2311.05657)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lumos-learning-agents-with-unified-data)), ([:octocat:](https://github.com/allenai/lumos)![GitHub Repo stars](https://img.shields.io/github/stars/allenai/lumos?style=social))  |
| 11.9 | :hearts: Accuracy of a Vision-Language Model on Challenging Medical Cases ([:x:](https://arxiv.org/abs/2311.05591)), ([:book:](https://browse.arxiv.org/pdf/2311.05591.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.05591.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.05591)), ([:house:](https://huggingface.co/papers/2311.05591)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/accuracy-of-a-vision-language-model-on)), ([:octocat:](https://github.com/2v/gpt4v-image-challenge)![GitHub Repo stars](https://img.shields.io/github/stars/2v/gpt4v-image-challenge?style=social))  |
| 11.9 | The testing framework for ML models, from tabular to LLMs ([:octocat:](https://github.com/Giskard-AI/giskard)![GitHub Repo stars](https://img.shields.io/github/stars/Giskard-AI/giskards?style=social)) |
| 11.9 | A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions ([:x:](https://arxiv.org/abs/2311.05232)), ([:book:](https://browse.arxiv.org/pdf/2311.05232.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.05232.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.05232)), ([:house:](https://huggingface.co/papers/2311.05232)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-survey-on-hallucination-in-large-language)) |
| 11.9 | :hearts: A Survey of Large Language Models in Medicine: Progress, Application, and Challenge ([:x:](https://arxiv.org/abs/2311.05112)), ([:book:](https://browse.arxiv.org/pdf/2311.05112.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.05112.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.05112)), ([:house:](https://huggingface.co/papers/2311.05112)), ([SS](https://www.semanticscholar.org/paper/A-Survey-of-Large-Language-Models-in-Medicine%3A-and-Zhou-Gu/bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-survey-of-large-language-models-in-medicine)), ([:octocat:](https://github.com/ai-in-health/medllmspracticalguide)![GitHub Repo stars](https://img.shields.io/github/stars/ai-in-health/medllmspracticalguide?style=social))  |
| 11.9 | GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and reusing ModulEs ([:x:](https://arxiv.org/abs/2311.04901)), ([:book:](https://browse.arxiv.org/pdf/2311.04901.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.04901.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.04901)), ([:house:](https://huggingface.co/papers/2311.04901)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/genome-generative-neuro-symbolic-visual)) |
| 11.9 | u-LLaVA: Unifying Multi-Modal Tasks via Large Language Model ([:x:](https://arxiv.org/abs/2311.05348)), ([:book:](https://browse.arxiv.org/pdf/2311.05348.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.05348.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.05348)), ([:house:](https://huggingface.co/papers/2311.05348)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/u-llava-unifying-multi-modal-tasks-via-large)) |
| 11.9 | On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving ([:x:](https://arxiv.org/abs/2311.05332)), ([:book:](https://browse.arxiv.org/pdf/2311.05332.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.05332.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.05332)), ([:house:](https://huggingface.co/papers/2311.05332)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/on-the-road-with-gpt-4v-ision-early)), ([:octocat:](https://github.com/pjlab-adg/gpt4v-ad-exploration)![GitHub Repo stars](https://img.shields.io/github/stars/pjlab-adg/gpt4v-ad-exploration?style=social))  |
| 11.9 | Humane AI Pin: ChatGPT Wearable to Launch with $699 Price Tag ([news](https://tech.co/news/humane-ai-pin-price-specs-launch)) |
| 11.9 | Microsoft briefly restricted employee access to OpenAI’s ChatGPT, citing security concerns ([news](https://www.cnbc.com/2023/11/09/microsoft-restricts-employee-access-to-openais-chatgpt.html)) |
| 11.8 | Unveiling Safety Vulnerabilities of Large Language Models ([:x:](https://arxiv.org/abs/2311.04124)), ([:book:](https://browse.arxiv.org/pdf/2311.04124.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.04124.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.04124)), ([:house:](https://huggingface.co/papers/2311.04124)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/unveiling-safety-vulnerabilities-of-large)) |
| 11.8 | Video Instance Matting ([:x:](https://arxiv.org/abs/2311.04212)), ([:book:](https://browse.arxiv.org/pdf/2311.04212.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.04212.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.04212)), ([:house:](https://huggingface.co/papers/2311.04212)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/video-instance-matting)) |
| 11.8 | I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models ([:x:](https://arxiv.org/abs/2311.04145)), ([:book:](https://browse.arxiv.org/pdf/2311.04145.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.04145.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.04145)), ([:house:](https://huggingface.co/papers/2311.04145)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/i2vgen-xl-high-quality-image-to-video)) |
| 11.8 | OtterHD: A High-Resolution Multi-modality Model ([:x:](https://arxiv.org/abs/2311.04219)), ([:book:](https://browse.arxiv.org/pdf/2311.04219.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.04219.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.04219)), ([:house:](https://huggingface.co/papers/2311.04219)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/otterhd-a-high-resolution-multi-modality)) |
| 11.8 | TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models ([:x:](https://arxiv.org/abs/2311.04589)), ([:book:](https://browse.arxiv.org/pdf/2311.04589.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.04589.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.04589)), ([:house:](https://huggingface.co/papers/2311.04589)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/teal-tokenize-and-embed-all-for-multi-modal)) |
| 11.8 | Holistic Evaluation of Text-To-Image Models ([:x:](https://arxiv.org/abs/2311.04287)), ([:book:](https://browse.arxiv.org/pdf/2311.04287.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.04287.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.04287)), ([:house:](https://huggingface.co/papers/2311.04287)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/holistic-evaluation-of-text-to-image-models)), ([:octocat:](https://github.com/stanford-crfm/helm)![GitHub Repo stars](https://img.shields.io/github/stars/stanford-crfm/helm?style=social))  |
| 11.8 | 3DiffTection: 3D Object Detection with Geometry-Aware Diffusion Features ([:x:](https://arxiv.org/abs/2311.04391)), ([:book:](https://browse.arxiv.org/pdf/2311.04391.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.04391.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.04391)), ([:house:](https://huggingface.co/papers/2311.04391)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/3difftection-3d-object-detection-with)) |
| 11.8 | NExT-Chat: An LMM for Chat, Detection and Segmentation ([:x:](https://arxiv.org/abs/2311.04498)), ([:book:](https://browse.arxiv.org/pdf/2311.04498.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.04498.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.04498)), ([:house:](https://huggingface.co/papers/2311.04498)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/next-chat-an-lmm-for-chat-detection-and)) |
| 11.8 | LRM: Large Reconstruction Model for Single Image to 3D ([:x:](https://arxiv.org/abs/2311.04400)), ([:book:](https://browse.arxiv.org/pdf/2311.04400.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.04400.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.04400)), ([:house:](https://huggingface.co/papers/2311.04400)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lrm-large-reconstruction-model-for-single)) |
| 11.8 | Prompt Cache: Modular Attention Reuse for Low-Latency Inference ([:x:](https://arxiv.org/abs/2311.04934)), ([:book:](https://browse.arxiv.org/pdf/2311.04934.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.04934.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.04934)), ([:house:](https://huggingface.co/papers/2311.04934)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/prompt-cache-modular-attention-reuse-for-low)) |
| 11.8 | Role play with large language models (Nature [https://doi.org/10.1038/s41586-023-06647-8](https://www.nature.com/articles/s41586-023-06647-8))
| 11.8 | How Accurate was ChatGPT for Common Allergy Myths? Pretty Accurate ([news](https://acaai.org/news/how-accurate-was-chatgpt-for-common-allergy-myths-pretty-accurate/)) |
| 11.8 | Amazon is reportedly racing to build an AI model called Olympus to take on ChatGPT and Bard ([news](https://www.businessinsider.com/amazon-is-building-ai-model-olympus-compete-with-openai-google-2023-11)) |
| 11.8 | The AI boom is shaking up the tech industry and moving markets. But is it all a mirage? ([news](https://www.bostonglobe.com/2023/11/08/business/ai-boom-hype-tech-markets/)) |
| 11.8 | Samsung unveils ChatGPT alternative Samsung Gauss that can generate text, code and images ([news](https://techcrunch.com/2023/11/08/samsung-unveils-chatgpt-alternative-samsung-gauss-that-can-generate-text-code-and-images/)) |
| 11.7 | Benefits and Harms of Large Language Models in Digital Mental Health ([:x:](https://arxiv.org/abs/2311.14693)), ([:book:](https://browse.arxiv.org/pdf/2311.14693.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.14693.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.14693)), ([:house:](https://huggingface.co/papers/2311.14693)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/benefits-and-harms-of-large-language-models)) |
| 11.7 | Evaluating Large Language Models in Ophthalmology ([:x:](https://arxiv.org/abs/2311.04933)), ([:book:](https://browse.arxiv.org/pdf/2311.04933.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.04933.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.04933)), ([:house:](https://huggingface.co/papers/2311.04933)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evaluating-large-language-models-in)) |
| 11.7 | Evaluating multiple large language models in pediatric ophthalmology ([:x:](https://arxiv.org/abs/2311.04368)), ([:book:](https://browse.arxiv.org/pdf/2311.04368.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.04368.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.04368)), ([:house:](https://huggingface.co/papers/2311.04368)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evaluating-multiple-large-language-models-in)) |
| 11.7 | Leveraging Large Language Models for Automated Proof Synthesis in Rust ([:x:](https://arxiv.org/abs/2311.03739)), ([:book:](https://browse.arxiv.org/pdf/2311.03739.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.03739.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.03739)), ([:house:](https://huggingface.co/papers/2311.03739)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/leveraging-large-language-models-for-3)) |
| 11.7 | SoundCam: A Dataset for Finding Humans Using Room Acoustics ([:x:](https://arxiv.org/abs/2311.03517)), ([:book:](https://browse.arxiv.org/pdf/2311.03517.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.03517.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.03517)), ([:house:](https://huggingface.co/papers/2311.03517)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/soundcam-a-dataset-for-finding-humans-using)) |
| 11.7 | Neural MMO 2.0: A Massively Multi-task Addition to Massively Multi-agent Learning ([:x:](https://arxiv.org/abs/2311.03736)), ([:book:](https://browse.arxiv.org/pdf/2311.03736.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.03736.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.03736)), ([:house:](https://huggingface.co/papers/2311.03736)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/neural-mmo-2-0-a-massively-multi-task)) |
| 11.7 | Random Field Augmentations for Self-Supervised Representation Learning ([:x:](https://arxiv.org/abs/2311.03629)), ([:book:](https://browse.arxiv.org/pdf/2311.03629.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.03629.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.03629)), ([:house:](https://huggingface.co/papers/2311.03629)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/random-field-augmentations-for-self)) |
| 11.7 | Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation ([:x:](https://arxiv.org/abs/2311.04254)), ([:book:](https://browse.arxiv.org/pdf/2311.04254.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.04254.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.04254)), ([:house:](https://huggingface.co/papers/2311.04254)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/everything-of-thoughts-defying-the-law-of)) |
| 11.7 | mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration ([:x:](https://arxiv.org/abs/2311.04257)), ([:book:](https://browse.arxiv.org/pdf/2311.04257.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.04257.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.04257)), ([:house:](https://huggingface.co/papers/2311.04257)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mplug-owl2-revolutionizing-multi-modal-large)), ([:octocat:](https://github.com/x-plug/mplug-owl)![GitHub Repo stars](https://img.shields.io/github/stars/x-plug/mplug-owl?style=social))  |
| 11.7 | GPT4All: An Ecosystem of Open Source Compressed Language Models ([:x:](https://arxiv.org/abs/2311.04931)), ([:book:](https://browse.arxiv.org/pdf/2311.04931.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.04931.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.04931)), ([:house:](https://huggingface.co/papers/2311.04931)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gpt4all-an-ecosystem-of-open-source)), ([:octocat:](https://github.com/nomic-ai/gpt4all)![GitHub Repo stars](https://img.shields.io/github/stars/nomic-ai/gpt4all?style=social))  |
| 11.7 | GLaMM: Pixel Grounding Large Multimodal Model ([:x:](https://arxiv.org/abs/2311.03356)), ([:book:](https://browse.arxiv.org/pdf/2311.03356.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.03356.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.03356)), ([:house:](https://huggingface.co/papers/2311.03356)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/glamm-pixel-grounding-large-multimodal-model)), ([:octocat:](https://github.com/mbzuai-oryx/groundingLMM)![GitHub Repo stars](https://img.shields.io/github/stars/mbzuai-oryx/groundingLMM?style=social))  |
| 11.7 | S-LoRA: Serving Thousands of Concurrent LoRA Adapters ([:x:](https://arxiv.org/abs/2311.03285)), ([:book:](https://browse.arxiv.org/pdf/2311.03285.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.03285.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.03285)), ([:house:](https://huggingface.co/papers/2311.03285)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/s-lora-serving-thousands-of-concurrent-lora)) |
| 11.7 | Ziya2: Data-centric Learning is All LLMs Need ([:x:](https://arxiv.org/abs/2311.03301)), ([:book:](https://browse.arxiv.org/pdf/2311.03301.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.03301.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.03301)), ([:house:](https://huggingface.co/papers/2311.03301)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ziya2-data-centric-learning-is-all-llms-need)) |
| 11.7 | LDM3D-VR: Latent Diffusion Model for 3D VR ([:x:](https://arxiv.org/abs/2311.03226)), ([:book:](https://browse.arxiv.org/pdf/2311.03226.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.03226.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.03226)), ([:house:](https://huggingface.co/papers/2311.03226)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ldm3d-vr-latent-diffusion-model-for-3d-vr)) |
| 11.6 | A Foundation Model for Music Informatics ([:x:](https://arxiv.org/abs/2311.03318)), ([:book:](https://browse.arxiv.org/pdf/2311.03318.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.03318.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.03318)), ([:house:](https://huggingface.co/papers/2311.03318)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-foundation-model-for-music-informatics)), ([:octocat:](https://github.com/minzwon/musicfm)![GitHub Repo stars](https://img.shields.io/github/stars/minzwon/musicfm?style=social))  |
| 11.6 | Can LLMs Follow Simple Rules? ([:x:](https://arxiv.org/abs/2311.04235)), ([:book:](https://browse.arxiv.org/pdf/2311.04235.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.04235.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.04235)), ([:house:](https://huggingface.co/papers/2311.04235)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/can-llms-follow-simple-rules)), ([:octocat:](https://github.com/normster/llm_rules)![GitHub Repo stars](https://img.shields.io/github/stars/normster/llm_rules?style=social))  |
| 11.6 | ‘ChatGPT detector’ catches AI-generated papers with unprecedented accuracy (Nature [doi: https://doi.org/10.1038/d41586-023-03479-4](https://www.nature.com/articles/d41586-023-03479-4)) |
| 11.6 | CogVLM: Visual Expert for Pretrained Language Models ([:x:](https://arxiv.org/abs/2311.03079)), ([:book:](https://browse.arxiv.org/pdf/2311.03079.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.03079.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.03079)), ([:house:](https://huggingface.co/papers/2311.03079)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/cogvlm-visual-expert-for-pretrained-language)), ([:octocat:](https://github.com/thudm/cogvlm)![GitHub Repo stars](https://img.shields.io/github/stars/thudm/cogvlm?style=social)) |
| 11.6 | Introducing GPTs ([blog](https://openai.com/blog/introducing-gpts))|
| 11.6 | New models and developer products announced at DevDay ([blog](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)) |
| 11.6 | OpenAI DevDay, Opening Keynote ([Youtube](https://www.youtube.com/watch?v=U9mJuUkhUzk)), ([tweet](https://twitter.com/OpenAI/status/1721596740024078340)) |
| 11.6 | All the news from OpenAI’s first developer conference ([news](https://www.theverge.com/2023/11/6/23948619/openai-chatgpt-devday-developer-conference-news)) |
| 11.6 | OpenAI Wants Everyone to Build Their Own Version of ChatGPT (Wired [news](https://www.wired.com/story/openai-wants-everyone-to-build-their-own-version-of-chatgpt/)) |      | 11.6 | Meet Angry Pumpkins: A game made using ChatGPT, DALL-E 3 and MidJourney ([news](https://indianexpress.com/article/technology/artificial-intelligence/angry-pumpkins-game-made-using-chatgpt-dall-e-3-midjourney-9015789/)) |
| 11.6 | ChatGPT subscribers may get a ‘GPT builder’ option soon ([news](https://www.theverge.com/2023/11/5/23947720/openai-chatgpt-gpt-builder-chatbot-creator-gizmo-tool-rumor-ai)) |
| 11.5 | Towards Generic Anomaly Detection and Understanding: Large-scale Visual-linguistic Model (GPT-4V) Takes the Lead ([:x:](https://arxiv.org/abs/2311.02782)), ([:book:](https://browse.arxiv.org/pdf/2311.02782.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.02782.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.02782)), ([:house:](https://huggingface.co/papers/2311.02782)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-generic-anomaly-detection-and)), ([:octocat:](https://github.com/caoyunkang/gpt4v-for-generic-anomaly-detection)![GitHub Repo stars](https://img.shields.io/github/stars/caoyunkang/gpt4v-for-generic-anomaly-detection?style=social))  |
| 11.5 | Levels of AGI: Operationalizing Progress on the Path to AGI ([:x:](https://arxiv.org/abs/2311.02462)), ([:book:](https://browse.arxiv.org/pdf/2311.02462.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.02462.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.02462)), ([:house:](https://huggingface.co/papers/2311.02462)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/levels-of-agi-operationalizing-progress-on)), ([SS](https://www.semanticscholar.org/paper/Levels-of-AGI%3A-Operationalizing-Progress-on-the-to-Morris-Sohl-Dickstein/a2160ce64f13948222d6619d8b8b3a86d2991772)) |
| 11.4 | Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions ([:x:](https://arxiv.org/abs/2311.07582)), ([:book:](https://browse.arxiv.org/pdf/2311.07582.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.07582.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.07582)), ([:house:](https://huggingface.co/papers/2311.07582)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evaluating-the-potential-of-leading-large)) |
| 11.4 | MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning ([:x:](https://arxiv.org/abs/2311.02303)), ([:book:](https://browse.arxiv.org/pdf/2311.02303.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.02303.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.02303)), ([:house:](https://huggingface.co/papers/2311.02303)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mftcoder-boosting-code-llms-with-multitask)), ([:octocat:](https://github.com/codefuse-ai/mftcoder)![GitHub Repo stars](https://img.shields.io/github/stars/codefuse-ai/mftcoder?style=social))  |
| 11.4 | Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs ([:x:](https://arxiv.org/abs/2311.02262)), ([:book:](https://browse.arxiv.org/pdf/2311.02262.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.02262.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.02262)), ([:house:](https://huggingface.co/papers/2311.02262)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tell-your-model-where-to-attend-post-hoc)), ([:octocat:](https://github.com/qingruzhang/pasta)![GitHub Repo stars](https://img.shields.io/github/stars/qingruzhang/pasta?style=social))  |
| 11.4 | Ultra-Long Sequence Distributed Transformer ([:x:](https://arxiv.org/abs/2311.02382)), ([:book:](https://browse.arxiv.org/pdf/2311.02382.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.02382.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.02382)), ([:house:](https://huggingface.co/papers/2311.02382)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ultra-long-sequence-distributed-transformer)) |
| 11.4 | Meet Grok – Elon Musk’s Answer to ChatGPT ([Tweet](https://twitter.com/BrianRoemmele/status/1720804264300474873)), ([news](https://wccftech.com/meet-grok-elon-musk-answer-to-chatgpt/)) |
| 11.4 | EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision ([:x:](https://arxiv.org/abs/2311.02077)), ([:book:](https://browse.arxiv.org/pdf/2311.02077.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.02077.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.02077)), ([:house:](https://huggingface.co/papers/2311.02077)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/emernerf-emergent-spatial-temporal-scene)) |
| 11.3 | Don't Make Your LLM an Evaluation Benchmark Cheater ([:x:](https://arxiv.org/abs/2311.01964)), ([:book:](https://browse.arxiv.org/pdf/2311.01964.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.01964.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.01964)), ([:house:](https://huggingface.co/papers/2311.01964)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/don-t-make-your-llm-an-evaluation-benchmark)), ([SS](https://www.semanticscholar.org/paper/Don't-Make-Your-LLM-an-Evaluation-Benchmark-Cheater-Zhou-Zhu/84725855d10b531eb8cbe54935dda0440c2fc750)) |
| 11.3 | LLM-driven Multimodal Target Volume Contouring in Radiation Oncology ([:x:](https://arxiv.org/abs/2311.01908)), ([:book:](https://browse.arxiv.org/pdf/2311.01908.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.01908.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.01908)), ([:house:](https://huggingface.co/papers/2311.01908)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llm-driven-multimodal-target-volume)) |
| 11.3 | FinGPT: Large Generative Models for a Small Language ([:x:](https://arxiv.org/abs/2311.05640)), ([:book:](https://browse.arxiv.org/pdf/2311.05640.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.05640.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.05640)), ([:house:](https://huggingface.co/papers/2311.05640)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fingpt-large-generative-models-for-a-small)) |
| 11.3 | :hearts: An Introduction to Natural Language Processing Techniques and Framework for Clinical Implementation in Radiation Oncology ([:x:](https://arxiv.org/abs/2311.02205)), ([:book:](https://browse.arxiv.org/pdf/2311.02205.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.02205.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.02205)), ([:house:](https://huggingface.co/papers/2311.02205)), ([SS](https://www.semanticscholar.org/paper/Large-Language-Models-Illuminate-a-Progressive-to-A-Yuan-Bao/8d2709ed1788a67e64425fb410bb49f3ee49e088)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/an-introduction-to-natural-language)) |
| 11.3 | Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review ([:x:](https://arxiv.org/abs/2311.01918)), ([:book:](https://browse.arxiv.org/pdf/2311.01918.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.01918.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.01918)), ([:house:](https://huggingface.co/papers/2311.01918)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-illuminate-a)), ([:octocat:](https://github.com/mingze-yuan/awesome-llm-healthcare)![GitHub Repo stars](https://img.shields.io/github/stars/mingze-yuan/awesome-llm-healthcare?style=social))  |
| 11.3 | FLAP: Fast Language-Audio Pre-training ([:x:](https://arxiv.org/abs/2311.01615)), ([:book:](https://browse.arxiv.org/pdf/2311.01615.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.01615.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.01615)), ([:house:](https://huggingface.co/papers/2311.01615)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/flap-fast-language-audio-pre-training)) |
| 11.3 | PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion ([:x:](https://arxiv.org/abs/2311.01767)), ([:book:](https://browse.arxiv.org/pdf/2311.01767.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.01767.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.01767)), ([:house:](https://huggingface.co/papers/2311.01767)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pptc-benchmark-evaluating-large-language)) |
| 11.3 | The world’s week on AI safety: powerful computing efforts launched to boost research (nature [doi: https://doi.org/10.1038/d41586-023-03472-x](https://www.nature.com/articles/d41586-023-03472-x)) |
| 11.3 | Forget ChatGPT, why Llama and open source AI win 2023 ([news](https://venturebeat.com/ai/forget-chatgpt-why-llama-and-open-source-ai-win-2023/)) |
| 11.3 | RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation ([:x:](https://arxiv.org/abs/2311.01455)), ([:book:](https://browse.arxiv.org/pdf/2311.01455.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.01455.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.01455)), ([:house:](https://huggingface.co/papers/2311.01455)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/robogen-towards-unleashing-infinite-data-for)) |
| 11.3 | Idempotent Generative Network ([:x:](https://arxiv.org/abs/2311.01462)), ([:book:](https://browse.arxiv.org/pdf/2311.01462.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.01462.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.01462)), ([:house:](https://huggingface.co/papers/2311.01462)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/idempotent-generative-network)) |
| 11.2 | ProAgent: From Robotic Process Automation to Agentic Process Automation ([:x:](https://arxiv.org/abs/2311.10751)), ([:book:](https://browse.arxiv.org/pdf/2311.10751.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10751.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10751)), ([:house:](https://huggingface.co/papers/2311.10751)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/proagent-from-robotic-process-automation-to)) |
| 11.2 | A Survey of Large Language Models for Autonomous Driving ([:x:](https://arxiv.org/abs/2311.01043)), ([:book:](https://browse.arxiv.org/pdf/2311.01043.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.01043.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.01043)), ([:house:](https://huggingface.co/papers/2311.01043)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-survey-of-large-language-models-for-1)), ([:octocat:](https://github.com/thinklab-sjtu/awesome-llm4ad)![GitHub Repo stars](https://img.shields.io/github/stars/thinklab-sjtu/awesome-llm4ad?style=social))  |
| 11.2 | TopicGPT: A Prompt-based Topic Modeling Framework ([:x:](https://arxiv.org/abs/2311.01449)), ([:book:](https://browse.arxiv.org/pdf/2311.01449.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.01449.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.01449)), ([:house:](https://huggingface.co/papers/2311.01449)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/topicgpt-a-prompt-based-topic-modeling)), ([:octocat:](https://github.com/chtmp223/topicgpt)![GitHub Repo stars](https://img.shields.io/github/stars/chtmp223/topicgpt?style=social))  |
| 11.2 | GOV.UK - Introducing the AI Safety Institute ([news](https://www.gov.uk/government/publications/ai-safety-institute-overview/introducing-the-ai-safety-institute)) |
| 11.2 | US to launch its own AI safety institute ([news](https://www.reuters.com/technology/us-launch-its-own-ai-safety-institute-raimondo-2023-11-01/)) |
| 11.2 | U.S. ARTIFICIAL INTELLIGENCE SAFETY INSTITUTE ([FAQ](https://www.nist.gov/artificial-intelligence/artificial-intelligence-safety-institute)) |
| 11.2 | NIST Seeks Collaborators for Consortium Supporting Artificial Intelligence Safety ([news](https://www.nist.gov/news-events/news/2023/11/nist-seeks-collaborators-consortium-supporting-artificial-intelligence)) |
| 11.2 | RoboVQA: Multimodal Long-Horizon Reasoning for Robotics ([:x:](https://arxiv.org/abs/2311.00899)), ([:book:](https://browse.arxiv.org/pdf/2311.00899.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.00899.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.00899)), ([:house:](https://huggingface.co/papers/2311.00899)), ([:eight_spoked_asterisk:]()) |
| 11.2 | E3 TTS: Easy End-to-End Diffusion-based Text to Speech ([:x:](https://arxiv.org/abs/2311.00945)), ([:book:](https://browse.arxiv.org/pdf/2311.00945.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.00945.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.00945)), ([:house:](https://huggingface.co/papers/2311.00945)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/e3-tts-easy-end-to-end-diffusion-based-text)) |
| 11.2 | In-Context Prompt Editing For Conditional Audio Generation ([:x:](https://arxiv.org/abs/2311.00895)), ([:book:](https://browse.arxiv.org/pdf/2311.00895.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.00895.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.00895)), ([:house:](https://huggingface.co/papers/2311.00895)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/in-context-prompt-editing-for-conditional)) |
| 11.2 | FlashDecoding++: Faster Large Language Model Inference on GPUs ([:x:](https://arxiv.org/abs/2311.01282)), ([:book:](https://browse.arxiv.org/pdf/2311.01282.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.01282.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.01282)), ([:house:](https://huggingface.co/papers/2311.01282)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/flashdecoding-faster-large-language-model)) |
| 11.2 | [The AI Engineer Foundation](https://www.aie.foundation/): Open Source for the Future of AI ([news](https://thenewstack.io/the-ai-engineer-foundation-open-source-for-the-future-of-ai/)), ([:octocat:](https://github.com/AI-Engineer-Foundation/agent-protocol)![GitHub Repo stars](https://img.shields.io/github/stars/AI-Engineer-Foundation/agent-protocol?style=social))  |
| 11.2 | LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing ([:x:](https://arxiv.org/abs/2311.00571)), ([:book:](https://browse.arxiv.org/pdf/2311.00571.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.00571.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.00571)), ([:house:](https://huggingface.co/papers/2311.00571)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llava-interactive-an-all-in-one-demo-for)) |
| 11.2 | De-Diffusion Makes Text a Strong Cross-Modal Interface ([:x:](https://arxiv.org/abs/2311.00618)), ([:book:](https://browse.arxiv.org/pdf/2311.00618.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.00618.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.00618)), ([:house:](https://huggingface.co/papers/2311.00618)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/de-diffusion-makes-text-a-strong-cross-modal)) |
| 11.2 | Controllable Music Production with Diffusion Models and Guidance Gradients ([:x:](https://arxiv.org/abs/2311.00613)), ([:book:](https://browse.arxiv.org/pdf/2311.00613.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.00613.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.00613)), ([:house:](https://huggingface.co/papers/2311.00613)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/controllable-music-production-with-diffusion)) |
| 11.1 | Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models ([:x:](https://arxiv.org/abs/2311.00287)), ([:book:](https://browse.arxiv.org/pdf/2311.00287.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.00287.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.00287)), ([:house:](https://huggingface.co/papers/2311.00287)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/knowledge-infused-prompting-assessing-and)), ([:octocat:](https://github.com/ritaranx/clingen)![GitHub Repo stars](https://img.shields.io/github/stars/ritaranx/clingen?style=social))  |
| 11.1 | AMSP: Super-Scaling LLM Training via Advanced Model States Partitioning ([:x:](https://arxiv.org/abs/2311.00257)), ([:book:](https://browse.arxiv.org/pdf/2311.00257.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.00257.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.00257)), ([:house:](https://huggingface.co/papers/2311.00257)), ([:eight_spoked_asterisk:]()) |
| 11.1 | ChatCoder: Chat-based Refine Requirement Improves LLMs' Code Generation ([:x:](https://arxiv.org/abs/2311.00272)), ([:book:](https://browse.arxiv.org/pdf/2311.00272.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.00272.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.00272)), ([:house:](https://huggingface.co/papers/2311.00272)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chatcoder-chat-based-refine-requirement)) |
| 11.1 | Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans? ([:x:](https://arxiv.org/abs/2311.00047)), ([:book:](https://browse.arxiv.org/pdf/2311.00047.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.00047.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.00047)), ([:house:](https://huggingface.co/papers/2311.00047)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/grounding-visual-illusions-in-language-do)) |
| 11.1 | ChipNeMo: Domain-Adapted LLMs for Chip Design ([:x:](https://arxiv.org/abs/2311.00176)), ([:book:](https://browse.arxiv.org/pdf/2311.00176.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.00176.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.00176)), ([:house:](https://huggingface.co/papers/2311.00176)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chipnemo-domain-adapted-llms-for-chip-design)) |
| 11.1 | Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling ([:x:](https://arxiv.org/abs/2311.00430)), ([:book:](https://browse.arxiv.org/pdf/2311.00430.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.00430.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.00430)), ([:house:](https://huggingface.co/papers/2311.00430)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/distil-whisper-robust-knowledge-distillation)), ([:octocat:](https://github.com/huggingface/distil-whisper)![GitHub Repo stars](https://img.shields.io/github/stars/huggingface/distil-whisper?style=social))  | 
| 11.1 | The Generative AI Paradox: "What It Can Create, It May Not Understand" ([:x:](https://arxiv.org/abs/2311.00059)), ([:book:](https://browse.arxiv.org/pdf/2311.00059.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.00059.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.00059)), ([:house:](https://huggingface.co/papers/2311.00059)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-generative-ai-paradox-what-it-can-create)) | 
| 11.1 | Learning From Mistakes Makes LLM Better Reasoner ([:x:](https://arxiv.org/abs/2310.20689)), ([:book:](https://browse.arxiv.org/pdf/2310.20689.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.20689.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.20689)), ([:house:](https://huggingface.co/papers/2310.20689)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/learning-from-mistakes-makes-llm-better)), ([:octocat:](https://github.com/microsoft/codet)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/codet?style=social)) |
| 11.1 | Exclusive: Stability AI brings advanced 3D and image fine-tuning to Stable Diffusion (VentureBeat[news](https://venturebeat.com/ai/exclusive-stability-ai-brings-advanced-3d-and-image-fine-tuning-to-stable-diffusion/)) 
| 11.1 | An Early Look at Stability AI's New Text to 3D Model ([news](https://www.maginative.com/article/an-early-look-at-stability-ais-new-text-to-3d-model/)) |
| 11.1 | Microsoft 365 Copilot is available for purchase starting today. Here's what to know (ZDnet [news](https://www.zdnet.com/article/what-is-microsoft-copilot-heres-everything-you-need-to-know/)) |  
| 11.1 | GOV.UK - The Bletchley Declaration by Countries Attending the AI Safety Summit, 1-2 November 2023 ([Policy paper](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023)) |
| 11.1 | Generative AI for Beginners - A Course ([:octocat:](https://github.com/microsoft/generative-ai-for-beginners)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/generative-ai-for-beginners?style=social)) | 
| 11.1 | GOV.UK - Countries agree to safe and responsible development of frontier AI in landmark Bletchley Declaration ([press](https://www.gov.uk/government/news/countries-agree-to-safe-and-responsible-development-of-frontier-ai-in-landmark-bletchley-declaration)) |
| 11.1 | JADE: A Linguistics-based Safety Evaluation Platform for LLM ([:x:](https://arxiv.org/abs/2311.00286)), ([:book:](https://browse.arxiv.org/pdf/2311.00286.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.00286.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.00286)), ([:house:](https://huggingface.co/papers/2311.00286)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/jade-a-linguistic-based-safety-evaluation)), ([:octocat:](https://github.com/whitzard-ai/jade-db)![GitHub Repo stars](https://img.shields.io/github/stars/whitzard-ai/jade-db?style=social))  |
| 10.31 | Taking control: Policies to address extinction risks from advanced AI ([:x:](https://arxiv.org/abs/2310.20563)), ([:book:](https://browse.arxiv.org/pdf/2310.20563.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.20563.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.20563)), ([:house:](https://huggingface.co/papers/2310.20563)), ([:eight_spoked_asterisk:]()) |
| 10.31 | Does GPT-4 Pass the Turing Test? ([:x:](https://arxiv.org/abs/2310.20216)), ([:book:](https://browse.arxiv.org/pdf/2310.20216.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.20216.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.20216)), ([:house:](https://huggingface.co/papers/2310.20216)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/does-gpt-4-pass-the-turing-test)), ([test](https://turingtest.live/)) |
| 10.31 | :hearts: A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging ([:x:](https://arxiv.org/abs/2310.20381)), ([:book:](https://browse.arxiv.org/pdf/2310.20381.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.20381.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.20381)), ([:house:](https://huggingface.co/papers/2310.20381)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-comprehensive-study-of-gpt-4v-s-multimodal)) |
| 10.31 | Artificial intelligence - UK Regulatory Outlook October 2023 ([news](https://www.osborneclarke.com/insights/Regulatory-Outlook-October-2023-Artificial-intelligence)) |
| 10.31 | MM-VID: Advancing Video Understanding with GPT-4V(ision) ([:x:](https://arxiv.org/abs/2310.19773)), ([:book:](https://browse.arxiv.org/pdf/2310.19773.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.19773.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.19773)), ([:house:](https://huggingface.co/papers/2310.19773)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mm-vid-advancing-video-understanding-with-gpt)) |
| 10.30 | EHRTutor: Enhancing Patient Understanding of Discharge Instructions ([:x:](https://arxiv.org/abs/2310.19212)), ([:book:](https://browse.arxiv.org/pdf/2310.19212.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.19212.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.19212)), ([:house:](https://huggingface.co/papers/2310.19212)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ehrtutor-enhancing-patient-understanding-of)) |
| 10.30 | Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities ([:x:](https://arxiv.org/abs/2310.19626)), ([:book:](https://browse.arxiv.org/pdf/2310.19626.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.19626.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.19626)), ([:house:](https://huggingface.co/papers/2310.19626)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/transformation-vs-tradition-artificial)) |
| 10.30 | Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models ([:x:](https://arxiv.org/abs/2310.19619)), ([:book:](https://browse.arxiv.org/pdf/2310.19619.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.19619.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.19619)), ([:house:](https://huggingface.co/papers/2310.19619)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-a-holistic-landscape-of-situated)), ([:octocat:](https://github.com/mars-tin/awesome-theory-of-mind)![GitHub Repo stars](https://img.shields.io/github/stars/mars-tin/awesome-theory-of-mind?style=social))  |
| 10.30 | RedPajama-Data-v2: an Open Dataset with 30 Trillion Tokens for Training Large Language Models ([blog](https://together.ai/blog/redpajama-data-v2)), ([:octocat:](https://github.com/togethercomputer/RedPajama-Data)![GitHub Repo stars](https://img.shields.io/github/stars/togethercomputer/RedPajama-Data?style=social)) |
| 10.30 | Awesome LLMs Evaluation Papers ([:octocat:](https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers)![GitHub Repo stars](https://img.shields.io/github/stars/tjunlp-lab/Awesome-LLMs-Evaluation-Papers?style=social)) |
| 10.30 | Evaluating Large Language Models: A Comprehensive Survey ([:x:](https://arxiv.org/abs/2310.19736)), ([:book:](https://browse.arxiv.org/pdf/2310.19736.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.19736.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.19736)), ([:house:](https://huggingface.co/papers/2310.19736)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evaluating-large-language-models-a)), ([SS](https://www.semanticscholar.org/paper/Evaluating-Large-Language-Models%3A-A-Comprehensive-Guo-Jin/2f13f56646f23f35f21d12a763f9a52f5cadab0a#related-papers)), ([:octocat:](https://github.com/tjunlp-lab/awesome-llms-evaluation-papers)![GitHub Repo stars](https://img.shields.io/github/stars/tjunlp-lab/awesome-llms-evaluation-papers?style=social))  |
| 10.30 | Phishing emails increase over 1,200 percent since ChatGPT launch ([news](https://betanews.com/2023/10/30/phishing-emails-increase-over-1200-percent-since-chatgpt-launch/)) |
| 10.30 | G7 Leaders’ Statement on the Hiroshima AI Process ([statement](https://digital-strategy.ec.europa.eu/en/library/g7-leaders-statement-hiroshima-ai-process)), ([download](https://ec.europa.eu/newsroom/dae/redirection/document/99644)), ([white house](https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/g7-leaders-statement-on-the-hiroshima-ai-process/)) |
| 10.30 | Commission welcomes G7 leaders' agreement on Guiding Principles and a Code of Conduct on Artificial Intelligence ([news](https://ec.europa.eu/commission/presscorner/detail/en/ip_23_5379)) |
| 10.30 | Hiroshima Process International Guiding Principles for Advanced AI system ([news](https://digital-strategy.ec.europa.eu/en/library/hiroshima-process-international-guiding-principles-advanced-ai-system)), ([download](https://ec.europa.eu/newsroom/dae/redirection/document/99643)) |
| 10.30 | Hiroshima Process International Code of Conduct for Advanced AI Systems ([news](https://digital-strategy.ec.europa.eu/en/library/hiroshima-process-international-code-conduct-advanced-ai-systems)), ([download](https://ec.europa.eu/newsroom/dae/redirection/document/99641)) |
| 10.30 | FACT SHEET: President Biden Issues Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence ([news](https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/)) |
| 10.30 | Joe Biden’s Sweeping New Executive Order Aims to Drag the US Government Into the Age of ChatGPT (Wired [news](https://www.wired.com/story/joe-bidens-executive-order-ai-us-government-chatgpt/)) |
| 10.30 | :hearts: Multimodal ChatGPT for Medical Applications: an Experimental Study of GPT-4V ([:x:](https://arxiv.org/abs/2310.19061)), ([:paperclip:](https://arxiv.org/pdf/2310.19061.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.19061)), ([:house:](https://huggingface.co/papers/2310.19061)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/multimodal-chatgpt-for-medical-applications)), ([:octocat:](https://github.com/zhilingyan/gpt4v-medical-report)![GitHub Repo stars](https://img.shields.io/github/stars/zhilingyan/gpt4v-medical-report?style=social))  |
| 10.30 | Atom: Low-bit Quantization for Efficient and Accurate LLM Serving ([:x:](https://arxiv.org/abs/2310.19102)), ([:paperclip:](https://arxiv.org/pdf/2310.19102.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.19102)), ([:house:](https://huggingface.co/papers/2310.19102)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/atom-low-bit-quantization-for-efficient-and)) |
| 10.30 | Skywork: A More Open Bilingual Foundation Model ([:x:](https://arxiv.org/abs/2310.19341)), ([:paperclip:](https://arxiv.org/pdf/2310.19341.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.19341)), ([:house:](https://huggingface.co/papers/2310.19341)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/skywork-a-more-open-bilingual-foundation)), ([:octocat:](https://github.com/skyworkai/skywork)![GitHub Repo stars](https://img.shields.io/github/stars/skyworkai/skywork?style=social)) |
| 10.30 | VideoCrafter1: Open Diffusion Models for High-Quality Video Generation ([:x:](https://arxiv.org/abs/2310.19512)), ([:paperclip:](https://arxiv.org/pdf/2310.19512.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.19512)), ([:house:](https://huggingface.co/papers/2310.19512)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/videocrafter1-open-diffusion-models-for-high)), ([:octocat:](https://github.com/ailab-cvc/videocrafter)![GitHub Repo stars](https://img.shields.io/github/stars/ailab-cvc/videocrafter?style=social))  |
| 10.30 | Text-to-3D with classifier score distillation ([:x:](https://arxiv.org/abs/2310.19415)), ([:paperclip:](https://arxiv.org/pdf/2310.19415.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.19415)), ([:house:](https://huggingface.co/papers/2310.19415)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/text-to-3d-with-classifier-score-distillation)) |
| 10.29 | TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise ([:x:](https://arxiv.org/abs/2310.19019)), ([:paperclip:](https://arxiv.org/pdf/2310.19019.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.19019)), ([:house:](https://huggingface.co/papers/2310.19019)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/teacherlm-teaching-to-fish-rather-than-giving)) |
| 10.28 | Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision ([:x:](https://arxiv.org/abs/2310.18689)), ([:book:](https://browse.arxiv.org/pdf/2310.18689.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.18689.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.18689)), ([:house:](https://huggingface.co/papers/2310.18689)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/foundational-models-in-medical-imaging-a)), ([:octocat:](https://github.com/xmindflow/Awesome-Foundation-Models-in-Medical-Imaging)![GitHub Repo stars](https://img.shields.io/github/stars/xmindflow/Awesome-Foundation-Models-in-Medical-Imaging?style=social)), ([SS](https://www.semanticscholar.org/paper/Foundational-Models-in-Medical-Imaging%3A-A-Survey-Azad-Azad/8e5d42f5b98146d0784fe85e29c768a4989e1478))  |
| 10.28 | Overview of Current Applications of Large Language Models in Various Medical Specialities ([:x:](https://arxiv.org/abs/2310.12882)), ([:book:](https://browse.arxiv.org/pdf/2310.12882.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.12882.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.12882)), ([:house:](https://huggingface.co/papers/2310.12882)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/overview-of-current-applications-of-large)) |
| 10.28 | Punica: Multi-Tenant LoRA Serving ([:x:](https://arxiv.org/abs/2310.18547)), ([:book:](https://browse.arxiv.org/pdf/2310.18547.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.18547.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.18547)), ([:house:](https://huggingface.co/papers/2310.18547)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/punica-multi-tenant-lora-serving)), ([:octocat:](https://github.com/punica-ai/punica)![GitHub Repo stars](https://img.shields.io/github/stars/punica-ai/punica?style=social))  |
| 10.28 | Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation ([:x:](https://arxiv.org/abs/2310.18628)), ([:paperclip:](https://arxiv.org/pdf/2310.18628.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.18628)), ([:house:](https://huggingface.co/papers/2310.18628)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/personalised-distillation-empowering-open)) |
| 10.27 | :hearts: Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare ([:x:](https://arxiv.org/abs/2310.17956)), ([:book:](https://browse.arxiv.org/pdf/2310.17956.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.17956.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.17956)), ([:house:](https://huggingface.co/papers/2310.17956)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/qilin-med-vl-towards-chinese-large-vision)), ([:octocat:](https://github.com/williamliujl/qilin-med-vl)![GitHub Repo stars](https://img.shields.io/github/stars/williamliujl/qilin-med-vl?style=social))  |
| 10.27 | JudgeLM: Fine-tuned Large Language Models are Scalable Judges ([:x:](https://arxiv.org/abs/2310.17631)), ([:paperclip:](https://arxiv.org/pdf/2310.17631.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.17631)), ([:house:](https://huggingface.co/papers/2310.17631)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/judgelm-fine-tuned-large-language-models-are)), ([:octocat:](https://github.com/baaivision/judgelm)![GitHub Repo stars](https://img.shields.io/github/stars/baaivision/judgelm?style=social)) |
| 10.27 | A Framework for Automated Measurement of Responsible AI Harms in Generative AI Applications ([:x:](https://arxiv.org/abs/2310.17750)), ([:paperclip:](https://arxiv.org/pdf/2310.17750.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.17750)), ([:house:](https://huggingface.co/papers/2310.17750)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-framework-for-automated-measurement-of)) |
| 10.27 | ControlLLM: Augment Language Models with Tools by Searching on Graphs ([:x:](https://arxiv.org/abs/2310.17796)), ([:paperclip:](https://arxiv.org/pdf/2310.17796.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.17796)), ([:house:](https://huggingface.co/papers/2310.17796)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/controlllm-augment-language-models-with-tools)) |
| 10.27 | FP8-LM: Training FP8 Large Language Models ([:x:](https://arxiv.org/abs/2310.18313)), ([:paperclip:](https://arxiv.org/pdf/2310.18313.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.18313)), ([:house:](https://huggingface.co/papers/2310.18313)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fp8-lm-training-fp8-large-language-models)), ([:octocat:](https://github.com/azure/ms-amp)![GitHub Repo stars](https://img.shields.io/github/stars/azure/ms-amp?style=social))  |
| 10.27 | United Nations creates advisory body to address AI governance (Reuters [news](https://www.reuters.com/technology/united-nations-creates-advisory-body-address-ai-governance-2023-10-26/), [UN AI Advisory Body](https://www.un.org/ai-advisory-body)) |
| 10.27 | Guarding the AI frontier: A proposal for federal regulation ([news](https://www.c4isrnet.com/opinion/2023/10/26/guarding-the-ai-frontier-a-proposal-for-federal-regulation/)) |
| 10.27 | GOV.UK - Emerging processes for frontier AI safety ([white paper](https://www.gov.uk/government/publications/emerging-processes-for-frontier-ai-safety) - [HTML](https://www.gov.uk/government/publications/emerging-processes-for-frontier-ai-safety/emerging-processes-for-frontier-ai-safety), [PDF](https://assets.publishing.service.gov.uk/media/653aabbd80884d000df71bdc/emerging-processes-frontier-ai-safety.pdf)) |
| 10.27 | GOV.UK - Leading frontier AI companies publish safety policies ([news](https://www.gov.uk/government/news/leading-frontier-ai-companies-publish-safety-policies)) |
| 10.26 | Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting ([:x:](https://arxiv.org/abs/2310.17811)), ([:book:](https://browse.arxiv.org/pdf/2310.17811.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.17811.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.17811)), ([:house:](https://huggingface.co/papers/2310.17811)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/style-aware-radiology-report-generation-with)) |
| 10.26 | UK Prime Minister announces world’s first AI Safety Institute ([news](https://www.csoonline.com/article/657279/uk-prime-minister-announces-worlds-first-ai-safety-institute.html)) |
| 10.26 | Using fine-tuned large language models to parse clinical notes in musculoskeletal pain disorders (Lancet [https://doi.org/10.1016/S2589-7500(23)00202-9](https://www.thelancet.com/journals/landig/article/PIIS2589-7500(23)00202-9/fulltext)) |
| 10.26 | Large Language Models as Generalizable Policies for Embodied Tasks ([project](https://llm-rl.github.io/)), ([:x:](https://arxiv.org/abs/2310.17722)), ([:book:](https://browse.arxiv.org/pdf/2310.17722.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.17722.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.17722)), ([:house:](https://huggingface.co/papers/2310.17722)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-as-generalizable)) |
| 10.26 | How the Foundation Model Transparency Index Distorts Transparency ([blog](https://blog.eleuther.ai/fmti-critique/)) |
| 10.26 | Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time ([:x:](https://arxiv.org/abs/2310.17157)), ([:paperclip:](https://arxiv.org/pdf/2310.17157.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.17157)), ([:house:](https://huggingface.co/papers/2310.17157)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/deja-vu-contextual-sparsity-for-efficient)), ([:octocat:](https://github.com/fminference/dejavu)![GitHub Repo stars](https://img.shields.io/github/stars/fminference/dejavu?style=social)) |
| 10.26 | Controlled Decoding from Language Models ([:x:](https://arxiv.org/abs/2310.17022)), ([:paperclip:](https://arxiv.org/pdf/2310.17022.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.17022)), ([:house:](https://huggingface.co/papers/2310.17022)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/controlled-decoding-from-language-models)) |
| 10.26 | HyperFields: Towards Zero-Shot Generation of NeRFs from Text ([:x:](https://arxiv.org/abs/2310.17075)), ([:paperclip:](https://arxiv.org/pdf/2310.17075.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.17075)), ([:house:](https://huggingface.co/papers/2310.17075)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hyperfields-towards-zero-shot-generation-of)) |
| 10.26 | CodeFusion: A Pre-trained Diffusion Model for Code Generation ([:x:](https://arxiv.org/abs/2310.17680)), ([:paperclip:](https://arxiv.org/pdf/2310.17680.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.17680)), ([:house:](https://huggingface.co/papers/2310.17680)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/codefusion-a-pre-trained-diffusion-model-for)) |
| 10.26 | BostonDynamics - a robot tour guide using Spot integrated with Chat GPT and other AI models as a proof of concept for the robotics applications of foundational models ([Youtube](https://www.youtube.com/watch?v=djzOBZUFzTw)) |
| 10.26 | A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation ([:x:](https://arxiv.org/abs/2310.16656)), ([:paperclip:](https://arxiv.org/pdf/2310.16656.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.16656)), ([:house:](https://huggingface.co/papers/2310.16656)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-picture-is-worth-a-thousand-words)) |
| 10.26 | DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior ([:x:](https://arxiv.org/abs/2310.16818)), ([:paperclip:](https://arxiv.org/pdf/2310.16818.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.16818)), ([:house:](https://huggingface.co/papers/2310.16818)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dreamcraft3d-hierarchical-3d-generation-with)) |
| 10.25 | Gov.UK - Frontier AI: capabilities and risks – discussion paper ([paper](https://www.gov.uk/government/publications/frontier-ai-capabilities-and-risks-discussion-paper/frontier-ai-capabilities-and-risks-discussion-paper)) |
| 10.25 | Qualcomm Raises Bar for On-Device Generative AI at Snapdragon Summit ([news](https://futurumgroup.com/insights/qualcomm-raises-bar-for-on-device-generative-ai-at-snapdragon-summit/)) - ([Keynote](https://www.qualcomm.com/company/events/snapdragon-summit/announcements)) |
| 10.25 | Artificial Intelligence in Health Care: Peter Lee on Empathy, Empowerment, and Equity ([blog](https://www.ihi.org/insights/artificial-intelligence-health-care-peter-lee-empathy-empowerment-and-equity)) |
| 10.25 | :hearts: An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives ([:x:](https://arxiv.org/abs/2310.17017)), ([:book:](https://browse.arxiv.org/pdf/2310.17017.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.17017.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.17017)), ([:house:](https://huggingface.co/papers/2310.17017)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/an-integrative-survey-on-mental-health)), ([:octocat:](https://github.com/jeffreych0/mental_chatbot_survey)![GitHub Repo stars](https://img.shields.io/github/stars/jeffreych0/mental_chatbot_survey?style=social))  |
| 10.25 | OpenAI - Frontier risk and preparedness ([Blog](https://openai.com/blog/frontier-risk-and-preparedness)) |
| 10.25 | Together with Anthropic, Google, and Microsoft, we’re announcing the new Executive Director of the Frontier Model Forum and a new $10 million AI Safety Fund ([blog](https://openai.com/blog/frontier-model-forum-updates)) |
| 10.25 | An Early Evaluation of GPT-4V(ision) ([:x:](https://arxiv.org/abs/2310.15504)), ([:book:](https://browse.arxiv.org/pdf/2310.15504.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.15504.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.15504)), ([:house:](https://huggingface.co/papers/2310.15504)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/an-early-evaluation-of-gpt-4v-ision)) |
| 10.25 | In-Context Learning Creates Task Vectors ([:x:](https://arxiv.org/abs/2310.15916)), ([:paperclip:](https://arxiv.org/pdf/2310.15916.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.15916)), ([:house:](https://huggingface.co/papers/2310.15916)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/in-context-learning-creates-task-vectors)) |
| 10.25 | Woodpecker: Hallucination Correction for Multimodal Large Language Models ([:x:](https://arxiv.org/abs/2310.16045)), ([:paperclip:](https://arxiv.org/pdf/2310.16045.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.16045)), ([:house:](https://huggingface.co/papers/2310.16045)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/woodpecker-hallucination-correction-for)), ([:octocat:](https://github.com/byfu/woodpecker)![GitHub Repo stars](https://img.shields.io/github/stars/bradyfu/woodpecker?style=social))  |
| 10.25 | Dissecting In-Context Learning of Translations in GPTs ([:x:](https://arxiv.org/abs/2310.15987)), ([:paperclip:](https://arxiv.org/pdf/2310.15987.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.15987)), ([:house:](https://huggingface.co/papers/2310.15987)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dissecting-in-context-learning-of)) |
| 10.24 | BLESS: Benchmarking Large Language Models on Sentence Simplification ([:x:](https://arxiv.org/abs/2310.15773)), ([:book:](https://browse.arxiv.org/pdf/2310.15773.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.15773.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.15773)), ([:house:](https://huggingface.co/papers/2310.15773)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/bless-benchmarking-large-language-models-on)), ([:octocat:](https://github.com/zurichnlp/bless)![GitHub Repo stars](https://img.shields.io/github/stars/zurichnlp/bless?style=social)) |
| 10.24 | NoteChat: A Dataset of Synthetic Doctor-Patient Conversations Conditioned on Clinical Notes ([:x:](https://arxiv.org/abs/2310.15959)), ([:book:](https://browse.arxiv.org/pdf/2310.15959.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.15959.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.15959)), ([:house:](https://huggingface.co/papers/2310.15959)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/notechat-a-dataset-of-synthetic-doctor)) |
| 10.24 | Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature ([:x:](https://arxiv.org/abs/2310.16146)), ([:book:](https://browse.arxiv.org/pdf/2310.16146.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.16146.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.16146)), ([:house:](https://huggingface.co/papers/2310.16146)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/clinfo-ai-an-open-source-retrieval-augmented)) || 10.24 | LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery ([:x:](https://arxiv.org/abs/2310.18356)), ([:paperclip:](https://arxiv.org/pdf/2310.18356.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.18356)), ([:house:](https://huggingface.co/papers/2310.18356)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lorashear-efficient-large-language-model)) |
| 10.24 | SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding ([:x:](https://arxiv.org/abs/2310.15504)), ([:paperclip:](https://arxiv.org/pdf/2310.15504.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.15504)), ([:house:](https://huggingface.co/papers/2310.15504)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sam-clip-merging-vision-foundation-models)) |
| 10.24 | Wonder3D: Single Image to 3D using Cross-Domain Diffusion ([:x:](https://arxiv.org/abs/2310.15008)), ([:paperclip:](https://arxiv.org/pdf/2310.15008.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.15008)), ([:house:](https://huggingface.co/papers/2310.15008)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/wonder3d-single-image-to-3d-using-cross)) |
| 10.24 | Matryoshka Diffusion Models ([:x:](https://arxiv.org/abs/2310.15504)), ([:paperclip:](https://arxiv.org/pdf/2310.15504.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.15504)), ([:house:](https://huggingface.co/papers/2310.15504)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/matryoshka-diffusion-models)) |
| 10.24 | DEsignBench: Exploring and Benchmarking DALL-E 3 for Imagining Visual Design ([:x:](https://arxiv.org/abs/2310.15144)), ([:paperclip:](https://arxiv.org/pdf/2310.15144.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.15144)), ([:house:](https://huggingface.co/papers/2310.15144)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/designbench-exploring-and-benchmarking-dall-e)) |
| 10.24 | FreeNoise: Tuning-Free Longer Video Diffusion Via Noise Rescheduling ([:x:](https://arxiv.org/abs/2310.15169)), ([:paperclip:](https://arxiv.org/pdf/2310.15169.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.15169)), ([:house:](https://huggingface.co/papers/2310.15169)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/freenoise-tuning-free-longer-video-diffusion)), ([:octocat:](https://github.com/arthur-qiu/longercrafter)![GitHub Repo stars](https://img.shields.io/github/stars/arthur-qiu/longercrafter?style=social))  |
| 10.24 | Branch-Solve-Merge Improves Large Language Model Evaluation and Generation ([:x:](https://arxiv.org/abs/2310.15123)), ([:paperclip:](https://arxiv.org/pdf/2310.15123.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.15123)), ([:house:](https://huggingface.co/papers/2310.15123)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/branch-solve-merge-improves-large-language)) |
| 10.23 | Systematic AI Approach for AGI: Addressing Alignment, Energy, and AGI Grand Challenges ([:x:](https://arxiv.org/abs/2310.15274)), ([:book:](https://browse.arxiv.org/pdf/2310.15274.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.15274.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.15274)), ([:house:](https://huggingface.co/papers/2310.15274)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/systematic-ai-approach-for-agi-addressing)) |
| 10.23 | Evaluating Large Language Models on Controlled Generation Tasks ([:x:](https://arxiv.org/abs/2310.14542)), ([:book:](https://browse.arxiv.org/pdf/2310.14542.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.14542.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.14542)), ([:house:](https://huggingface.co/papers/2310.14542)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evaluating-large-language-models-on)) |
| 10.23 | AlpaCare:Instruction-tuned Large Language Models for Medical Application ([:x:](https://arxiv.org/abs/2310.14558)), ([:book:](https://browse.arxiv.org/pdf/2310.14558.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.14558.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.14558)), ([:house:](https://huggingface.co/papers/2310.14558)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/alpacare-instruction-tuned-large-language)), ([:octocat:](https://github.com/xzhang97666/alpacare)![GitHub Repo stars](https://img.shields.io/github/stars/xzhang97666/alpacare?style=social))  |
| 10.23 | Large Search Model: Redefining Search Stack in the Era of LLMs ([:x:](https://arxiv.org/abs/2310.14587)), ([:book:](https://browse.arxiv.org/pdf/2310.14587.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.14587.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.14587)), ([:house:](https://huggingface.co/papers/2310.14587)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-search-model-redefining-search-stack-in)) |
| 10.23 | InstructExcel: A Benchmark for Natural Language Instruction in Excel ([:x:](https://arxiv.org/abs/2310.14495)), ([:paperclip:](https://arxiv.org/pdf/2310.14495.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.14495)), ([:house:](https://huggingface.co/papers/2310.14495)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/instructexcel-a-benchmark-for-natural)) |
| 10.23 | HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models ([:x:](https://arxiv.org/abs/2310.14566)), ([:book:](https://browse.arxiv.org/pdf/2310.14566.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.14566.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.14566)), ([:house:](https://huggingface.co/papers/2310.14566)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hallusionbench-you-see-what-you-think-or-you)), ([:octocat:](https://github.com/tianyi-lab/hallusionbench)![GitHub Repo stars](https://img.shields.io/github/stars/tianyi-lab/hallusionbench?style=social))  |
| 10.23 | Moral Foundations of Large Language Models ([:x:](https://arxiv.org/abs/2310.15337)), ([:paperclip:](https://arxiv.org/pdf/2310.15337.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.15337)), ([:house:](https://huggingface.co/papers/2310.15337)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/moral-foundations-of-large-language-models)) |
| 10.23 | Exploring the Boundaries of GPT-4 in Radiology ([:x:](https://arxiv.org/abs/2310.14573)), ([:paperclip:](https://arxiv.org/pdf/2310.14573.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.14573)), ([:house:](https://huggingface.co/papers/2310.14573)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/exploring-the-boundaries-of-gpt-4-in)) |
| 10.22 | An International Consortium for Evaluations of Societal-Scale Risks from Advanced AI ([:x:](https://arxiv.org/abs/2310.14455)), ([:book:](https://browse.arxiv.org/pdf/2310.14455.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.14455.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.14455)), ([:house:](https://huggingface.co/papers/2310.14455)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/an-international-consortium-for-evaluations)) |
| 10.22 | Assessing the Utilization of Large Language Models in Medical Education: Insights From Undergraduate Medical Students (Cureus [DOI: 10.7759/cureus.47468](https://www.cureus.com/articles/197012-assessing-the-utilization-of-large-language-models-in-medical-education-insights-from-undergraduate-medical-students#!/)) |
| 10.21 | TexFusion: Synthesizing 3D Textures with Text-Guided Image Diffusion Models ([:x:](https://arxiv.org/abs/2310.13772)), ([:paperclip:](https://arxiv.org/pdf/2310.13772.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.13772)), ([:house:](https://huggingface.co/papers/2310.13772)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/texfusion-synthesizing-3d-textures-with-text-1)) |
| 10.21 | Ensemble-Instruct: Generating Instruction-Tuning Data with a Heterogeneous Mixture of LMs ([:x:](https://arxiv.org/abs/2310.13961)), ([:paperclip:](https://arxiv.org/pdf/2310.13961.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.13961)), ([:house:](https://huggingface.co/papers/2310.13961)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ensemble-instruct-generating-instruction)) |
| 10.21 | Specific versus General Principles for Constitutional AI ([:x:](https://arxiv.org/abs/2310.13798)), ([:paperclip:](https://arxiv.org/pdf/2310.13798.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.13798)), ([:house:](https://huggingface.co/papers/2310.13798)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/specific-versus-general-principles-for)) |
| 10.21 | Let's Synthesize Step by Step: Iterative Dataset Synthesis with Large Language Models by Extrapolating Errors from Small Models ([:x:](https://arxiv.org/abs/2310.13671)), ([:paperclip:](https://arxiv.org/pdf/2310.13671.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.13671)), ([:house:](https://huggingface.co/papers/2310.13671)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/let-s-synthesize-step-by-step-iterative)) |
| 10.21 | Contrastive Preference Learning: Learning from Human Feedback without RL ([:x:](https://arxiv.org/abs/2310.13639)), ([:paperclip:](https://arxiv.org/pdf/2310.13639.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.13639)), ([:house:](https://huggingface.co/papers/2310.13639)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/contrastive-prefence-learning-learning-from)) |
| 10.20 | Democratizing Reasoning Ability: Tailored Learning from Large Language Model ([:x:](https://arxiv.org/abs/2310.13332)), ([:paperclip:](https://arxiv.org/pdf/2310.13332.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.13332)), ([:house:](https://huggingface.co/papers/2310.13332)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/democratizing-reasoning-ability-tailored)) |
| 10.20 | DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics ([:x:](https://arxiv.org/abs/2310.13268)), ([:paperclip:](https://arxiv.org/pdf/2310.13268.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.13268)), ([:house:](https://huggingface.co/papers/2310.13268)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dpm-solver-v3-improved-diffusion-ode-solver)), ([:octocat:](https://github.com/thu-ml/dpm-solver-v3)![GitHub Repo stars](https://img.shields.io/github/stars/thu-ml/dpm-solver-v3?style=social))  |
| 10.20 | Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models ([:x:](https://arxiv.org/abs/2310.13127)), ([:paperclip:](https://arxiv.org/pdf/2310.13127.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.13127)), ([:house:](https://huggingface.co/papers/2310.13127)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/auto-instruct-automatic-instruction)) |
| 10.20 | Localizing and Editing Knowledge in Text-to-Image Generative Models ([:x:](https://arxiv.org/abs/2310.13730)), ([:paperclip:](https://arxiv.org/pdf/2310.13730.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.13730)), ([:house:](https://huggingface.co/papers/2310.13730)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/localizing-and-editing-knowledge-in-text-to)) |
| 10.20 | Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots ([:x:](https://arxiv.org/abs/2310.13724)), ([:paperclip:](https://arxiv.org/pdf/2310.13724.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.13724)), ([:house:](https://huggingface.co/papers/2310.13724)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/habitat-3-0-a-co-habitat-for-humans-avatars)) |
| 10.20 | SALMONN: Towards Generic Hearing Abilities for Large Language Models ([:x:](https://arxiv.org/abs/2310.13289)), ([:paperclip:](https://arxiv.org/pdf/2310.13289.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.13289)), ([:house:](https://huggingface.co/papers/2310.13289)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/salmonn-towards-generic-hearing-abilities-for)), ([:octocat:](https://github.com/bytedance/salmonn)![GitHub Repo stars](https://img.shields.io/github/stars/bytedance/salmonn?style=social))  |
| 10.20 | Teaching Language Models to Self-Improve through Interactive Demonstrations ([:x:](https://arxiv.org/abs/2310.13522)), ([:paperclip:](https://arxiv.org/pdf/2310.13522.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.13522)), ([:house:](https://huggingface.co/papers/2310.13522)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/teaching-language-models-to-self-improve)) |
| 10.20 | DreamSpace: Dreaming Your Room Space with Text-Driven Panoramic Texture Propagation ([:x:](https://arxiv.org/abs/2310.13119)), ([:paperclip:](https://arxiv.org/pdf/2310.13119.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.13119)), ([:house:](https://huggingface.co/papers/2310.13119)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dreamspace-dreaming-your-room-space-with-text)) |
| 10.20 | Creative Robot Tool Use with Large Language Models ([:x:](https://arxiv.org/abs/2310.13065)), ([:paperclip:](https://arxiv.org/pdf/2310.13065.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.13065)), ([:house:](https://huggingface.co/papers/2310.13065)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/creative-robot-tool-use-with-large-language)) |
| 10.20 | Tuna: Instruction Tuning using Feedback from Large Language Models ([:x:](https://arxiv.org/abs/2310.13385)), ([:paperclip:](https://arxiv.org/pdf/2310.13385.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.13385)), ([:house:](https://huggingface.co/papers/2310.13385)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tuna-instruction-tuning-using-feedback-from)), ([:octocat:](https://github.com/microsoft/lmops)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/lmops?style=social))  |
| 10.20 | ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search ([:x:](https://arxiv.org/abs/2310.13227)), ([:paperclip:](https://arxiv.org/pdf/2310.13227.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.13227)), ([:house:](https://huggingface.co/papers/2310.13227)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/toolchain-efficient-action-space-navigation)) |
| 10.20 | SILC: Improving Vision Language Pretraining with Self-Distillation ([:x:](https://arxiv.org/abs/2310.13355)), ([:paperclip:](https://arxiv.org/pdf/2310.13355.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.13355)), ([:house:](https://huggingface.co/papers/2310.13355)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/silc-improving-vision-language-pretraining)) |
| 10.20 | Towards Understanding Sycophancy in Language Models ([:x:](https://arxiv.org/abs/2310.13548)), ([:paperclip:](https://arxiv.org/pdf/2310.13548.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.13548)), ([:house:](https://huggingface.co/papers/2310.13548)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-understanding-sycophancy-in-language)), ([:octocat:](https://github.com/meg-tong/sycophancy-eval)![GitHub Repo stars](https://img.shields.io/github/stars/meg-tong/sycophancy-eval?style=social))  |
| 10.20 | ScaleLong: Towards More Stable Training of Diffusion Model via Scaling Network Long Skip Connection ([:x:](https://arxiv.org/abs/2310.13545)), ([:paperclip:](https://arxiv.org/pdf/2310.13545.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.13545)), ([:house:](https://huggingface.co/papers/2310.13545)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/scalelong-towards-more-stable-training-of)) |
| 10.20 | 3D-GPT: Procedural 3D Modeling with Large Language Models ([:x:](https://arxiv.org/abs/2310.12945)), ([:paperclip:](https://arxiv.org/pdf/2310.12945.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.12945)), ([:house:](https://huggingface.co/papers/2310.12945)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/3d-gpt-procedural-3d-modeling-with-large)) |
| 10.20 | Eureka: Human-Level Reward Design via Coding Large Language Models ([:x:](https://arxiv.org/abs/2310.12931)), ([:paperclip:](https://arxiv.org/pdf/2310.12931.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.12931)), ([:house:](https://huggingface.co/papers/2310.12931)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/eureka-human-level-reward-design-via-coding)), ([:octocat:](https://github.com/eureka-research/Eureka)![GitHub Repo stars](https://img.shields.io/github/stars/eureka-research/Eureka?style=social))  |
| 10.20 | AgentTuning: Enabling Generalized Agent Abilities for LLMs ([:x:](https://arxiv.org/abs/2310.12823)), ([:paperclip:](https://arxiv.org/pdf/2310.12823.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.12823)), ([:house:](https://huggingface.co/papers/2310.12823)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/agenttuning-enabling-generalized-agent)), ([:octocat:](https://github.com/thudm/agenttuning)![GitHub Repo stars](https://img.shields.io/github/stars/thudm/agenttuning?style=social))  |
| 10.20 | Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning ([:x:](https://arxiv.org/abs/2310.12921)), ([:paperclip:](https://arxiv.org/pdf/2310.12921.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.12921)), ([:house:](https://huggingface.co/papers/2310.12921)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vision-language-models-are-zero-shot-reward)) |
| 10.20 | AutoMix: Automatically Mixing Language Models ([:x:](https://arxiv.org/abs/2310.12963)), ([:paperclip:](https://arxiv.org/pdf/2310.12963.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.12963)), ([:house:](https://huggingface.co/papers/2310.12963)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/automix-automatically-mixing-language-models)), ([:octocat:](https://github.com/automix-llm/automix)![GitHub Repo stars](https://img.shields.io/github/stars/automix-llm/automix?style=social))  |
| 10.20 | An Emulator for Fine-Tuning Large Language Models using Small Language Models ([:x:](https://arxiv.org/abs/2310.12962)), ([:paperclip:](https://arxiv.org/pdf/2310.12962.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.12962)), ([:house:](https://huggingface.co/papers/2310.12962)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/an-emulator-for-fine-tuning-large-language)) |
| 10.20 | ChatGPT parent OpenAI seeks $86bn valuation (FT ([news](https://www.ft.com/content/e4ab95c9-5b45-4996-a69e-46075d6428e5)) |
| 10.19 | The Foundation Model Transparency Index ([:x:](https://arxiv.org/abs/2310.12941)), ([:book:](https://browse.arxiv.org/pdf/2310.12941.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.12941.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.12941)), ([:house:](https://huggingface.co/papers/2310.12941)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-foundation-model-transparency-index)), ([SS](https://www.semanticscholar.org/paper/The-Foundation-Model-Transparency-Index-Bommasani-Klyman/07ee3807eb7a67dca8ed7b472c1af4110d97e95a)), ([:octocat:](https://github.com/stanford-crfm/fmti)![GitHub Repo stars](https://img.shields.io/github/stars/stanford-crfm/fmti?style=social))  |
| 10.19 | An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning ([:x:](https://arxiv.org/abs/2310.12274)), ([:paperclip:](https://arxiv.org/pdf/2310.12274.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.12274)), ([:house:](https://huggingface.co/papers/2310.12274)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/an-image-is-worth-multiple-words-learning)) |
| 10.19 | Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative Editing ([:x:](https://arxiv.org/abs/2310.12404)), ([:paperclip:](https://arxiv.org/pdf/2310.12404.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.12404)), ([:house:](https://huggingface.co/papers/2310.15504)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/loop-copilot-conducting-ai-ensembles-for)) |
| 10.19 | Safe RLHF: Safe Reinforcement Learning from Human Feedback  ([:x:](https://arxiv.org/abs/2310.12773)), ([:paperclip:](https://arxiv.org/pdf/2310.12773.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.12773)), ([:house:](https://huggingface.co/papers/2310.12773)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/safe-rlhf-safe-reinforcement-learning-from)), ([:octocat:](https://github.com/pku-alignment/safe-rlhf)![GitHub Repo stars](https://img.shields.io/github/stars/pku-alignment/safe-rlhf?style=social))  |
| 10.19 | Enhancing High-Resolution 3D Generation through Pixel-wise Gradient Clipping ([:x:](https://arxiv.org/abs/2310.12474)), ([:paperclip:](https://arxiv.org/pdf/2310.12474.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.12474)), ([:house:](https://huggingface.co/papers/2310.12474)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/enhancing-high-resolution-3d-generation)), ([:octocat:](https://github.com/fudan-zvg/pgc-3d)![GitHub Repo stars](https://img.shields.io/github/stars/fudan-zvg/pgc-3d?style=social))  |
| 10.18 | DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors ([:x:](https://arxiv.org/abs/2310.12190)), ([:book:](https://browse.arxiv.org/pdf/2310.12190.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.12190.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.12190)), ([:house:](https://huggingface.co/papers/2310.12190)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dynamicrafter-animating-open-domain-images)), ([:octocat:](https://github.com/ailab-cvc/videocrafter)![GitHub Repo stars](https://img.shields.io/github/stars/ailab-cvc/videocrafter?style=social))  |
| 10.18 | Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection ([:x:](https://arxiv.org/abs/2310.11511)), ([:paperclip:](https://arxiv.org/pdf/2310.11511.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.11511)), ([:house:](https://huggingface.co/papers/2310.11511)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/self-rag-learning-to-retrieve-generate-and)), ([:octocat:](https://github.com/AkariAsai/self-rag)![GitHub Repo stars](https://img.shields.io/github/stars/AkariAsai/self-rag?style=social))  |
| 10.18 | MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models ([:x:](https://arxiv.org/abs/2310.11954)), ([:paperclip:](https://arxiv.org/pdf/2310.11954.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.11954)), ([:house:](https://huggingface.co/papers/2310.11954)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/musicagent-an-ai-agent-for-music)), ([:octocat:](https://github.com/microsoft/muzic)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/muzic?style=social))  |
| 10.18 | Progressive3D: Progressively Local Editing for Text-to-3D Content Creation with Complex Semantic Prompts ([:x:](https://arxiv.org/abs/2310.11784)), ([:paperclip:](https://arxiv.org/pdf/2310.11784.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.11784)), ([:house:](https://huggingface.co/papers/2310.11784)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/progressive3d-progressively-local-editing-for)) |
| 10.18 | BitNet: Scaling 1-bit Transformers for Large Language Models ([:x:](https://arxiv.org/abs/2310.11453)), ([:paperclip:](https://arxiv.org/pdf/2310.11453.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.11453)), ([:house:](https://huggingface.co/papers/2310.11453)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/bitnet-scaling-1-bit-transformers-for-large)), ([:octocat:](https://github.com/kyegomez/BitNet)![GitHub Repo stars](https://img.shields.io/github/stars/kyegomez/BitNet?style=social))  |
| 10.18 | 4K4D: Real-Time 4D View Synthesis at 4K Resolution ([:x:](https://arxiv.org/abs/2310.11448)), ([:paperclip:](https://arxiv.org/pdf/2310.11448.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.11448)), ([:house:](https://huggingface.co/papers/2310.11448)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/4k4d-real-time-4d-view-synthesis-at-4k)) |
| 10.18 | VeRA: Vector-based Random Matrix Adaptation ([:x:](https://arxiv.org/abs/2310.11454)), ([:paperclip:](https://arxiv.org/pdf/2310.11454.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.11454)), ([:house:](https://huggingface.co/papers/2310.11454)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vera-vector-based-random-matrix-adaptation)) |
| 10.18 | Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V ([:x:](https://arxiv.org/abs/2310.11441)),  ([:book:](https://browse.arxiv.org/pdf/2310.11441.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.11441.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.11441)), ([:house:](https://huggingface.co/papers/2310.11441)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/set-of-mark-prompting-unleashes-extraordinary)) |
| 10.18 | EvalCrafter: Benchmarking and Evaluating Large Video Generation Models ([:x:](https://arxiv.org/abs/2310.11440)), ([:paperclip:](https://arxiv.org/pdf/2310.11440.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.11440)), ([:house:](https://huggingface.co/papers/2310.11440)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evalcrafter-benchmarking-and-evaluating-large)) |
| 10.17 | Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting ([:x:](https://arxiv.org/abs/2310.11324)), ([:book:](https://browse.arxiv.org/pdf/2310.11324.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.11324.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.11324)), ([:house:](https://huggingface.co/papers/2310.11324)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/quantifying-language-models-sensitivity-to)), ([:octocat:](https://github.com/msclar/formatspread)![GitHub Repo stars](https://img.shields.io/github/stars/msclar/formatspread?style=social))  |
| 10.17 | Integrating LLM, EEG, and Eye-Tracking Biomarker Analysis for Word-Level Neural State Classification in Semantic Inference Reading Comprehension ([:x:](https://arxiv.org/abs/2309.15714)), ([:book:](https://browse.arxiv.org/pdf/2309.15714.pdf)), ([:paperclip:](https://arxiv.org/pdf/2309.15714.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.15714)), ([:house:](https://huggingface.co/papers/2309.15714)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chatgpt-bci-word-level-neural-state)) |
| 10.17 | Emulating Human Cognitive Processes for Expert-Level Medical Question-Answering with Large Language Models ([:x:](https://arxiv.org/abs/2310.11266)), ([:book:](https://browse.arxiv.org/pdf/2310.11266.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.11266.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.11266)), ([:house:](https://huggingface.co/papers/2310.11266)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/emulating-human-cognitive-processes-for)) |
| 10.17 | TEQ: Trainable Equivalent Transformation for Quantization of LLMs ([:x:](https://arxiv.org/abs/2310.10944)), ([:paperclip:](https://arxiv.org/pdf/2310.10944.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.10944)), ([:house:](https://huggingface.co/papers/2310.10944)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/teq-trainable-equivalent-transformation-for)), ([:octocat:](https://github.com/intel/neural-compressor)![GitHub Repo stars](https://img.shields.io/github/stars/intel/neural-compressor?style=social))  |
| 10.17 | LAMP: Learn A Motion Pattern for Few-Shot-Based Video Generation ([:x:](https://arxiv.org/abs/2310.10769)), ([:paperclip:](https://arxiv.org/pdf/2310.10769.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.10769)), ([:house:](https://huggingface.co/papers/2310.10769)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lamp-learn-a-motion-pattern-for-few-shot)), ([:octocat:](https://github.com/RQ-Wu/LAMP)![GitHub Repo stars](https://img.shields.io/github/stars/RQ-Wu/LAMP?style=social))  |
| 10.17 | CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion ([:x:](https://arxiv.org/abs/2310.11248)), ([:paperclip:](https://arxiv.org/pdf/2310.11248.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.11248)), ([:house:](https://huggingface.co/papers/2310.11248)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/crosscodeeval-a-diverse-and-multilingual)) |
| 10.17 | Context-Aware Meta-Learning ([:x:](https://arxiv.org/abs/2310.10971)), ([:paperclip:](https://arxiv.org/pdf/2310.10971.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.10971)), ([:house:](https://huggingface.co/papers/2310.10971)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/context-aware-meta-learning)) |
| 10.17 | H2O Open Ecosystem for State-of-the-art Large Language Models ([:x:](https://arxiv.org/abs/2310.13012)), ([:paperclip:](https://arxiv.org/pdf/2310.13012.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.13012)), ([:house:](https://huggingface.co/papers/2310.13012)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/h2o-open-ecosystem-for-state-of-the-art-large)) |
| 10.17 | In-Context Pretraining: Language Modeling Beyond Document Boundaries ([:x:](https://arxiv.org/abs/2310.10638)), ([:paperclip:](https://arxiv.org/pdf/2310.10638.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.10638)), ([:house:](https://huggingface.co/papers/2310.10638)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/in-context-pretraining-language-modeling)) |
| 10.17 | Interactive Task Planning with Language Models ([:x:](https://arxiv.org/abs/2310.10645)), ([:paperclip:](https://arxiv.org/pdf/2310.10645.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.10645)), ([:house:](https://huggingface.co/papers/2310.10645)), ([:eight_spoked_asterisk:](https://www.nature.com/articles/d41586-023-03235-8)) |
| 10.17 | Video Language Planning ([:x:](https://arxiv.org/abs/2310.15504)), ([:paperclip:](https://arxiv.org/pdf/2310.10625.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.10625)), ([:house:](https://huggingface.co/papers/2310.10625)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/video-language-planning)) |
| 10.16 | OpenAgents: An Open Platform for Language Agents in the Wild ([:x:](https://arxiv.org/abs/2310.10634)), ([:book:](https://browse.arxiv.org/pdf/2310.10634.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.10634.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.10634)), ([:house:](https://huggingface.co/papers/2310.10634)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/openagents-an-open-platform-for-language)), ([:octocat:](https://github.com/xlang-ai/openagents)![GitHub Repo stars](https://img.shields.io/github/stars/xlang-ai/openagents?style=social))  |
| 10.16 | How ChatGPT is transforming the postdoc experience (Nature 622, 655-657 (2023) ([doi: https://doi.org/10.1038/d41586-023-03235-8]()) |
| 10.16 | Llemma: An Open Language Model For Mathematics ([:x:](https://arxiv.org/abs/2310.10631)), ([:paperclip:](https://arxiv.org/pdf/2310.10631.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.10631)), ([:house:](https://huggingface.co/papers/2310.10631)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llemma-an-open-language-model-for-mathematics)), ([:octocat:](https://github.com/EleutherAI/math-lm)![GitHub Repo stars](https://img.shields.io/github/stars/EleutherAI/math-lm?style=social))  |
| 10.15 | AutoAgents: A Framework for Automatic Agent Generation ([:x:](https://arxiv.org/abs/2309.17288)), ([:book:](https://browse.arxiv.org/pdf/2309.17288.pdf)), ([:paperclip:](https://arxiv.org/pdf/2309.17288.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.17288)), ([:house:](https://huggingface.co/papers/2309.17288)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/autoagents-a-framework-for-automatic-agent)), ([:octocat:](https://github.com/Link-AGI/AutoAgents)![GitHub Repo stars](https://img.shields.io/github/stars/Link-AGI/AutoAgents?style=social))  |
| 10.15 | Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis ([:x:](https://arxiv.org/abs/2310.09909)), ([:book:](https://browse.arxiv.org/pdf/2310.09909.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.09909.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.09909)), ([:house:](https://huggingface.co/papers/2310.09909)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/can-gpt-4v-ision-serve-medical-applications)) |
| 10.14 | MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning ([:x:](https://arxiv.org/abs/2310.09478)), ([:paperclip:](https://arxiv.org/pdf/2310.09478.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.09478)), ([:house:](https://huggingface.co/papers/2310.09478)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/minigpt-v2-large-language-model-as-a-unified)), ([:octocat:](https://github.com/vision-cair/minigpt-4)![GitHub Repo stars](https://img.shields.io/github/stars/vision-cair/minigpt-4?style=social))  |
| 10.14 | Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model ([:x:](https://arxiv.org/abs/2310.09520)), ([:paperclip:](https://arxiv.org/pdf/2310.09520.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.09520)), ([:house:](https://huggingface.co/papers/2310.09520)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/reward-augmented-decoding-efficient)), ([:octocat:](https://github.com/haikangdeng/RAD)![GitHub Repo stars](https://img.shields.io/github/stars/haikangdeng/RAD?style=social))  |
| 10.14 | Table-GPT: Table-tuned GPT for Diverse Table Tasks ([:x:](https://arxiv.org/abs/2310.09263)), ([:paperclip:](https://arxiv.org/pdf/2310.09263.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.09263)), ([:house:](https://huggingface.co/papers/2310.09263)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/table-gpt-table-tuned-gpt-for-diverse-table)) |
| 10.14 | PaLI-3 Vision Language Models: Smaller, Faster, Stronger ([:x:](https://arxiv.org/abs/2310.09199)), ([:paperclip:](https://arxiv.org/pdf/2310.09199.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.09199)), ([:house:](https://huggingface.co/papers/2310.09199)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pali-3-vision-language-models-smaller-faster)), ([:octocat:](https://github.com/kyegomez/PALI3)![GitHub Repo stars](https://img.shields.io/github/stars/kyegomez/PALI3?style=social))  |
| 10.13 | Multinational AGI Consortium (MAGIC): A Proposal for International Coordination on AI ([:x:](https://arxiv.org/abs/2310.09217)), ([:book:](https://browse.arxiv.org/pdf/2310.09217.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.09217.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.09217)), ([:house:](https://huggingface.co/papers/2310.09217)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/multinational-agi-consortium-magic-a-proposal)) |
| 10.13 | A Zero-Shot Language Agent for Computer Control with Structured Reflection ([:x:](https://arxiv.org/abs/2310.08740)), ([:paperclip:](https://arxiv.org/pdf/2310.08740.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.08740)), ([:house:](https://huggingface.co/papers/2310.08740)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-zero-shot-language-agent-for-computer)) |
| 10.13 | The Consensus Game: Language Model Generation via Equilibrium Search ([:x:](https://arxiv.org/abs/2310.09139)), ([:paperclip:](https://arxiv.org/pdf/2310.09139.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.09139)), ([:house:](https://huggingface.co/papers/2310.09139)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-consensus-game-language-model-generation)) |
| 10.13 | LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models ([:x:](https://arxiv.org/abs/2310.08659)), ([:paperclip:](https://arxiv.org/pdf/2310.08659.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.08659)), ([:house:](https://huggingface.co/papers/2310.08659)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/loftq-lora-fine-tuning-aware-quantization-for)) |
| 10.13 | CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules ([:x:](https://arxiv.org/abs/2310.08992)), ([:paperclip:](https://arxiv.org/pdf/2310.08992.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.08992)), ([:house:](https://huggingface.co/papers/2310.08992)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/codechain-towards-modular-code-generation)) |
| 10.13 | Toward Joint Language Modeling for Speech Units and Text ([:x:](https://arxiv.org/abs/2310.08715)), ([:paperclip:](https://arxiv.org/pdf/2310.08715.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.08715)), ([:house:](https://huggingface.co/papers/2310.08715)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/toward-joint-language-modeling-for-speech)) |
| 10.13 | Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation ([:x:](https://arxiv.org/abs/2310.08541)), ([:paperclip:](https://arxiv.org/pdf/2310.08541.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.08541)), ([:house:](https://huggingface.co/papers/2310.08541)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/idea2img-iterative-self-refinement-with-gpt)) |
| 10.13 | HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion ([:x:](https://arxiv.org/abs/2310.08579)), ([:paperclip:](https://arxiv.org/pdf/2310.08579.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.08579)), ([:house:](https://huggingface.co/papers/2310.08579)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hyperhuman-hyper-realistic-human-generation)) |
| 10.13 | GaussianDreamer: Fast Generation from Text to 3D Gaussian Splatting with Point Cloud Priors ([:x:](https://arxiv.org/abs/2310.08529)), ([:paperclip:](https://arxiv.org/pdf/2310.08529.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.08529)), ([:house:](https://huggingface.co/papers/2310.08529)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gaussiandreamer-fast-generation-from-text-to)) |
| 10.13 | MotionDirector: Motion Customization of Text-to-Video Diffusion Models ([:x:](https://arxiv.org/abs/2310.08465)), ([:paperclip:](https://arxiv.org/pdf/2310.08465.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.08465)), ([:house:](https://huggingface.co/papers/2310.08465)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/motiondirector-motion-customization-of-text)) |
| 10.12 | Organizational preparedness for the use of large language models in pathology informatics (Journal of Pathology Informatics, [https://doi.org/10.1016/j.jpi.2023.100338](https://www.sciencedirect.com/science/article/pii/S2153353923001529)) |
| 10.12 | :hearts: FDA creates new advisory committee for digital health and AI ([news](https://www.medicaldevice-network.com/news/fda-creates-new-advisory-committee-for-digital-health-and-ai/)) |
| 10.12 | EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation ([:x:](https://arxiv.org/abs/2310.08185)), ([:paperclip:](https://arxiv.org/pdf/2310.08185.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.08185)), ([:house:](https://huggingface.co/papers/2310.08185)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/eipe-text-evaluation-guided-iterative-plan)) |
| 10.12 | LangNav: Language as a Perceptual Representation for Navigation ([:x:](https://arxiv.org/abs/2310.07889)), ([:paperclip:](https://arxiv.org/pdf/2310.07889.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.07889)), ([:house:](https://huggingface.co/papers/2310.07889)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/langnav-language-as-a-perceptual)) |
| 10.12 | Octopus: Embodied Vision-Language Programmer from Environmental Feedback ([:x:](https://arxiv.org/abs/2310.08588)), ([:paperclip:](https://arxiv.org/pdf/2310.08588.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.08588)), ([:house:](https://huggingface.co/papers/2310.08588)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/octopus-embodied-vision-language-programmer)), ([:octocat:](https://github.com/dongyh20/octopus)![GitHub Repo stars](https://img.shields.io/github/stars/dongyh20/octopus?style=social))  |
| 10.12 | Prometheus: Inducing Fine-grained Evaluation Capability in Language Models ([:x:](https://arxiv.org/abs/2310.08491)), ([:paperclip:](https://arxiv.org/pdf/2310.08491.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.08491)), ([:house:](https://huggingface.co/papers/2310.08491)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/prometheus-inducing-fine-grained-evaluation)), ([:octocat:](https://github.com/kaistAI/Prometheus)![GitHub Repo stars](https://img.shields.io/github/stars/kaistAI/Prometheus?style=social))  |
| 10.12 | Can GPT models be Financial Analysts? An Evaluation of ChatGPT and GPT-4 on mock CFA Exams ([:x:](https://arxiv.org/abs/2310.08678)), ([:paperclip:](https://arxiv.org/pdf/2310.08678.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.08678)), ([:house:](https://huggingface.co/papers/2310.08678)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/can-gpt-models-be-financial-analysts-an)) |
| 10.11 | Exploring the Landscape of Large Language Models In Medical Question Answering: Observations and Open Questions ([:x:](https://arxiv.org/abs/2310.07225)), ([:book:](https://browse.arxiv.org/pdf/2310.07225.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.07225.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.07225)), ([:house:](https://huggingface.co/papers/2310.07225)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/exploring-the-landscape-of-large-language)) |
| 10.11 | Lemur: Harmonizing Natural Language and Code for Language Agents ([:x:](https://arxiv.org/abs/2310.06830)), ([:paperclip:](https://arxiv.org/pdf/2310.06830.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.06830)), ([:house:](https://huggingface.co/papers/2310.06830)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lemur-harmonizing-natural-language-and-code)), ([:octocat:](https://github.com/openlemur/lemur)![GitHub Repo stars](https://img.shields.io/github/stars/openlemur/lemur?style=social))  |
| 10.11 | Apple - Ferret: Refer and Ground Anything Anywhere at Any Granularity ([:x:](https://arxiv.org/abs/2310.07704)), ([:book:](https://browse.arxiv.org/pdf/2310.07704.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.07704.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.07704)), ([:house:](https://huggingface.co/papers/2310.07704)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ferret-refer-and-ground-anything-anywhere-at)), ([:octocat:](https://github.com/apple/ml-ferret)![GitHub Repo stars](https://img.shields.io/github/stars/apple/ml-ferret?style=social))  |
| 10.10 | Open-Sourcing Highly Capable Foundation Models: An Evaluation of Risks, Benefits, and Alternative Methods for Pursuing Open-Source Objective ([paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4596436)), ([PDF](https://deliverypdf.ssrn.com/delivery.php?ID=563006110027083020093085109105065110102013067092070087126001109100016086077078126113099122116002019025028004087096118030088064126032013032039089005115120023084002095057039003090064121078124029104072125104072001019007125127074008124070029007029104012126&EXT=pdf&INDEX=TRUE)) |
| 10.10 | Teaching Language Models to Hallucinate Less with Synthetic Tasks ([:x:](https://arxiv.org/abs/2310.06827)), ([:book:](https://browse.arxiv.org/pdf/2310.06827.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.06827.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.06827)), ([:house:](https://huggingface.co/papers/2310.06827)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/teaching-language-models-to-hallucinate-less)) |
| 10.10 | Towards Mitigating Hallucination in Large Language Models via Self-Reflection ([:x:](https://arxiv.org/abs/2310.06271)), ([:book:](https://browse.arxiv.org/pdf/2310.06271.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.06271.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.06271)), ([:house:](https://huggingface.co/papers/2310.06271)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-mitigating-hallucination-in-large)) |
| 10.10 | Multilingual Jailbreak Challenges in Large Language Models ([:x:](https://arxiv.org/abs/2310.06474)), ([:book:](https://browse.arxiv.org/pdf/2310.06474.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.06474.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.06474)), ([:house:](https://huggingface.co/papers/2310.06474)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/multilingual-jailbreak-challenges-in-large)), ([:octocat:](https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs)![GitHub Repo stars](https://img.shields.io/github/stars/DAMO-NLP-SG/multilingual-safety-for-LLMs?style=social))  |
| 10.10 | Feasibility of Using the Privacy-preserving Large Language Model Vicuna for Labeling Radiology Reports (RSNA Radiology [https://doi.org/10.1148/radiol.231147](https://pubs.rsna.org/doi/10.1148/radiol.231147)) |
| 10.10 | Benchmarking and Explaining Large Language Model-based Code Generation: A Causality-Centric Approach ([:x:](https://arxiv.org/abs/2310.06680)), ([:paperclip:](https://arxiv.org/pdf/2310.06680.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.06680)), ([:house:](https://huggingface.co/papers/2310.06680)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/benchmarking-and-explaining-large-language)), ([:octocat:](https://github.com/brucewlee/lftk)![GitHub Repo stars](https://img.shields.io/github/stars/brucewlee/lftk?style=social))  |
| 10.10 | How ChatGPT and other AI tools could disrupt scientific publishing (Nature 622, 234-236 (2023) [doi: https://doi.org/10.1038/d41586-023-03144-w](https://www.nature.com/articles/d41586-023-03144-w)) |
| 10.9 | GraphLLM: Boosting Graph Reasoning Ability of Large Language Model ([:x:](https://arxiv.org/abs/2310.05845)), ([:book:](https://browse.arxiv.org/pdf/2310.05845.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.05845.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.05845)), ([:house:](https://huggingface.co/papers/2310.05845)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/graphllm-boosting-graph-reasoning-ability-of)), ([:octocat:](https://github.com/mistyreed63849/graph-llm)![GitHub Repo stars](https://img.shields.io/github/stars/ mistyreed63849/graph-llm?style=social))  |
| 10.9 | HyperAttention: Long-context Attention in Near-Linear Time ([:x:](https://arxiv.org/abs/2310.05869)), ([:book:](https://browse.arxiv.org/pdf/2310.05869.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.05869.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.05869)), ([:house:](https://huggingface.co/papers/2310.05869)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hyperattention-long-context-attention-in-near)), ([:octocat:](https://github.com/zhaoolee/garss)![GitHub Repo stars](https://img.shields.io/github/stars/zhaoolee/garss?style=social))  |
| 10.9 | :hearts: A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics ([:x:](https://arxiv.org/abs/2310.05694)), ([:paperclip:](https://arxiv.org/pdf/2310.05694.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.05694)), ([:house:](https://huggingface.co/papers/2310.05694)), ([:eight_spoked_asterisk:]()), ([:octocat:](https://github.com/kaihe-better/llm-for-healthcare)![GitHub Repo stars](https://img.shields.io/github/stars/kaihe-better/llm-for-healthcare?style=social)) |
| 10.8 | :hearts: ChatRadio-Valuer: A Chat Large Language Model for Generalizable Radiology Report Generation Based on Multi-institution and Multi-system Data ([:x:](https://arxiv.org/abs/2310.05242)), ([:book:](https://browse.arxiv.org/pdf/2310.05242.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.05242.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.05242)), ([:house:](https://huggingface.co/papers/2310.05242)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chatradio-valuer-a-chat-large-language-model)) |
| 10.7 | Data-Centric Financial Large Language Models ([:x:](https://arxiv.org/abs/2310.17784)), ([:paperclip:](https://arxiv.org/pdf/2310.17784.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.17784)), ([:house:](https://huggingface.co/papers/2310.17784)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/data-centric-financial-large-language-models)) |
| 10.6 | Segmented Harmonic Loss: Handling Class-Imbalanced Multi-Label Clinical Data for Medical Coding with Large Language Models ([:x:](https://arxiv.org/abs/2310.04595)), ([:book:](https://browse.arxiv.org/pdf/2310.04595.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.04595.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.04595)), ([:house:](https://huggingface.co/papers/2310.04595)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/segmented-harmonic-loss-handling-class)) |
| 10.6 | Governments race to regulate AI tools (Reuters [news](https://www.reuters.com/technology/governments-race-regulate-ai-tools-2023-10-06/)) |
| 10.6 | MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning ([:x:](https://arxiv.org/abs/2310.03731)), ([:paperclip:](https://arxiv.org/pdf/2310.03731.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.03731)), ([:house:](https://huggingface.co/papers/2310.03731)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mathcoder-seamless-code-integration-in-llms)), ([:octocat:](https://github.com/mathllm/mathcoder)![GitHub Repo stars](https://img.shields.io/github/stars/mathllm/mathcoder?style=social))  |
| 10.6 | Improved Baselines with Visual Instruction Tuning ([:x:](https://arxiv.org/abs/2310.03744)), ([:paperclip:](https://arxiv.org/pdf/2310.03744.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.03744)), ([:house:](https://huggingface.co/papers/2310.03744)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/improved-baselines-with-visual-instruction)), ([:octocat:](https://github.com/haotian-liu/LLaVA)![GitHub Repo stars](https://img.shields.io/github/stars/haotian-liu/LLaVA?style=social))  |
| 10.6 | Aligning Text-to-Image Diffusion Models with Reward Backpropagation ([:x:](https://arxiv.org/abs/2310.03739)), ([:paperclip:](https://arxiv.org/pdf/2310.03739.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.03739)), ([:house:](https://huggingface.co/papers/2310.03739)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/aligning-text-to-image-diffusion-models-with)), ([:octocat:](https://github.com/mihirp1998/alignprop)![GitHub Repo stars](https://img.shields.io/github/stars/mihirp1998/alignprop?style=social))  |
| 10.6 | DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines ([:x:](https://arxiv.org/abs/2310.03714)), ([:paperclip:](https://arxiv.org/pdf/2310.03714.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.03714)), ([:house:](https://huggingface.co/papers/2310.03714)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dspy-compiling-declarative-language-model)), ([:octocat:](https://github.com/stanfordnlp/dspy)![GitHub Repo stars](https://img.shields.io/github/stars/stanfordnlp/dspy?style=social))  |
| 10.6 | Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency ([:x:](https://arxiv.org/abs/2310.03734)), ([:paperclip:](https://arxiv.org/pdf/2310.03734.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.03734)), ([:house:](https://huggingface.co/papers/2310.03734)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/leveraging-unpaired-data-for-vision-language)) |
| 10.6 | A Long Way to Go: Investigating Length Correlations in RLHF ([:x:](https://arxiv.org/abs/2310.03716)), ([:paperclip:](https://arxiv.org/pdf/2310.03716.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.03716)), ([:house:](https://huggingface.co/papers/2310.03716)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-long-way-to-go-investigating-length)) |
| 10.6 | Drag View: Generalizable Novel View Synthesis with Unposed Imagery ([:x:](https://arxiv.org/abs/2310.03704)), ([:paperclip:](https://arxiv.org/pdf/2310.03704.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.03704)), ([:house:](https://huggingface.co/papers/2310.03704)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/drag-view-generalizable-novel-view-synthesis)) |
| 10.6 | HeaP: Hierarchical Policies for Web Actions using LLMs ([:x:](https://arxiv.org/abs/2310.03720)), ([:paperclip:](https://arxiv.org/pdf/2310.03720.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.03720)), ([:house:](https://huggingface.co/papers/2310.03720)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/heap-hierarchical-policies-for-web-actions)) |
| 10.5 | Redefining Digital Health Interfaces with Large Language Models ([:x:](https://arxiv.org/abs/2310.03560)), ([:book:](https://browse.arxiv.org/pdf/2310.03560.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.03560.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.03560)), ([:house:](https://huggingface.co/papers/2310.03560)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/redefining-digital-health-interfaces-with)) |
| 10.5 | Benchmarking a foundation LLM on its ability to re-label structure names in accordance with the AAPM TG-263 report ([:x:](https://arxiv.org/abs/2310.03874)), ([:book:](https://browse.arxiv.org/pdf/2310.03874.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.03874.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.03874)), ([:house:](https://huggingface.co/papers/2310.03874)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/benchmarking-a-foundation-llm-on-its-ability)) |
| 10.5 | Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT (Nat Comput Sci 3, 833–838 (2023). [https://doi.org/10.1038/s43588-023-00527-x](https://www.nature.com/articles/s43588-023-00527-x)) |
| 10.5 | Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning ([:x:](https://arxiv.org/abs/2310.03094)), ([:paperclip:](https://arxiv.org/pdf/2310.03094.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.03094)), ([:house:](https://huggingface.co/papers/2310.03094)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-model-cascades-with-mixture-of)) |
| 10.5 | FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation ([:x:](https://arxiv.org/abs/2310.03214)), ([:paperclip:](https://arxiv.org/pdf/2310.03214.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.03214)), ([:house:](https://huggingface.co/papers/2310.03214)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/freshllms-refreshing-large-language-models)), ([:octocat:](https://github.com/freshllms/freshqa)![GitHub Repo stars](https://img.shields.io/github/stars/freshllms/freshqa?style=social))  |
| 10.5 | Kandinsky: an Improved Text-to-Image Synthesis with Image Prior and Latent Diffusion ([:x:](https://arxiv.org/abs/2310.03502)), ([:paperclip:](https://arxiv.org/pdf/2310.03502.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.03502)), ([:house:](https://huggingface.co/papers/2310.03502)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/kandinsky-an-improved-text-to-image-synthesis)), ([:octocat:](https://github.com/ai-forever/movqgan)![GitHub Repo stars](https://img.shields.io/github/stars/ai-forever/movqgan?style=social))  |
| 10.4 | Functional trustworthiness of AI systems by statistically valid testing ([:x:](https://arxiv.org/abs/2310.02727)), ([:book:](https://browse.arxiv.org/pdf/2310.02727.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.02727.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.02727)), ([:house:](https://huggingface.co/papers/2310.02727)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/functional-trustworthiness-of-ai-systems-by)) |
| 10.4 | EcoAssistant: Using LLM Assistant More Affordably and Accurately ([:x:](https://arxiv.org/abs/2310.03046)), ([:paperclip:](https://arxiv.org/pdf/2310.03046.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.03046)), ([:house:](https://huggingface.co/papers/2310.03046)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ecoassistant-using-llm-assistant-more)), ([:octocat:](https://github.com/jieyuz2/ecoassistant)![GitHub Repo stars](https://img.shields.io/github/stars/jieyuz2/ecoassistant?style=social))  |
| 10.4 | How FaR Are Large Language Models From Agents with Theory-of-Mind? ([:x:](https://arxiv.org/abs/2310.03051)), ([:paperclip:](https://arxiv.org/pdf/2310.03051.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.03051)), ([:house:](https://huggingface.co/papers/2310.03051)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/how-far-are-large-language-models-from-agents)) |
| 10.3 | Low-Resource Languages Jailbreak GPT-4 ([:x:](https://arxiv.org/abs/2310.02446)), ([:book:](https://browse.arxiv.org/pdf/2310.02446.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.02446.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.02446)), ([:house:](https://huggingface.co/papers/2310.02446)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/low-resource-languages-jailbreak-gpt-4)) |
| 10.3 | Can large language models provide useful feedback on research papers? A large-scale empirical analysis ([:x:](https://arxiv.org/abs/2310.01783)), ([:book:](https://browse.arxiv.org/pdf/2310.01783.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.01783.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.01783)), ([:house:](https://huggingface.co/papers/2310.01783)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/can-large-language-models-provide-useful)), ([:octocat:](https://github.com/weixin-liang/llm-scientific-feedback)![GitHub Repo stars](https://img.shields.io/github/stars/weixin-liang/llm-scientific-feedback?style=social))  |
| 10.3 | :hearts: Conversational Health Agents: A Personalized LLM-Powered Agent Framework ([:x:](https://arxiv.org/abs/2310.02374)), ([:paperclip:](https://arxiv.org/pdf/2310.02374.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.02374)), ([:house:](https://huggingface.co/papers/2310.02374)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/conversational-health-agents-a-personalized)) |
| 10.3 | Large Language Models Cannot Self-Correct Reasoning Yet ([:x:](https://arxiv.org/abs/2310.01798)), ([:paperclip:](https://arxiv.org/pdf/2310.01798.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.01798)), ([:house:](https://huggingface.co/papers/2310.01798)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-cannot-self-correct)) |
| 10.3 | ImagenHub: Standardizing the evaluation of conditional image generation models ([:x:](https://arxiv.org/abs/2310.01596)), ([:paperclip:](https://arxiv.org/pdf/2310.01596.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.01596)), ([:house:](https://huggingface.co/papers/2310.01596)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/imagenhub-standardizing-the-evaluation-of)) |
| 10.3 | Large Language Models as Analogical Reasoners ([:x:](https://arxiv.org/abs/2310.01714)), ([:paperclip:](https://arxiv.org/pdf/2310.01714.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.01714)), ([:house:](https://huggingface.co/papers/2310.01714)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-as-analogical-reasoners)) |
| 10.3 | SmartPlay : A Benchmark for LLMs as Intelligent Agents ([:x:](https://arxiv.org/abs/2310.01557)), ([:paperclip:](https://arxiv.org/pdf/2310.01557.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.01557)), ([:house:](https://huggingface.co/papers/2310.01557)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/smartplay-a-benchmark-for-llms-as-intelligent)) |
| 10.3 | Conditional Diffusion Distillation ([:x:](https://arxiv.org/abs/2310.01407)), ([:paperclip:](https://arxiv.org/pdf/2310.01407.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.01407)), ([:house:](https://huggingface.co/papers/2310.15504)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/conditional-diffusion-distillation)) |
| 10.2 | Evaluating the Application of Large Language Models in Clinical Research Contexts (JAMA [doi:10.1001/jamanetworkopen.2023.35924](https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2809977)) |
| 10.2 | Who is ChatGPT? Benchmarking LLMs' Psychological Portrayal Using PsychoBench ([:x:](https://arxiv.org/abs/2310.01386)), ([:book:](https://browse.arxiv.org/pdf/2310.01386.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.01386.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.01386)), ([:house:](https://huggingface.co/papers/2310.01386)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/who-is-chatgpt-benchmarking-llms)), ([:octocat:](https://github.com/cuhk-arise/psychobench)![GitHub Repo stars](https://img.shields.io/github/stars/cuhk-arise/psychobench?style=social))  |
| 10.2 | Investigating the Efficacy of Large Language Models in Reflective Assessment Methods through Chain of Thoughts Prompting ([:x:](https://arxiv.org/abs/2310.00272)), ([:book:](https://browse.arxiv.org/pdf/2310.00272.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.00272.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.00272)), ([:house:](https://huggingface.co/papers/2310.00272)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/investigating-the-efficacy-of-large-language)) |
| 10.2 | Mirror Diffusion Models for Constrained and Watermarked Generation ([:x:](https://arxiv.org/abs/2310.01236)), ([:paperclip:](https://arxiv.org/pdf/2310.01236.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.01236)), ([:house:](https://huggingface.co/papers/2310.01236)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mirror-diffusion-models-for-constrained-and)) |
| 10.2 | UniAudio: An Audio Foundation Model Toward Universal Audio Generation ([:x:](https://arxiv.org/abs/2310.00704)), ([:paperclip:](https://arxiv.org/pdf/2310.00704.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.00704)), ([:house:](https://huggingface.co/papers/2310.00704)), ([:eight_spoked_asterisk:](https://cs.paperswithcode.com/paper/uniaudio-an-audio-foundation-model-toward-1)), ([:octocat:](https://github.com/yangdongchao/uniaudio)![GitHub Repo stars](https://img.shields.io/github/stars/yangdongchao/uniaudio?style=social))  |
| 10.2 | Enable Language Models to Implicitly Learn Self-Improvement From Data ([:x:](https://arxiv.org/abs/2310.00898)), ([:paperclip:](https://arxiv.org/pdf/2310.00898.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.00898)), ([:house:](https://huggingface.co/papers/2310.00898)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/enable-language-models-to-implicitly-learn)) |
| 10.1 | PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis ([:x:](https://arxiv.org/abs/2310.00426)), ([:paperclip:](https://arxiv.org/pdf/2310.00426.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.00426)), ([:house:](https://huggingface.co/papers/2310.00426)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pixart-a-fast-training-of-diffusion)), ([:octocat:](https://github.com/PixArt-alpha/PixArt-alpha)![GitHub Repo stars](https://img.shields.io/github/stars/PixArt-alpha/PixArt-alpha?style=social))  |
| 9.30 | Coordinated pausing: An evaluation-based coordination scheme for frontier AI developers ([:x:](https://arxiv.org/abs/2310.00374)), ([:book:](https://browse.arxiv.org/pdf/2310.00374.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.00374.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.00374)), ([:house:](https://huggingface.co/papers/2310.00374)), ([:eight_spoked_asterisk:]()), ([SS](https://www.semanticscholar.org/paper/Coordinated-pausing%3A-An-evaluation-based-scheme-for-Alaga-Schuett/4e3254d90560d19b42419e9b4c9367cf3674dcad)) |
| 9.30 | Deployment Corrections: An incident response framework for frontier AI models ([:x:](https://arxiv.org/abs/2310.00328)), ([:book:](https://browse.arxiv.org/pdf/2310.00328.pdf)), ([:paperclip:](https://arxiv.org/pdf/2310.00328.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2310.00328)), ([:house:](https://huggingface.co/papers/2310.00328)), ([:eight_spoked_asterisk:]()), ([SS](https://www.semanticscholar.org/paper/Deployment-Corrections%3A-An-incident-response-for-AI-O'Brien-Ee/69773e5a978b94ad50fb4bb5d977e7b4c7c8d8f2)) |
| 9.30 | Open-Sourcing Highly Capable Foundation Models: An evaluation of risks, benefits, and alternative methods for pursuing open-source objectives ([:x:](https://arxiv.org/abs/2309.09227)), ([:paperclip:](https://arxiv.org/pdf/2309.09227.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.09227)), ([:house:](https://huggingface.co/papers/2309.09227)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/open-sourcing-highly-capable-foundation)) |
| 9.29 | An evaluation of GPT models for phenotype concept recognition ([:x:](https://arxiv.org/abs/2309.17169)), ([:book:](https://browse.arxiv.org/pdf/2309.17169.pdf)), ([:paperclip:](https://arxiv.org/pdf/2309.17169.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.17169)), ([:house:](https://huggingface.co/papers/2309.17169)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/an-evaluation-of-gpt-models-for-phenotype)) |
| 9.29 | Vision Transformers Need Registers ([:x:](https://arxiv.org/abs/2309.16588)), ([:paperclip:](https://arxiv.org/pdf/2309.16588.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.16588)), ([:house:](https://huggingface.co/papers/2309.16588)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vision-transformers-need-registers)), ([:octocat:](https://github.com/facebookresearch/dinov2)![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/dinov2?style=social))  |
| 9.29 | The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision) ([:x:](https://arxiv.org/abs/2309.17421)), ([:book:](https://browse.arxiv.org/pdf/2310.17421.pdf)), ([:paperclip:](https://arxiv.org/pdf/2309.17421.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.17421)), ([:house:](https://huggingface.co/papers/2309.17421)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-dawn-of-lmms-preliminary-explorations)) |
| 9.29 | DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation ([:x:](https://arxiv.org/abs/2309.16653)), ([:paperclip:](https://arxiv.org/pdf/2309.16653.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.16653)), ([:house:](https://huggingface.co/papers/2309.16653)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dreamgaussian-generative-gaussian-splatting)), ([:octocat:](https://github.com/dreamgaussian/dreamgaussian)![GitHub Repo stars](https://img.shields.io/github/stars/dreamgaussian/dreamgaussian?style=social))  |
| 9.29 | Text-to-3D using Gaussian Splatting ([:x:](https://arxiv.org/abs/2309.16585)), ([:paperclip:](https://arxiv.org/pdf/2309.16585.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.16585)), ([:house:](https://huggingface.co/papers/2309.16585)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/text-to-3d-using-gaussian-splatting)), ([:octocat:](https://github.com/gsgen3d/gsgen)![GitHub Repo stars](https://img.shields.io/github/stars/gsgen3d/gsgen?style=social))  |
| 9.29 | Qwen Technical Report ([:x:](https://arxiv.org/abs/2309.16609)), ([:paperclip:](https://arxiv.org/pdf/2309.16609.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.16609)), ([:house:](https://huggingface.co/papers/2309.16609)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/qwen-technical-report)), ([:octocat:](https://github.com/qwenlm/qwen )![GitHub Repo stars](https://img.shields.io/github/stars/qwenlm/qwen ?style=social))  |
| 9.29 | Deep Geometrized Cartoon Line Inbetweening ([:x:](https://arxiv.org/abs/2309.16643)), ([:paperclip:](https://arxiv.org/pdf/2309.16643.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.16643)), ([:house:](https://huggingface.co/papers/2309.16643)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/deep-geometrized-cartoon-line-inbetweening-1)), ([:octocat:](https://github.com/lisiyao21/animeinbet)![GitHub Repo stars](https://img.shields.io/github/stars/lisiyao21/animeinbet?style=social))  |
| 9.29 | Demystifying CLIP Data ([:x:](https://arxiv.org/abs/2309.16671)), ([:paperclip:](https://arxiv.org/pdf/2309.16671.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.16671)), ([:house:](https://huggingface.co/papers/2309.16671)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/demystifying-clip-data)), ([:octocat:](https://github.com/facebookresearch/metaclip)![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/metaclip?style=social))  |
| 9.29 | MotionLM: Multi-Agent Motion Forecasting as Language Modeling ([:x:](https://arxiv.org/abs/2309.16534)), ([:paperclip:](https://arxiv.org/pdf/2309.16534.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.16534)), ([:house:](https://huggingface.co/papers/2309.16534)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/motionlm-multi-agent-motion-forecasting-as-1)) |
| 9.29 | GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond ([:x:](https://arxiv.org/abs/2309.16583)), ([:paperclip:](https://arxiv.org/pdf/2309.16583.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.16583)), ([:house:](https://huggingface.co/papers/2309.16583)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gpt-fathom-benchmarking-large-language-models)), ([:octocat:](https://github.com/gpt-fathom/gpt-fathom)![GitHub Repo stars](https://img.shields.io/github/stars/gpt-fathom/gpt-fathom?style=social)) |
| 9.29 | RealFill: Reference-Driven Generation for Authentic Image Completion ([:x:](https://arxiv.org/abs/2309.16668)), ([:paperclip:](https://arxiv.org/pdf/2309.16668.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.16668)), ([:house:](https://huggingface.co/papers/2309.16668)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/realfill-reference-driven-generation-for)) |
| 9.29 | CCEdit: Creative and Controllable Video Editing via Diffusion Models ([:x:](https://arxiv.org/abs/2309.16496)), ([:paperclip:](https://arxiv.org/pdf/2309.16496.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.16496)), ([:house:](https://huggingface.co/papers/2309.16496)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ccedit-creative-and-controllable-video)) |
| 9.29 | ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning ([:x:](https://arxiv.org/abs/2309.16650)), ([:paperclip:](https://arxiv.org/pdf/2309.16650.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.16650)), ([:house:](https://huggingface.co/papers/2309.16650)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/conceptgraphs-open-vocabulary-3d-scene-graphs)) |
| 9.28 | Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack ([:x:](https://arxiv.org/abs/2309.15807)), ([:paperclip:](https://arxiv.org/pdf/2309.15807.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.15807)), ([:house:](https://huggingface.co/papers/2309.15807)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/emu-enhancing-image-generation-models-using)) |
| 9.28 | Language models in molecular discovery ([:x:](https://arxiv.org/abs/2309.16235)), ([:paperclip:](https://arxiv.org/pdf/2309.16235.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.16235)), ([:house:](https://huggingface.co/papers/2309.16235)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/language-models-in-molecular-discovery)) |
| 9.28 | Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation ([:x:](https://arxiv.org/abs/2309.16429)), ([:paperclip:](https://arxiv.org/pdf/2309.16429.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.16429)), ([:house:](https://huggingface.co/papers/2309.16429)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/diverse-and-aligned-audio-to-video-generation)), ([:octocat:](https://github.com/guyyariv/TempoTokens)![GitHub Repo stars](https://img.shields.io/github/stars/guyyariv/TempoTokens?style=social))  |
| 9.28 | AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models ([:x:](https://arxiv.org/abs/2309.16414)), ([:paperclip:](https://arxiv.org/pdf/2309.16414.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.16414)), ([:house:](https://huggingface.co/papers/2309.16414)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/autoclip-auto-tuning-zero-shot-classifiers)) |
| 9.28 | AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model ([:x:](https://arxiv.org/abs/2309.16058)), ([:paperclip:](https://arxiv.org/pdf/2309.16058.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.16058)), ([:house:](https://huggingface.co/papers/2309.16058)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/anymal-an-efficient-and-scalable-any-modality)) |
| 9.28 | Effective Long-Context Scaling of Foundation Models ([:x:](https://arxiv.org/abs/2309.16039)), ([:paperclip:](https://arxiv.org/pdf/2309.16039.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.16039)), ([:house:](https://huggingface.co/papers/2309.16039)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/effective-long-context-scaling-of-foundation)), ([:octocat:](https://github.com/openlmlab/leval)![GitHub Repo stars](https://img.shields.io/github/stars/openlmlab/leval?style=social)) |
| 9.28 | Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation ([:x:](https://arxiv.org/abs/2309.15818)), ([:paperclip:](https://arxiv.org/pdf/2309.15818.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.15818)), ([:house:](https://huggingface.co/papers/2309.15818)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/show-1-marrying-pixel-and-latent-diffusion)), ([:octocat:](https://github.com/showlab/show-1)![GitHub Repo stars](https://img.shields.io/github/stars/showlab/show-1?style=social))  |
| 9.27 | A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future ([:x:](https://arxiv.org/abs/2309.15402)), ([:book:](https://browse.arxiv.org/pdf/2309.15402.pdf)), ([:paperclip:](https://arxiv.org/pdf/2309.15402.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.15402)), ([:house:](https://huggingface.co/papers/2309.15402)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-survey-of-chain-of-thought-reasoning)), ([:octocat:](https://github.com/zchuz/cot-reasoning-survey)![GitHub Repo stars](https://img.shields.io/github/stars/zchuz/cot-reasoning-survey?style=social))  |
| 9.27 | NeuRBF: A Neural Fields Representation with Adaptive Radial Basis Functions ([:x:](https://arxiv.org/abs/2309.15426)), ([:paperclip:](https://arxiv.org/pdf/2309.15426.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.15426)), ([:house:](https://huggingface.co/papers/2309.15426)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/neurbf-a-neural-fields-representation-with-1)), ([:octocat:](https://github.com/oppo-us-research/NeuRBF)![GitHub Repo stars](https://img.shields.io/github/stars/oppo-us-research/NeuRBF?style=social)) |
| 9.27 | Low-rank Adaptation of Large Language Model Rescoring for Parameter-Efficient Speech Recognition ([:x:](https://arxiv.org/abs/2309.15223)), ([:paperclip:](https://arxiv.org/pdf/2309.15223.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.15223)), ([:house:](https://huggingface.co/papers/2309.15223)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/low-rank-adaptation-of-large-language-model)) |
| 9.27 | Jointly Training Large Autoregressive Multimodal Models ([:x:](https://arxiv.org/abs/2309.15564)), ([:paperclip:](https://arxiv.org/pdf/2309.15564.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.15564)), ([:house:](https://huggingface.co/papers/2309.15564)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/jointly-training-large-autoregressive)) |
| 9.27 | DECO: Dense Estimation of 3D Human-Scene Contact In The Wild ([:x:](https://arxiv.org/abs/2309.15273)), ([:paperclip:](https://arxiv.org/pdf/2309.15273.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.15273)), ([:house:](https://huggingface.co/papers/2309.15273)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/deco-dense-estimation-of-3d-human-scene-1)) |
| 9.27 | Finite Scalar Quantization: VQ-VAE Made Simple ([:x:](https://arxiv.org/abs/2309.15505)), ([:paperclip:](https://arxiv.org/pdf/2309.15505.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.15505)), ([:house:](https://huggingface.co/papers/2309.15505)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/finite-scalar-quantization-vq-vae-made-simple)), ([:octocat:](https://github.com/google-research/google-research)![GitHub Repo stars](https://img.shields.io/github/stars/google-research/google-research?style=social))  |
| 9.27 | VPA: Fully Test-Time Visual Prompt Adaptation ([:x:](https://arxiv.org/abs/2309.15251)), ([:paperclip:](https://arxiv.org/pdf/2309.15251.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.15251)), ([:house:](https://huggingface.co/papers/2309.15504)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vpa-fully-test-time-visual-prompt-adaptation)) |
| 9.27 | LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models ([:x:](https://arxiv.org/abs/2309.15103)), ([:paperclip:](https://arxiv.org/pdf/2309.15103.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.15103)), ([:house:](https://huggingface.co/papers/2309.15103)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lavie-high-quality-video-generation-with)) |
| 9.27 | VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning ([:x:](https://arxiv.org/abs/2309.15091)), ([:paperclip:](https://arxiv.org/pdf/2309.15091.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.15091)), ([:house:](https://huggingface.co/papers/2309.15091)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/videodirectorgpt-consistent-multi-scene-video)) |
| 9.27 | Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models ([:x:](https://arxiv.org/abs/2309.15098)), ([:paperclip:](https://arxiv.org/pdf/2309.15098.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.15098)), ([:house:](https://huggingface.co/papers/2309.15098)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/attention-satisfies-a-constraint-satisfaction)) |
| 9.26 | :hearts: Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI ([:x:](https://arxiv.org/abs/2309.01463)), ([:book:](https://browse.arxiv.org/pdf/2309.01463.pdf)), ([:paperclip:](https://arxiv.org/pdf/2309.01463.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.01463)), ([SS](https://www.semanticscholar.org/paper/Creating-Trustworthy-LLMs%3A-Dealing-with-in-AI-Ahmad-Yaramis/be2b0125f3161739c685e9f86d9fd49f9f6d99c8)), ([:house:](https://huggingface.co/papers/2309.01463)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/creating-trustworthy-llms-dealing-with)) |
| 9.26 | QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models ([:x:](https://arxiv.org/abs/2309.14717)), ([:paperclip:](https://arxiv.org/pdf/2309.14717.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.14717)), ([:house:](https://huggingface.co/papers/2309.14717)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/qa-lora-quantization-aware-low-rank)) |
| 9.26 | Aligning Large Multimodal Models with Factually Augmented RLHF ([:x:](https://arxiv.org/abs/2309.14525)), ([:paperclip:](https://arxiv.org/pdf/2309.14525.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.14525)), ([:house:](https://huggingface.co/papers/2309.1514525504)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/aligning-large-multimodal-models-with)) |
| 9.26 | DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models ([:x:](https://arxiv.org/abs/2309.14509)), ([:paperclip:](https://arxiv.org/pdf/2309.14509.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.14509)), ([:house:](https://huggingface.co/papers/2309.14509)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/deepspeed-ulysses-system-optimizations-for)) |
| 9.26 | Efficient Post-training Quantization with FP8 Formats ([:x:](https://arxiv.org/abs/2309.14592)), ([:paperclip:](https://arxiv.org/pdf/2309.14592.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.14592)), ([:house:](https://huggingface.co/papers/2309.14592)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/efficient-post-training-quantization-with-fp8)), ([:octocat:](https://github.com/intel/neural-compressor)![GitHub Repo stars](https://img.shields.io/github/stars/intel/neural-compressor?style=social))  |
| 9.26 | DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention ([:x:](https://arxiv.org/abs/2309.14327)), ([:paperclip:](https://arxiv.org/pdf/2309.14327.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.14327)), ([:house:](https://huggingface.co/papers/2309.14327)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/deepspeed-visualchat-multi-round-multi-image)), ([:octocat:](https://github.com/microsoft/deepspeedexamples)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/deepspeedexamples?style=social))  |
| 9.26 | Small-scale proxies for large-scale Transformer training instabilities ([:x:](https://arxiv.org/abs/2309.14322)), ([:paperclip:](https://arxiv.org/pdf/2309.14322.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.14322)), ([:house:](https://huggingface.co/papers/2309.14322)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/small-scale-proxies-for-large-scale)) |
| 9.25 | VidChapters-7M: Video Chapters at Scale ([:x:](https://arxiv.org/abs/2309.13952)), ([:paperclip:](https://arxiv.org/pdf/2309.13952.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.13952)), ([:house:](https://huggingface.co/papers/2309.13952)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vidchapters-7m-video-chapters-at-scale)), ([:octocat:](https://github.com/antoyang/VidChapters)![GitHub Repo stars](https://img.shields.io/github/stars/antoyang/VidChapters?style=social))  |
| 9.25 | Evaluating Cognitive Maps and Planning in Large Language Models with CogEval ([:x:](https://arxiv.org/abs/2309.15129)), ([:paperclip:](https://arxiv.org/pdf/2309.15129.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.15129)), ([:house:](https://huggingface.co/papers/2309.15129)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evaluating-cognitive-maps-and-planning-in)) |
| 9.23 | MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation ([:x:](https://arxiv.org/abs/2309.13042)), ([:paperclip:](https://arxiv.org/pdf/2309.13042.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.13042)), ([:house:](https://huggingface.co/papers/2309.13042)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mosaicfusion-diffusion-models-as-data)), ([:octocat:](https://github.com/jiahao000/mosaicfusion)![GitHub Repo stars](https://img.shields.io/github/stars/jiahao000/mosaicfusion?style=social))  |
| 9.23 | Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model ([:x:](https://arxiv.org/abs/2309.13018)), ([:paperclip:](https://arxiv.org/pdf/2309.13018.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.13018)), ([:house:](https://huggingface.co/papers/2309.13018)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dynamic-asr-pathways-an-adaptive-masking)) |
| 9.23 | Robotic Offline RL from Internet Videos via Value-Function Pre-Training ([:x:](https://arxiv.org/abs/2309.13041)), ([:paperclip:](https://arxiv.org/pdf/2309.13041.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.13041)), ([:house:](https://huggingface.co/papers/2309.13041)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/robotic-offline-rl-from-internet-videos-via)) |
| 9.23 | Exploring Large Language Models' Cognitive Moral Development through Defining Issues Test ([:x:](https://arxiv.org/abs/2309.13356)), ([:paperclip:](https://arxiv.org/pdf/2309.13356.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.13356)), ([:house:](https://huggingface.co/papers/2309.13356)), ([:eight_spoked_asterisk:]()) |
| 9.23 | Calibrating LLM-Based Evaluator ([:x:](https://arxiv.org/abs/2309.13308)), ([:paperclip:](https://arxiv.org/pdf/2309.13308.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.13308)), ([:house:](https://huggingface.co/papers/2309.13308)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/calibrating-llm-based-evaluator)) |
| 9.22 | Affect Recognition in Conversations Using Large Language Models ([:x:](https://arxiv.org/abs/2309.12881)), ([:book:](https://browse.arxiv.org/pdf/2309.12881.pdf)), ([:paperclip:](https://arxiv.org/pdf/2309.12881.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.12881)), ([:house:](https://huggingface.co/papers/2309.12881)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/affect-recognition-in-conversations-using)) |
| 9.22 | DRG-LLaMA : Tuning LLaMA Model to Predict Diagnosis-related Group for Hospitalized Patients ([:x:](https://arxiv.org/abs/2309.12625)), ([:book:](https://browse.arxiv.org/pdf/2309.12625.pdf)), ([:paperclip:](https://arxiv.org/pdf/2309.12625.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.12625)), ([:house:](https://huggingface.co/papers/2309.12625)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/drg-llama-tuning-llama-model-to-predict)), ([:octocat:](https://github.com/hanyin88/drg-llama)![GitHub Repo stars](https://img.shields.io/github/stars/hanyin88/drg-llama?style=social))  |
| 9.22 | CodePlan: Repository-level Coding using LLMs and Planning ([:x:](https://arxiv.org/abs/2309.12499)), ([:paperclip:](https://arxiv.org/pdf/2309.12499.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.12499)), ([:house:](https://huggingface.co/papers/2309.12499)), ([:eight_spoked_asterisk:]()) |
| 9.22 | DualToken-ViT: Position-aware Efficient Vision Transformer with Dual Token Fusion ([:x:](https://arxiv.org/abs/2309.12424)), ([:paperclip:](https://arxiv.org/pdf/2309.12424.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.12424)), ([:house:](https://huggingface.co/papers/2309.12424)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dualtoken-vit-position-aware-efficient-vision)) |
| 9.22 | LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models ([:x:](https://arxiv.org/abs/2309.12307)), ([:paperclip:](https://arxiv.org/pdf/2309.12307.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.12307)), ([:house:](https://huggingface.co/papers/2309.12307)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/longlora-efficient-fine-tuning-of-long)), ([:octocat:](https://github.com/dvlab-research/longlora)![GitHub Repo stars](https://img.shields.io/github/stars/dvlab-research/longlora?style=social)) |
| 9.22 | LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent ([:x:](https://arxiv.org/abs/2309.12311)), ([:paperclip:](https://arxiv.org/pdf/2309.12311.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.12311)), ([:house:](https://huggingface.co/papers/2309.12311)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llm-grounder-open-vocabulary-3d-visual)) |
| 9.22 | MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models ([:x:](https://arxiv.org/abs/2309.12284)), ([:paperclip:](https://arxiv.org/pdf/2309.12284.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.12284)), ([:house:](https://huggingface.co/papers/2309.12284)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/metamath-bootstrap-your-own-mathematical)) |
| 9.22 | Game of Thrones author sues ChatGPT owner OpenAI (BBC [news](https://www.bbc.com/news/technology-66866577)) |
| 9.22 | Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition (NeurIPS 2023 [abstract](https://openreview.net/forum?id=kdFR6IUEW6)), ([PDF](https://openreview.net/pdf?id=kdFR6IUEW6)) |
| 9.21 | Foundation Metrics: Quantifying Effectiveness of Healthcare Conversations powered by Generative AI ([:x:](https://arxiv.org/abs/2309.12444)), ([:book:](https://browse.arxiv.org/pdf/2309.12444.pdf)), ([:paperclip:](https://arxiv.org/pdf/2309.12444.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.12444)), ([:house:](https://huggingface.co/papers/2309.12444)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/foundation-metrics-quantifying-effectiveness)) |
| 9.21 | How Robust is Google's Bard to Adversarial Image Attacks? ([:x:](https://arxiv.org/abs/2309.11751)), ([:paperclip:](https://arxiv.org/pdf/2309.11751.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.11751)), ([:house:](https://huggingface.co/papers/2309.11751)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/how-robust-is-google-s-bard-to-adversarial)), ([:octocat:](https://github.com/thu-ml/attack-bard)![GitHub Repo stars](https://img.shields.io/github/stars/thu-ml/attack-bard?style=social))  |
| 9.21 | SCREWS: A Modular Framework for Reasoning with Revisions ([:x:](https://arxiv.org/abs/2309.13075)), ([:paperclip:](https://arxiv.org/pdf/2309.13075.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.13075)), ([:house:](https://huggingface.co/papers/2309.13075)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/screws-a-modular-framework-for-reasoning-with)), ([:octocat:](https://github.com/kumar-shridhar/screws)![GitHub Repo stars](https://img.shields.io/github/stars/kumar-shridhar/screws?style=social))  |
| 9.21 | OpenAI release preview of Dall-E 3 ([tweet](https://twitter.com/OpenAI/status/1704545442749628695)), ([DALL·E 3](https://openai.com/dall-e-3)) |
| 9.21 | LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset ([:x:](https://arxiv.org/abs/2309.11998)), ([:paperclip:](https://arxiv.org/pdf/2309.11998.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.11998)), ([:house:](https://huggingface.co/papers/2309.11998)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lmsys-chat-1m-a-large-scale-real-world-llm)) |
| 9.21 | BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model ([:x:](https://arxiv.org/abs/2309.11568)), ([:paperclip:](https://arxiv.org/pdf/2309.11568.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.11568)), ([:house:](https://huggingface.co/papers/2309.11568)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/btlm-3b-8k-7b-parameter-performance-in-a-3b)) | 
| 9.21 | A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models ([:x:](https://arxiv.org/abs/2309.11674)), ([:paperclip:](https://arxiv.org/pdf/2309.11674.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.11674)), ([:house:](https://huggingface.co/papers/2309.11674)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-paradigm-shift-in-machine-translation)), ([:octocat:](https://github.com/fe1ixxu/alma)![GitHub Repo stars](https://img.shields.io/github/stars/fe1ixxu/alma?style=social))  |
| 9.21 | DreamLLM: Synergistic Multimodal Comprehension and Creation ([:x:](https://arxiv.org/abs/2309.11499)), ([:paperclip:](https://arxiv.org/pdf/2309.11499.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.11499)), ([:house:](https://huggingface.co/papers/2309.11499)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dreamllm-synergistic-multimodal-comprehension)), ([:octocat:](https://github.com/RunpeiDong/DreamLLM)![GitHub Repo stars](https://img.shields.io/github/stars/RunpeiDong/DreamLLM?style=social)) |
| 9.21 | FreeU: Free Lunch in Diffusion U-Net ([:x:](https://arxiv.org/abs/2309.11497)), ([:paperclip:](https://arxiv.org/pdf/2309.11497.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.11497)), ([:house:](https://huggingface.co/papers/2309.11497)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/freeu-free-lunch-in-diffusion-u-net)) |
| 9.21 | Kosmos-2.5: A Multimodal Literate Model ([:x:](https://arxiv.org/abs/2309.11419)), ([:paperclip:](https://arxiv.org/pdf/2309.11419.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.11419)), ([:house:](https://huggingface.co/papers/2309.11419)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/kosmos-2-5-a-multimodal-literate-model)) |
| 9.21 | Chain-of-Verification Reduces Hallucination in Large Language Models ([:x:](https://arxiv.org/abs/2309.11495)), ([:paperclip:](https://arxiv.org/pdf/2309.11495.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.11495)), ([:house:](https://huggingface.co/papers/2309.11495)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chain-of-verification-reduces-hallucination)) |
| 9.20 | OpenChat: Advancing Open-source Language Models with Mixed-Quality Data ([:x:](https://arxiv.org/abs/2309.11235)), ([:book:](https://browse.arxiv.org/pdf/2309.11235.pdf)),, ([:paperclip:](https://arxiv.org/pdf/2309.11235.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.11235)), ([:house:](https://huggingface.co/papers/2309.11235)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/openchat-advancing-open-source-language)), ([:octocat:](https://github.com/imoneoi/openchat)![GitHub Repo stars](https://img.shields.io/github/stars/imoneoi/openchat?style=social))  |
| 9.20 | A Large-scale Dataset for Audio-Language Representation Learning ([:x:](https://arxiv.org/abs/2309.11500)), ([:paperclip:](https://arxiv.org/pdf/2309.11500.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.11500)), ([:house:](https://huggingface.co/papers/2309.11500)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-large-scale-dataset-for-audio-language)) | 
| 9.20 | LMDX: Language Model-based Document Information Extraction and Localization ([:x:](https://arxiv.org/abs/2309.10952)), ([:paperclip:](https://arxiv.org/pdf/2309.10952.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.10952)), ([:house:](https://huggingface.co/papers/2309.10952)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lmdx-language-model-based-document)) |
| 9.20 | The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute ([:x:](https://arxiv.org/abs/2309.11197)), ([:paperclip:](https://arxiv.org/pdf/2309.11197.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.11197)), ([:house:](https://huggingface.co/papers/2309.11197)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-languini-kitchen-enabling-language)) |
| 9.20 | OpenAI’s Dall-E 3 Is an Art Generator Powered by ChatGPT (Wired [news](https://www.wired.com/story/dall-e-3-open-ai-chat-gpt/)) |
| 9.20 | OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch ([:x:](https://arxiv.org/abs/2309.10706)), ([:paperclip:](https://arxiv.org/pdf/2309.10706.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.10706)), ([:house:](https://huggingface.co/papers/2309.10706)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/openba-an-open-sourced-15b-bilingual)), ([:octocat:](https://github.com/opennlg/openba)![GitHub Repo stars](https://img.shields.io/github/stars/opennlg/openba?style=social)) |
| 9.19 | MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback ([:x:](https://arxiv.org/abs/2311.10691)), ([:book:](https://browse.arxiv.org/pdf/2311.10691.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.10691.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.10691)), ([:house:](https://huggingface.co/papers/2311.10691)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mint-evaluating-llms-in-multi-turn)), ([SS](https://www.semanticscholar.org/paper/MINT%3A-Evaluating-LLMs-in-Multi-turn-Interaction-and-Wang-Wang/12b233752c7097ea6525622bed238ae2d2193c5a)) |
| 9.19 | Enhancing Health Data Interoperability with Large Language Models: A FHIR Study ([:x:](https://arxiv.org/abs/2309.12989)), ([:book:](https://browse.arxiv.org/pdf/2309.12989.pdf)), ([:paperclip:](https://arxiv.org/pdf/2309.12989.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.12989)), ([:house:](https://huggingface.co/papers/2309.12989)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/enhancing-health-data-interoperability-with)) |
| 9.19 | OpenCog Hyperon: A Framework for AGI at the Human Level and Beyond ([:x:](https://arxiv.org/abs/2309.18318)), ([:paperclip:](https://arxiv.org/pdf/2309.18318.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.18318)), ([:house:](https://huggingface.co/papers/2309.18318)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/opencog-hyperon-a-framework-for-agi-at-the)) |
| 9.19 | SlimPajama-DC: Understanding Data Combinations for LLM Training ([:x:](https://arxiv.org/abs/2309.10818)), ([:paperclip:](https://arxiv.org/pdf/2309.10818.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.10818)), ([:house:](https://huggingface.co/papers/2309.10818)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/slimpajama-dc-understanding-data-combinations)) |
| 9.19 | Baichuan 2: Open Large-scale Language Models ([:x:](https://arxiv.org/abs/2309.10305)), ([:paperclip:](https://arxiv.org/pdf/2309.10305.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.10305)), ([:house:](https://huggingface.co/papers/2309.10305)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/baichuan-2-open-large-scale-language-models)), ([:octocat:](https://github.com/baichuan-inc/baichuan2)![GitHub Repo stars](https://img.shields.io/github/stars/baichuan-inc/baichuan2?style=social)) |
| 9.19 | Stabilizing RLHF through Advantage Model and Selective Rehearsal ([:x:](https://arxiv.org/abs/2309.10202)), ([:paperclip:](https://arxiv.org/pdf/2309.10202.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.10202)), ([:house:](https://huggingface.co/papers/2309.10202)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/stabilizing-rlhf-through-advantage-model-and)) |
| 9.19 | 360^circ Reconstruction From a Single Image Using Space Carved Outpainting ([:x:](https://arxiv.org/abs/2309.10279)), ([:paperclip:](https://arxiv.org/pdf/2309.10279.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.10279)), ([:house:](https://huggingface.co/papers/2309.10279)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/360-circ-reconstruction-from-a-single-image)) |
| 9.19 | Language Modeling Is Compression ([:x:](https://arxiv.org/abs/2309.10668)), ([:paperclip:](https://arxiv.org/pdf/2309.10668.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.10668)), ([:house:](https://huggingface.co/papers/2309.10668)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/language-modeling-is-compression)) |
| 9.19 | Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions ([:x:](https://arxiv.org/abs/2309.10150)), ([:paperclip:](https://arxiv.org/pdf/2309.10150.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.10150)), ([:house:](https://huggingface.co/papers/2309.10150)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/q-transformer-scalable-offline-reinforcement)) |
| 9.18. | Data Formulator: AI-powered Concept-driven Visualization Authoring ([:x:](https://arxiv.org/abs/2309.10094)), ([:paperclip:](https://arxiv.org/pdf/2309.10094.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.10094)), ([:house:](https://huggingface.co/papers/2309.10094)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/data-formulator-ai-powered-concept-driven)) |
| 9.18 | MindAgent: Emergent Gaming Interaction ([:x:](https://arxiv.org/abs/2309.09971)), ([:paperclip:](https://arxiv.org/pdf/2309.09971.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.09971)), ([:house:](https://huggingface.co/papers/2309.09971)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mindagent-emergent-gaming-interaction)) |
| 9.18 | An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models ([:x:](https://arxiv.org/abs/2309.09958)), ([:paperclip:](https://arxiv.org/pdf/2309.09958.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.09958)), ([:house:](https://huggingface.co/papers/2309.09958)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/an-empirical-study-of-scaling-instruct-tuned)), ([:octocat:](https://github.com/haotian-liu/LLaVA)![GitHub Repo stars](https://img.shields.io/github/stars/haotian-liu/LLaVA?style=social)) |
| 9.18 | LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models ([:x:](https://arxiv.org/abs/2309.09506)), ([:paperclip:](https://arxiv.org/pdf/2309.09506.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.09506)), ([:house:](https://huggingface.co/papers/2309.09506)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/layoutnuwa-revealing-the-hidden-layout)), ([:octocat:](https://github.com/projectnuwa/layoutnuwa)![GitHub Repo stars](https://img.shields.io/github/stars/projectnuwa/layoutnuwa?style=social)) |
| 9.18 | Multimodal Foundation Models: From Specialists to General-Purpose Assistants ([:x:](https://arxiv.org/abs/2309.10020)), ([:paperclip:](https://arxiv.org/pdf/2309.10020.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.10020)), ([:house:](https://huggingface.co/papers/2309.10020)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/multimodal-foundation-models-from-specialists)), ([:octocat:](https://github.com/computer-vision-in-the-wild/cvinw_readings)![GitHub Repo stars](https://img.shields.io/github/stars/computer-vision-in-the-wild/cvinw_readings?style=social)) |
| 9.18 | Adapting Large Language Models via Reading Comprehension ([:x:](https://arxiv.org/abs/2309.09530)), ([:paperclip:](https://arxiv.org/pdf/2309.09530.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.09530)), ([:house:](https://huggingface.co/papers/2309.09530)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/adapting-large-language-models-via-reading)), ([:octocat:](https://github.com/microsoft/lmops)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/lmops?style=social)) |
| 9.17 | OWL: A Large Language Model for IT Operations ([:x:](https://arxiv.org/abs/2309.09298)), ([:paperclip:](https://arxiv.org/pdf/2309.09298.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.09298)), ([:house:](https://huggingface.co/papers/2309.09298)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/owl-a-large-language-model-for-it-operations)) |
| 9.17 | CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages ([:x:](https://arxiv.org/abs/2309.09400)), ([:paperclip:](https://arxiv.org/pdf/2309.09400.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.09400)), ([:house:](https://huggingface.co/papers/2309.09400)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/culturax-a-cleaned-enormous-and-multilingual)) |
| 9.17 | Contrastive Decoding Improves Reasoning in Large Language Models ([:x:](https://arxiv.org/abs/2309.09117)), ([:paperclip:](https://arxiv.org/pdf/2309.09117.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.09117)), ([:house:](https://huggingface.co/papers/2309.09117)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/contrastive-decoding-improves-reasoning-in)) |
| 9.16 | PDFTriage: Question Answering over Long, Structured Documents ([:x:](https://arxiv.org/abs/2309.08872)), ([:paperclip:](https://arxiv.org/pdf/2309.08872.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.08872)), ([:house:](https://huggingface.co/papers/2309.08872)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pdftriage-question-answering-over-long)) |
| 9.16 | Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large Language Models for Dynamic Inference Using Sorted Fine-Tuning (SoFT) ([:x:](https://arxiv.org/abs/2309.08968)), ([:paperclip:](https://arxiv.org/pdf/2309.08968.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.08968)), ([:house:](https://huggingface.co/papers/2309.08968)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sorted-llama-unlocking-the-potential-of)) |
| 9.16 | Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data? ([:x:](https://arxiv.org/abs/2309.08963)), ([:paperclip:](https://arxiv.org/pdf/2309.08963.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.08963)), ([:house:](https://huggingface.co/papers/2309.08963)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/struc-bench-are-large-language-models-really)) |
| 9.15 | Compositional Foundation Models for Hierarchical Planning ([:x:](https://arxiv.org/abs/2309.08587)), ([:paperclip:](https://arxiv.org/pdf/2309.08587.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.08587)), ([:house:](https://huggingface.co/papers/2309.08587)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/compositional-foundation-models-for)) |
| 9.15 | Scaling Laws for Sparsely-Connected Foundation Models ([:x:](https://arxiv.org/abs/2309.08520)), ([:paperclip:](https://arxiv.org/pdf/2309.08520.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.08520)), ([:house:](https://huggingface.co/papers/2309.08520)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/scaling-laws-for-sparsely-connected)) |
| 9.15 | Investigating Answerability of LLMs for Long-Form Question Answering ([:x:](https://arxiv.org/abs/2309.08210)), ([:paperclip:](https://arxiv.org/pdf/2309.08210.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.08210)), ([:house:](https://huggingface.co/papers/2309.08210)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/investigating-answerability-of-llms-for-long)) |
| 9.15 | Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers ([:x:](https://arxiv.org/abs/2309.08532)), ([:paperclip:](https://arxiv.org/pdf/2309.08532.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.08532)), ([:house:](https://huggingface.co/papers/2309.08532)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/connecting-large-language-models-with)) |
| 9.15 | TextBind: Multi-turn Interleaved Multimodal Instruction-following ([:x:](https://arxiv.org/abs/2309.08637)), ([:paperclip:](https://arxiv.org/pdf/2309.08637.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.08637)), ([:house:](https://huggingface.co/papers/2309.08637)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/textbind-multi-turn-interleaved-multimodal)) |
| 9.15 | LASER: LLM Agent with State-Space Exploration for Web Navigation ([:x:](https://arxiv.org/abs/2309.08172)), ([:paperclip:](https://arxiv.org/pdf/2309.08172.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.08172)), ([:house:](https://huggingface.co/papers/2309.08172)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/laser-llm-agent-with-state-space-exploration)) |
| 9.14 | Towards Artificial General Intelligence (AGI) in the Internet of Things (IoT): Opportunities and Challenges ([:x:](https://arxiv.org/abs/2309.07438)), ([:book:](https://browse.arxiv.org/pdf/2309.07438.pdf)), ([:paperclip:](https://arxiv.org/pdf/2309.07438.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.07438)), ([:house:](https://huggingface.co/papers/2309.07438)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-artificial-general-intelligence-agi)) |
| 9.14 | The Rise and Potential of Large Language Model Based Agents: A Survey ([:x:](https://arxiv.org/abs/2309.07864)), ([:paperclip:](https://arxiv.org/pdf/2309.07864.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.07864)), ([:house:](https://huggingface.co/papers/2309.07864)), ([:eight_spoked_asterisk:]()) ,([:octocat:](https://github.com/WooooDyy/LLM-Agent-Paper-List)![GitHub Repo stars](https://img.shields.io/github/stars/WooooDyy/LLM-Agent-Paper-List?style=social)) |  
| 9.14 | Agents: An Open-source Framework for Autonomous Language Agents ([:x:](https://arxiv.org/abs/2309.07870)), ([:paperclip:](https://arxiv.org/pdf/2309.07870.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.07870)), ([:house:](https://huggingface.co/papers/2309.07870)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/agents-an-open-source-framework-for)), ([:octocat:](https://github.com/aiwaves-cn/agents)![GitHub Repo stars](https://img.shields.io/github/stars/aiwaves-cn/agents?style=social)) |
| 9.14 | Generative Image Dynamics ([:x:](https://arxiv.org/abs/2309.07906)), ([:paperclip:](https://arxiv.org/pdf/2309.07906.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.07906)), ([:house:](https://huggingface.co/papers/2309.07906)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/generative-image-dynamics)) |
| 9.14 | Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts ([:x:](https://arxiv.org/abs/2309.07430)), ([:paperclip:](https://arxiv.org/pdf/2309.07430.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.07430)), ([:house:](https://huggingface.co/papers/2309.07430)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/clinical-text-summarization-adapting-large)), ([:octocat:](https://github.com/stanfordmimi/clin-summ)![GitHub Repo stars](https://img.shields.io/github/stars/stanfordmimi/clin-summ?style=social)) |
| 9.14 | AudioSR: Versatile Audio Super-resolution at Scale ([:x:](https://arxiv.org/abs/2309.07314)), ([:paperclip:](https://arxiv.org/pdf/2309.07314.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.07314)), ([:house:](https://huggingface.co/papers/2309.07314)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/audiosr-versatile-audio-super-resolution-at)) |
| 9.14 | Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation? ([:x:](https://arxiv.org/abs/2309.07462)), ([:paperclip:](https://arxiv.org/pdf/2309.07462.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.07462)), ([:house:](https://huggingface.co/papers/2309.07462)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/are-large-language-model-based-evaluators-the)) |
| 9.14 | Ambiguity-Aware In-Context Learning with Large Language Models ([:x:](https://arxiv.org/abs/2309.07900)), ([:paperclip:](https://arxiv.org/pdf/2309.07900.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.07900)), ([:house:](https://huggingface.co/papers/2309.07900)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ambiguity-aware-in-context-learning-with)) |
| 9.13 | RAIN: Your Language Models Can Align Themselves without Finetuning ([:x:](https://arxiv.org/abs/2309.07124)), ([:paperclip:](https://arxiv.org/pdf/2309.07124.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.07124)), ([:house:](https://huggingface.co/papers/2309.07124)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rain-your-language-models-can-align)) |
| 9.13 | Text-Guided Generation and Editing of Compositional 3D Avatars ([:x:](https://arxiv.org/abs/2309.07125)), ([:paperclip:](https://arxiv.org/pdf/2309.07125.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.07125)), ([:house:](https://huggingface.co/papers/2309.07125)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/text-guided-generation-and-editing-of)) |
| 9.13 | MagiCapture: High-Resolution Multi-Concept Portrait Customization ([:x:](https://arxiv.org/abs/2309.06895)), ([:paperclip:](https://arxiv.org/pdf/2309.06895.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.06895)), ([:house:](https://huggingface.co/papers/2309.06895)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/magicapture-high-resolution-multi-concept)) |
| 9.13 | DreamStyler: Paint by Style Inversion with Text-to-Image Diffusion Models ([:x:](https://arxiv.org/abs/2309.06933)), ([:paperclip:](https://arxiv.org/pdf/2309.06933.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.06933)), ([:house:](https://huggingface.co/papers/2309.06933)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dreamstyler-paint-by-style-inversion-with)) |
| 9.12 | Re-Reading Improves Reasoning in Language Models ([:x:](https://arxiv.org/abs/2309.06275)), ([:paperclip:](https://arxiv.org/pdf/2309.06275.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.06275)), ([:house:](https://huggingface.co/papers/2309.06275)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/2309-06275)) |
| 9.12 | A Survey of Hallucination in Large Foundation Models ([:x:](https://arxiv.org/abs/2309.05922)), ([:paperclip:](https://arxiv.org/pdf/2309.05922.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.05922)), ([:house:](https://huggingface.co/papers/2309.05922)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-survey-of-hallucination-in-large-foundation)), ([:octocat:](https://github.com/vr25/hallucination-foundation-model-survey)![GitHub Repo stars](https://img.shields.io/github/stars/vr25/hallucination-foundation-model-survey?style=social))  |
| 9.12 | Learning Disentangled Avatars with Hybrid 3D Representations ([:x:](https://arxiv.org/abs/2309.06441)), ([:paperclip:](https://arxiv.org/pdf/2309.06441.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.06441)), ([:house:](https://huggingface.co/papers/2309.06441)), ([:eight_spoked_asterisk:](p)) |
| 9.12 | InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation ([:x:](https://arxiv.org/abs/2309.06380)), ([:paperclip:](https://arxiv.org/pdf/2309.06380.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.06380)), ([:house:](https://huggingface.co/papers/2309.06380)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/instaflow-one-step-is-enough-for-high-quality)), ([:octocat:](https://github.com/gnobitab/instaflow)![GitHub Repo stars](https://img.shields.io/github/stars/gnobitab/instaflow?style=social)) |
| 9.12 | Efficient Memory Management for Large Language Model Serving with PagedAttention ([:x:](https://arxiv.org/abs/2309.06180)), ([:paperclip:](https://arxiv.org/pdf/2309.06180.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.06180)), ([:house:](https://huggingface.co/papers/2309.06180)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/2309-06180)), ([:octocat:](https://github.com/zhaoolee/garss)![GitHub Repo stars](https://img.shields.io/github/stars/zhaoolee/garss?style=social)) |
| 9.12 | Large Language Model for Science: A Study on P vs. NP ([:x:](https://arxiv.org/abs/2309.05689)), ([:paperclip:](https://arxiv.org/pdf/2309.05689.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.05689)), ([:house:](https://huggingface.co/papers/2309.05689)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-model-for-science-a-study-on-p)), ([:octocat:](https://github.com/microsoft/lmops)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/lmops?style=social)) |
| 9.12 | AstroLLaMA: Towards Specialized Foundation Models in Astronomy ([:x:](https://arxiv.org/abs/2309.06126)), ([:paperclip:](https://arxiv.org/pdf/2309.06126.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.06126)), ([:house:](https://huggingface.co/papers/2309.06126)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/2309-06126)) |
| 9.12 | Uncovering mesa-optimization algorithms in Transformers ([:x:](https://arxiv.org/abs/2309.05858)), ([:paperclip:](https://arxiv.org/pdf/2309.05858.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.05858)), ([:house:](https://huggingface.co/papers/2309.05858)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/uncovering-mesa-optimization-algorithms-in)) |
| 9.11 | MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning ([:x:](https://arxiv.org/abs/2309.05653)), ([:paperclip:](https://arxiv.org/pdf/2309.05653.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.05653)), ([:house:](https://huggingface.co/papers/2309.05653)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mammoth-building-math-generalist-models)) |
| 9.11 | PhotoVerse: Tuning-Free Image Customization with Text-to-Image Diffusion Models ([:x:](https://arxiv.org/abs/2309.05793)), ([:paperclip:](https://arxiv.org/pdf/2309.05793.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.05793)), ([:house:](https://huggingface.co/papers/2309.05793)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/photoverse-tuning-free-image-customization)) |
| 9.11 | Large Language Models for Compiler Optimization ([:x:](https://arxiv.org/abs/2309.07062)), ([:paperclip:](https://arxiv.org/pdf/2309.07062.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.07062)), ([:house:](https://huggingface.co/papers/2309.07062)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-for-compiler)) |
| 9.11 | NExT-GPT: Any-to-Any Multimodal LLM ([:x:](https://arxiv.org/abs/2309.05519)), ([:paperclip:](https://arxiv.org/pdf/2309.05519.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.05519)), ([:house:](https://huggingface.co/papers/2309.05519)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/next-gpt-any-to-any-multimodal-llm)), ([:octocat:](https://github.com/NExT-GPT/NExT-GPT)![GitHub Repo stars](https://img.shields.io/github/stars/NExT-GPT/NExT-GPT?style=social)) |
| 9.11 | Textbooks Are All You Need II: phi-1.5 technical report ([:x:](https://arxiv.org/abs/2309.05463)), ([:paperclip:](https://arxiv.org/pdf/2309.05463.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.05463)), ([:house:](https://huggingface.co/papers/2309.05463)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/textbooks-are-all-you-need-ii-phi-1-5)) |
| 9.11 | Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs ([:x:](https://arxiv.org/abs/2309.05516)), ([:paperclip:](https://arxiv.org/pdf/2309.05516.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.05516)), ([:house:](https://huggingface.co/papers/2309.05516)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/optimize-weight-rounding-via-signed-gradient)), ([:octocat:](https://github.com/intel/neural-compressor)![GitHub Repo stars](https://img.shields.io/github/stars/intel/neural-compressor?style=social)) |
| 9.10 | Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art ([:x:](https://arxiv.org/abs/2309.04902)), ([:book:](https://browse.arxiv.org/pdf/2309.04902.pdf)), ([:paperclip:](https://arxiv.org/pdf/2309.04902.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.04902)), ([:house:](https://huggingface.co/papers/2309.04902)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/transformers-in-small-object-detection-a)), ([:octocat:](https://github.com/arekavandi/transformer-sod)![GitHub Repo stars](https://img.shields.io/github/stars/arekavandi/transformer-sod?style=social))  |
| 9.10 | Neurons in Large Language Models: Dead, N-gram, Positional ([:x:](https://arxiv.org/abs/2309.04827)), ([:paperclip:](https://arxiv.org/pdf/2309.04827.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.04827)), ([:house:](https://huggingface.co/papers/2309.04827)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/neurons-in-large-language-models-dead-n-gram)), ([:octocat:](https://github.com/zhaoolee/garss)![GitHub Repo stars](https://img.shields.io/github/stars/zhaoolee/garss?style=social)) |
| 9.9 | MADLAD-400: A Multilingual And Document-Level Large Audited Dataset ([:x:](https://arxiv.org/abs/2309.04662)), ([:paperclip:](https://arxiv.org/pdf/2309.04662.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.04662)), ([:house:](https://huggingface.co/papers/2309.04662)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/madlad-400-a-multilingual-and-document-level)) |
| 9.9 | When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale ([:x:](https://arxiv.org/abs/2309.04564)), ([:paperclip:](https://arxiv.org/pdf/2309.04564.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.04564)), ([:house:](https://huggingface.co/papers/2309.04564)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/when-less-is-more-investigating-data-pruning)) |
| 9.9 | FIAT: Fusing learning paradigms with Instruction-Accelerated Tuning ([:x:](https://arxiv.org/abs/2309.04663)), ([:paperclip:](https://arxiv.org/pdf/2309.04663.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.04663)), ([:house:](https://huggingface.co/papers/2309.04663)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fiat-fusing-learning-paradigms-with)) |
| 9.8 | From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting ([:x:](https://arxiv.org/abs/2309.04269)), ([:paperclip:](https://arxiv.org/pdf/2309.04269.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.04269)), ([:house:](https://huggingface.co/papers/2309.04269)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/from-sparse-to-dense-gpt-4-summarization-with)) |
| 9.8 | Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts ([:x:](https://arxiv.org/abs/2309.04354)), ([:paperclip:](https://arxiv.org/pdf/2309.04354.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.04354)), ([:house:](https://huggingface.co/papers/2309.04354)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mobile-v-moes-scaling-down-vision)) |
| 9.7. |FIND: A Function Description Benchmark for Evaluating Interpretability Methods ([:x:](https://arxiv.org/abs/2309.03886)), ([:book:](https://browse.arxiv.org/pdf/2309.03886.pdf)), ([:paperclip:](https://arxiv.org/pdf/2309.03886.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.03886)), ([:house:](https://huggingface.co/papers/2309.03886)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-function-interpretation-benchmark-for)), ([:octocat:](https://github.com/multimodal-interpretability/find)![GitHub Repo stars](https://img.shields.io/github/stars/multimodal-interpretability/find?style=social))  |
| 9.7 | GOV.UK - Frontier AI Taskforce: first progress report ([report](https://www.gov.uk/government/publications/frontier-ai-taskforce-first-progress-report/frontier-ai-taskforce-first-progress-report)) |
| 9.7 | InstructDiffusion: A Generalist Modeling Interface for Vision Tasks ([:x:](https://arxiv.org/abs/2309.03895)), ([:paperclip:](https://arxiv.org/pdf/2309.03895.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.03895)), ([:house:](https://huggingface.co/papers/2309.03895)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/instructdiffusion-a-generalist-modeling)) |
| 9.7 | ImageBind-LLM: Multi-modality Instruction Tuning ([:x:](https://arxiv.org/abs/2309.03905)), ([:paperclip:](https://arxiv.org/pdf/2309.03905.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.03905)), ([:house:](https://huggingface.co/papers/2309.03905)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/imagebind-llm-multi-modality-instruction)), ([:octocat:](https://github.com/opengvlab/llama-adapter)![GitHub Repo stars](https://img.shields.io/github/stars/opengvlab/llama-adapter?style=social)) |
| 9.7 | ProPainter: Improving Propagation and Transformer for Video Inpainting ([:x:](https://arxiv.org/abs/2309.03897)), ([:paperclip:](https://arxiv.org/pdf/2309.03897.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.03897)), ([:house:](https://huggingface.co/papers/2309.03897)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/propainter-improving-propagation-and)), ([:octocat:](https://github.com/sczhou/propainter)![GitHub Repo stars](https://img.shields.io/github/stars/sczhou/propainter?style=social)) |
| 9.7 | Tracking Anything with Decoupled Video Segmentation ([:x:](https://arxiv.org/abs/2309.03903)), ([:paperclip:](https://arxiv.org/pdf/2309.03903.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.03903)), ([:house:](https://huggingface.co/papers/2309.03903)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tracking-anything-with-decoupled-video)), ([:octocat:](https://github.com/hkchengrex/Tracking-Anything-with-DEVA)![GitHub Repo stars](https://img.shields.io/github/stars/hkchengrex/Tracking-Anything-with-DEVA?style=social)) |
| 9.7 | FLM-101B: An Open LLM and How to Train It with $100K Budget ([:x:](https://arxiv.org/abs/2309.03852)), ([:paperclip:](https://arxiv.org/pdf/2309.03852.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.03852)), ([:house:](https://huggingface.co/papers/2309.03852)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/flm-101b-an-open-llm-and-how-to-train-it-with)) |
| 9.7 | Large-Scale Automatic Audiobook Creation ([:x:](https://arxiv.org/abs/2309.03926)), ([:paperclip:](https://arxiv.org/pdf/2309.03926.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.03926)), ([:house:](https://huggingface.co/papers/2309.03926)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-scale-automatic-audiobook-creation)) |
| 9.7 | Large Language Models as Optimizers ([:x:](https://arxiv.org/abs/2309.03409)), ([:paperclip:](https://arxiv.org/pdf/2309.03409.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.03409)), ([:house:](https://huggingface.co/papers/2309.03409)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-as-optimizers)) |
| 9.7 | SyncDreamer: Generating Multiview-consistent Images from a Single-view Image ([:x:](https://arxiv.org/abs/2309.03453)), ([:paperclip:](https://arxiv.org/pdf/2309.03453.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.03453)), ([:house:](https://huggingface.co/papers/2309.03453)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/syncdreamer-generating-multiview-consistent)) |
| 9.7 | Text2Control3D: Controllable 3D Avatar Generation in Neural Radiance Fields using Geometry-Guided Text-to-Image Diffusion Model ([:x:](https://arxiv.org/abs/2309.03550)), ([:paperclip:](https://arxiv.org/pdf/2309.03550.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.03550)), ([:house:](https://huggingface.co/papers/2309.03550)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/text2control3d-controllable-3d-avatar)) |
| 9.7 | DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models ([:x:](https://arxiv.org/abs/2309.03883)), ([:paperclip:](https://arxiv.org/pdf/2309.03883.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.03883)), ([:house:](https://huggingface.co/papers/2309.03883)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dola-decoding-by-contrasting-layers-improves)), ([:octocat:](https://github.com/voidism/dola)![GitHub Repo stars](https://img.shields.io/github/stars/voidism/dola?style=social)) |
| 9.7 | XGen-7B Technical Report ([:x:](https://arxiv.org/abs/2309.03450)), ([:paperclip:](https://arxiv.org/pdf/2309.03450.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.03450)), ([:house:](https://huggingface.co/papers/2309.03450)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/xgen-7b-technical-report)) |
| 9.7 | Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation ([:x:](https://arxiv.org/abs/2309.03549)), ([:paperclip:](https://arxiv.org/pdf/2309.03549.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.03549)), ([:house:](https://huggingface.co/papers/2309.03549)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/reuse-and-diffuse-iterative-denoising-for)) |
| 9.7 | SLiMe: Segment Like Me ([:x:](https://arxiv.org/abs/2309.03179)), ([:paperclip:](https://arxiv.org/pdf/2309.03179.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.03179)), ([:house:](https://huggingface.co/papers/2309.03179)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/slime-segment-like-me)) |
| 9.6 | GPT Can Solve Mathematical Problems Without a Calculator ([:x:](https://arxiv.org/abs/2309.03241)), ([:paperclip:](https://arxiv.org/pdf/2309.03241.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.03241)), ([:house:](https://huggingface.co/papers/2309.03241)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gpt-can-solve-mathematical-problems-without-a)), ([:octocat:](https://github.com/thudm/mathglm)![GitHub Repo stars](https://img.shields.io/github/stars/thudm/mathglm?style=social)) |
| 9.6 | Physically Grounded Vision-Language Models for Robotic Manipulation ([:x:](https://arxiv.org/abs/2309.02561)), ([:paperclip:](https://arxiv.org/pdf/2309.02561.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.02561)), ([:house:](https://huggingface.co/papers/2309.02561)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/physically-grounded-vision-language-models)) |
| 9.6 | Doppelgangers: Learning to Disambiguate Images of Similar Structures ([:x:](https://arxiv.org/abs/2309.02420)), ([:paperclip:](https://arxiv.org/pdf/2309.02420.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.02420)), ([:house:](https://huggingface.co/papers/2309.02420)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/doppelgangers-learning-to-disambiguate-images)), ([:octocat:](https://github.com/RuojinCai/Doppelgangers)![GitHub Repo stars](https://img.shields.io/github/stars/RuojinCai/Doppelgangers?style=social)) |
| 9.5 | Artificial General Intelligence for Radiation Oncology ([:x:](https://arxiv.org/abs/2309.02590)), ([:book:](https://browse.arxiv.org/pdf/2309.02590.pdf)), ([:paperclip:](https://arxiv.org/pdf/2309.02590.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.02590)), ([:house:](https://huggingface.co/papers/2309.02590)), ([:eight_spoked_asterisk:]()) |
| 9.5 | Cognitive Architectures for Language Agents ([:x:](https://arxiv.org/abs/2309.02427)), ([:paperclip:](https://arxiv.org/pdf/2309.02427.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.02427)), ([:house:](https://huggingface.co/papers/2309.02427)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/cognitive-architectures-for-language-agents)), ([:octocat:](https://github.com/ysymyth/awesome-language-agents)![GitHub Repo stars](https://img.shields.io/github/stars/ysymyth/awesome-language-agents?style=social)) |
| 9.5 | Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning ([:x:](https://arxiv.org/abs/2309.02591)), ([:paperclip:](https://arxiv.org/pdf/2309.02591.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.02591)), ([:house:](https://huggingface.co/papers/2309.02591)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/scaling-autoregressive-multi-modal-models)) |
| 9.5 | One Wide Feedforward is All You Need ([:x:](https://arxiv.org/abs/2309.01826)), ([:paperclip:](https://arxiv.org/pdf/2309.01826.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.01826)), ([:house:](https://huggingface.co/papers/2309.01826)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/one-wide-feedforward-is-all-you-need)) |
| 9.5 | AniPortraitGAN: Animatable 3D Portrait Generation from 2D Image Collections ([:x:](https://arxiv.org/abs/2309.02186)), ([:paperclip:](https://arxiv.org/pdf/2309.02186.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.02186)), ([:house:](https://huggingface.co/papers/2309.02186)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/aniportraitgan-animatable-3d-portrait)) |
| 9.5 | PromptTTS 2: Describing and Generating Voices with Text Prompt ([:x:](https://arxiv.org/abs/2309.02285)), ([:paperclip:](https://arxiv.org/pdf/2309.02285.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.02285)), ([:house:](https://huggingface.co/papers/2309.02285)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/prompttts-2-describing-and-generating-voices)) |
| 9.5 | Hierarchical Masked 3D Diffusion Model for Video Outpainting ([:x:](https://arxiv.org/abs/2309.02119)), ([:paperclip:](https://arxiv.org/pdf/2309.02119.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.02119)), ([:house:](https://huggingface.co/papers/2309.02119)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hierarchical-masked-3d-diffusion-model-for)) |
| 9.4 | Concepts is All You Need: A More Direct Path to AGI ([:x:](https://arxiv.org/abs/2309.01622)), ([:book:](https://browse.arxiv.org/pdf/2309.01622.pdf)), ([:paperclip:](https://arxiv.org/pdf/2309.01622.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.01622)), ([:house:](https://huggingface.co/papers/2309.01622)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/concepts-is-all-you-need-a-more-direct-path)) |
| 9.4 | StyleAdapter: A Single-Pass LoRA-Free Model for Stylized Image Generation ([:x:](https://arxiv.org/abs/2309.01770)), ([:paperclip:](https://arxiv.org/pdf/2309.01770.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.01770)), ([:house:](https://huggingface.co/papers/2309.01770)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/styleadapter-a-single-pass-lora-free-model)) |
| 9.4 | ControlMat: A Controlled Generative Approach to Material Capture ([:x:](https://arxiv.org/abs/2309.01700)), ([:paperclip:](https://arxiv.org/pdf/2309.01700.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.01700)), ([:house:](https://huggingface.co/papers/2309.01700)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/controlmat-a-controlled-generative-approach)) | 
| 9.3 | ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models ([:x:](https://arxiv.org/abs/2309.00986)), ([:paperclip:](https://arxiv.org/pdf/2309.00986.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.00986)), ([:house:](https://huggingface.co/papers/2309.00986)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/modelscope-agent-building-your-customizable)), ([:octocat:](https://github.com/modelscope/modelscope-agent)![GitHub Repo stars](https://img.shields.io/github/stars/modelscope/modelscope-agent?style=social)) |
| 9.2 | Bias and Fairness in Large Language Models: A Survey ([:x:](https://arxiv.org/abs/2309.00770)), ([:paperclip:](https://arxiv.org/pdf/2309.00770.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.00770)), ([:house:](https://huggingface.co/papers/2309.00770)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/bias-and-fairness-in-large-language-models-a)), ([:octocat:](https://github.com/i-gallegos/fair-llm-benchmark)![GitHub Repo stars](https://img.shields.io/github/stars/i-gallegos/fair-llm-benchmark?style=social)) |
| 9.2 | Contrastive Feature Masking Open-Vocabulary Vision Transformer ([:x:](https://arxiv.org/abs/2309.00775)), ([:paperclip:](https://arxiv.org/pdf/2309.00775.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.00775)), ([:house:](https://huggingface.co/papers/2309.00775)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/contrastive-feature-masking-open-vocabulary)) |
| 9.2 | Efficient RLHF: Reducing the Memory Usage of PPO ([:x:](https://arxiv.org/abs/2309.00754)), ([:paperclip:](https://arxiv.org/pdf/2309.00754.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.00754)), ([:house:](https://huggingface.co/papers/2309.00754)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/efficient-rlhf-reducing-the-memory-usage-of)) |
| 9.2 | MagicProp: Diffusion-based Video Editing via Motion-aware Appearance Propagation ([:x:](https://arxiv.org/abs/2309.00908)), ([:paperclip:](https://arxiv.org/pdf/2309.00908.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.00908)), ([:house:](https://huggingface.co/papers/2309.00908)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/magicprop-diffusion-based-video-editing-via)) |
| 9.2 | Google's search for an AI future as it turns 25 (BBC [news](https://www.bbc.com/news/technology-66659361)) |
| 9.2 | ChatGPT Glossary: 41 AI Terms that Everyone Should Know ([blog](https://www.cnet.com/tech/computing/chatgpt-glossary-41-ai-terms-that-everyone-should-know/)) |
| 9.2 | CityDreamer: Compositional Generative Model of Unbounded 3D Cities ([:x:](https://arxiv.org/abs/2309.00610)), ([:paperclip:](https://arxiv.org/pdf/2309.00610.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.00610)), ([:house:](https://huggingface.co/papers/2309.00610)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/citydreamer-compositional-generative-model-of)) |
| 9.2 | Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following ([:x:](https://arxiv.org/abs/2309.00615)), ([:paperclip:](https://arxiv.org/pdf/2309.00615.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.00615)), ([:house:](https://huggingface.co/papers/2309.00615)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/point-bind-point-llm-aligning-point-cloud)), ([:octocat:](https://github.com/ziyuguo99/point-bind_point-llm)![GitHub Repo stars](https://img.shields.io/github/stars/ziyuguo99/point-bind_point-llm?style=social)) |
| 9.1 | RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback ([:x:](https://arxiv.org/abs/2309.00267)), ([:paperclip:](https://arxiv.org/pdf/2309.00267.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.00267)), ([:house:](https://huggingface.co/papers/2309.00267)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rlaif-scaling-reinforcement-learning-from)) |
| 9.1 | YaRN: Efficient Context Window Extension of Large Language Models ([:x:](https://arxiv.org/abs/2309.00071)), ([:paperclip:](https://arxiv.org/pdf/2309.00071.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.00071)), ([:house:](https://huggingface.co/papers/2309.00071)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/yarn-efficient-context-window-extension-of)), ([:octocat:](https://github.com/jquesnelle/yarn)![GitHub Repo stars](https://img.shields.io/github/stars/jquesnelle/yarn?style=social)) |
| 9.1 | VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation ([:x:](https://arxiv.org/abs/2309.00398)), ([:paperclip:](https://arxiv.org/pdf/2309.00398.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.00398)), ([:house:](https://huggingface.co/papers/2309.00398)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/videogen-a-reference-guided-latent-diffusion)) |
| 9.1 | Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior ([:x:](https://arxiv.org/abs/2309.00359)), ([:paperclip:](https://arxiv.org/pdf/2309.00359.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.00359)), ([:house:](https://huggingface.co/papers/2309.00359)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-content-and-behavior-models-to)) |
| 9.1 | FACET: Fairness in Computer Vision Evaluation Benchmark ([:x:](https://arxiv.org/abs/2309.00035)), ([:paperclip:](https://arxiv.org/pdf/2309.00035.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.00035)), ([:house:](https://huggingface.co/papers/2309.00035)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/facet-fairness-in-computer-vision-evaluation)) | 
| 9.1 | UT Researchers Use AI to Translate Thoughts Into Text ([blog](https://alcalde.texasexes.org/2023/09/ut-ai-research-thoughts-to-text/)) |
| 9.1 | Baidu launches Ernie chatbot after Chinese government approval ([news](https://www.theverge.com/2023/8/31/23853878/baidu-launch-ernie-ai-chatbot-china)) |
| 9.1 | The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants ([:x:](https://arxiv.org/abs/2308.16884)), ([:paperclip:](https://arxiv.org/pdf/2308.16884.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.16884)), ([:house:](https://huggingface.co/papers/2308.16884)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-belebele-benchmark-a-parallel-reading)) |
| 8.31 | PointLLM: Empowering Large Language Models to Understand Point Clouds ([:x:](https://arxiv.org/abs/2308.16911)), ([:book:](https://browse.arxiv.org/pdf/2308.16911.pdf)), ([:paperclip:](https://arxiv.org/pdf/2308.16911.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.16911)), ([:house:](https://huggingface.co/papers/2308.16911)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pointllm-empowering-large-language-models-to)), ([:octocat:](https://github.com/openrobotlab/pointllm)![GitHub Repo stars](https://img.shields.io/github/stars/openrobotlab/pointllm?style=social))  |
| 8.31 | AI Agents – Build and Host LLM Apps At Scale ([blog](https://blog.abacus.ai/blog/2023/08/31/supercharge-productivity-accomplish-10x-more-with-ai-agents/)) |
| 8.31 | UAE launches Arabic large language model in Gulf push into generative AI ([blog](https://www.ft.com/content/ab36d481-9e7c-4d18-855d-7d313db0db0d)) |
| 8.31 | UK MPs Propose Allies Form AI Union to Guard Against Adversaries ([news](https://www.bnnbloomberg.ca/uk-mps-propose-allies-form-ai-union-to-guard-against-adversaries-1.1965386)) |
| 8.31 | OpenAI released a new Teaching with AI ([blog](https://mashable.com/article/chatgpt-ai-guide-for-educators-teachers)) |
| 8.31 | BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge ([:x:](https://arxiv.org/abs/2308.16458)), ([:paperclip:](https://arxiv.org/pdf/2308.16458.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.16458)), ([:house:](https://huggingface.co/papers/2308.16458)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/biocoder-a-benchmark-for-bioinformatics-code)), ([:octocat:](https://github.com/gersteinlab/biocoder)![GitHub Repo stars](https://img.shields.io/github/stars/gersteinlab/biocoder?style=social))  |
| 8.31 | Any-Size-Diffusion: Toward Efficient Text-Driven Synthesis for Any-Size HD Images ([:x:](https://arxiv.org/abs/2308.16582)), ([:paperclip:](https://arxiv.org/pdf/2308.16582.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.16582)), ([:house:](https://huggingface.co/papers/2308.16582)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/any-size-diffusion-toward-efficient-text)) |
| 8.31 | LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models ([:x:](https://arxiv.org/abs/2308.16137)), ([:paperclip:](https://arxiv.org/pdf/2308.16137.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.16137)), ([:house:](https://huggingface.co/papers/2308.16137)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lm-infinite-simple-on-the-fly-length)) |
| 8.31 | MVDream: Multi-view Diffusion for 3D Generation ([:x:](https://arxiv.org/abs/2308.16512)), ([:paperclip:](https://arxiv.org/pdf/2308.16512.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.16512)), ([:house:](https://huggingface.co/papers/2308.16512)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mvdream-multi-view-diffusion-for-3d)) |
| 8.31 | Emergence of Segmentation with Minimalistic White-Box Transformers ([:x:](https://arxiv.org/abs/2308.16271)), ([:paperclip:](https://arxiv.org/pdf/2308.16271.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.16271)), ([:house:](https://huggingface.co/papers/2308.16271)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/emergence-of-segmentation-with-minimalistic)), ([:octocat:](https://github.com/ma-lab-berkeley/crate)![GitHub Repo stars](https://img.shields.io/github/stars/ma-lab-berkeley/crate?style=social)) |
| 8.31 | Learning Vision-based Pursuit-Evasion Robot Policies ([project](https://abajcsy.github.io/vision-based-pursuit/)), ([:x:](https://arxiv.org/abs/2308.16185)), ([:paperclip:](https://arxiv.org/pdf/2308.16185.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.16185)), ([:house:](https://huggingface.co/papers/2308.16185)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/learning-vision-based-pursuit-evasion-robot)) |
| 8.30 | SAM-Med2D ([:x:](https://arxiv.org/abs/2308.16184)), ([:paperclip:](https://arxiv.org/pdf/2308.16184.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.16184)), ([:house:](https://huggingface.co/papers/2308.16184)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sam-med2d)), ([:octocat:](https://github.com/OpenGVLab/SAM-Med2D)![GitHub Repo stars](https://img.shields.io/github/stars/OpenGVLab/SAM-Med2D?style=social)) |
| 8.30 | Large language models aren’t people. Let’s stop testing them as if they were (MIT TR [blog](https://www.technologyreview.com/2023/08/30/1078670/large-language-models-arent-people-lets-stop-testing-them-like-they-were/)) |
| 8.30 | Sobering Reports on AI for CPR, Cancer Treatment Advice ([blog](https://hms.harvard.edu/news/sobering-reports-ai-cpr-cancer-treatment-advice)) |
| 8.30 | Why Generative AI Needs Another Breakthrough Moment ([blog](https://pro.morningconsult.com/analysis/generative-ai-chat-gpt-why-breakthrough)) |
| 8.30 | Chinese ChatGPT alternatives just got approved for the general public (MIT TR [news](https://www.technologyreview.com/2023/08/30/1078714/chinese-chatgpt-ernie-government-approval/)) |
| 8.30 | OpenAI Nears $1 Billion of Annual Sales as ChatGPT Takes Off ([news](https://www.bloomberg.com/news/articles/2023-08-30/openai-nears-1-billion-of-annual-sales-as-chatgpt-takes-off)), ([archive.today](https://archive.is/MU0Ng)) |
| 8.30 | International Governance of Civilian AI: A Jurisdictional Certification Approach ([:x:](https://arxiv.org/abs/2308.15514)), ([:paperclip:](https://arxiv.org/pdf/2308.15514.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.15514)), ([:house:](https://huggingface.co/papers/2308.15514)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/international-governance-of-civilian-ai-a)) |
| 8.30 | AnomalyGPT: Detecting Industrial Anomalies using Large Vision-Language Models ([:x:](https://arxiv.org/abs/2308.15366)), ([:paperclip:](https://arxiv.org/pdf/2308.15366.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.15366)), ([:house:](https://huggingface.co/papers/2308.15366)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/anomalygpt-detecting-industrial-anomalies)), ([:octocat:](https://github.com/casia-iva-lab/anomalygpt)![GitHub Repo stars](https://img.shields.io/github/stars/casia-iva-lab/anomalygpt?style=social))  |
| 8.30 | LLaSM: Large Language and Speech Model ([proejct](https://huggingface.co/spaces/LinkSoul/LLaSM)), ([:x:](https://arxiv.org/abs/2308.15930)), ([:paperclip:](https://arxiv.org/pdf/2308.15930.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.15930)), ([:house:](https://huggingface.co/papers/2308.15930)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llasm-large-language-and-speech-model)), ([:octocat:](https://github.com/linksoul-ai/llasm)![GitHub Repo stars](https://img.shields.io/github/stars/linksoul-ai/llasm?style=social))  |
| 8.30 | RoboTAP: Tracking Arbitrary Points for Few-Shot Visual Imitation ([project](https://robotap.github.io/)), ([:x:](https://arxiv.org/abs/2308.15975)), ([:paperclip:](https://arxiv.org/pdf/2308.15975.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.15975)), ([:house:](https://huggingface.co/papers/2308.15975)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/robotap-tracking-arbitrary-points-for-few)) |
| 8.29 | Vector Search with OpenAI Embeddings: Lucene Is All You Need ([:x:](https://arxiv.org/abs/2308.14963)), ([:paperclip:](https://arxiv.org/pdf/2308.14963.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.14963)), ([:house:](https://huggingface.co/papers/2308.14963)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vector-search-with-openai-embeddings-lucene)) |
| 8.29 | Radiology-Llama2: Best-in-Class Large Language Model for Radiology ([:x:](https://arxiv.org/abs/2309.06419)), ([:paperclip:](https://arxiv.org/pdf/2309.06419.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2309.06419)), ([:house:](https://huggingface.co/papers/2309.06419)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/radiology-llama2-best-in-class-large-language)) |
| 8.29 | Inside Google's Plans To Fix Healthcare With Generative AI (Forbes [news](https://www.forbes.com/sites/katiejennings/2023/08/29/google-healthcare-generative-ai/?sh=62681557eab9)) |
| 8.29 | Google’s new Vertex AI features to unlock advanced LLM capabilities ([blog](https://www.infoworld.com/article/3705495/google-s-new-vertex-ai-features-to-unlock-advanced-llm-capabilities.html)) |
| 8.29 | The company landscape for artificial intelligence in large-molecule drug discovery (nature reviews drug discovery [doi: https://doi.org/10.1038/d41573-023-00139-0](https://www.nature.com/articles/d41573-023-00139-0)) |
| 8.29 | Full Code Medical Launches Full Code AI, the First Integration of ChatGPT in Software-Based Medical Simulation ([blog](https://www.digitaljournal.com/pr/news/pr-zen/full-code-medical-launches-full-code-ai-the-first-integration-of-chatgpt-in-software-based-medical-simulation)) |
| 8.29 | ChatGPT in Medical Education and Research: A Boon or a Bane? ([DOI: 10.7759/cureus.44316](https://www.cureus.com/articles/173399-chatgpt-in-medical-education-and-research-a-boon-or-a-bane#!/)) |
| 8.29 | OpenAI Unveils ChatGPT for Businesses, Stepping Up Revenue Push ([news](https://www.bloomberg.com/news/articles/2023-08-28/openai-unveils-chatgpt-for-business-customers-stepping-up-revenue-push)), ([archive.today](https://archive.is/abY2u)) |
| 8.28 | Graph Meets LLMs: Towards Large Graph Models ([:x:](https://arxiv.org/abs/2308.14522)), ([:book:](https://browse.arxiv.org/pdf/2308.14522.pdf)), ([:paperclip:](https://arxiv.org/pdf/2308.14522.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.14522)), ([:house:](https://huggingface.co/papers/2308.14522)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-graph-models-a-perspective)), ([:octocat:](https://github.com/thumnlab/awesome-large-graph-model)![GitHub Repo stars](https://img.shields.io/github/stars/thumnlab/awesome-large-graph-model?style=social))  |
| 8.28 | AI Deception: A Survey of Examples, Risks, and Potential Solutions ([:x:](https://arxiv.org/abs/2308.14752)), ([:paperclip:](https://arxiv.org/pdf/2308.14752.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.14752)), ([:house:](https://huggingface.co/papers/2308.14752)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ai-deception-a-survey-of-examples-risks-and)) |
| 8.28 | Is the AI boom already over? ([blog](https://www.vox.com/technology/2023/8/19/23837705/openai-chatgpt-microsoft-bing-google-generating-less-interest)) |
| 8.28 | Most Americans haven’t used ChatGPT; few think it will have a major impact on their job ([Pew Research Center [news](https://www.pewresearch.org/short-reads/2023/08/28/most-americans-havent-used-chatgpt-few-think-it-will-have-a-major-impact-on-their-job/)) |
| 8.28 | OpenAI - Introducing ChatGPT Enterprise ([blog](https://openai.com/blog/introducing-chatgpt-enterprise)) |
| 8.28 | PointHPS: Cascaded 3D Human Pose and Shape Estimation from Point Clouds ([:x:](https://arxiv.org/abs/2308.14492)), ([:paperclip:](https://arxiv.org/pdf/2308.14492.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.14492)), ([:house:](https://huggingface.co/papers/2308.14492)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pointhps-cascaded-3d-human-pose-and-shape)) |
| 8.27 | MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records ([:x:](https://arxiv.org/abs/2308.14089)), ([:paperclip:](https://arxiv.org/pdf/2308.14089.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.14089)), ([:house:](https://huggingface.co/papers/2308.14089)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/medalign-a-clinician-generated-dataset-for)) |
| 8.26 | ORES: Open-vocabulary Responsible Visual Synthesis ([:x:](https://arxiv.org/abs/2308.13785)), ([:paperclip:](https://arxiv.org/pdf/2308.13785.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.13785)), ([:house:](https://huggingface.co/papers/2308.13785)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ores-open-vocabulary-responsible-visual)), ([:octocat:](https://github.com/kodenii/ores)![GitHub Repo stars](https://img.shields.io/github/stars/kodenii/ores?style=social)) |
| 8.25 | The One Generative AI Risk That No One Is Talking About ([blog](https://www.investors.com/news/technology/generative-ai-battle-forms-should-big-tech-control-its-path/)) |
| 8.25 | Korea’s Naver joins generative AI race with HyperCLOVA X large language model ([blog](https://venturebeat.com/ai/koreas-naver-joins-generative-ai-race-with-hyperclova-x-large-language-model/)) |
| 8.25 | Can ChatGPT Transform Healthcare? ([blog](https://www.makeuseof.com/can-chatgpt-transform-healthcare/)) |
| 8.25 | OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models ([:x:](https://arxiv.org/abs/2308.13137)), ([:paperclip:](https://arxiv.org/pdf/2308.13137.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.13137)), ([:house:](https://huggingface.co/papers/2308.13137)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/omniquant-omnidirectionally-calibrated)), ([:octocat:](https://github.com/opengvlab/omniquant)![GitHub Repo stars](https://img.shields.io/github/stars/opengvlab/omniquant?style=social))  |
| 8.25 | Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities ([:x:](https://arxiv.org/abs/2308.12966)), ([:paperclip:](https://arxiv.org/pdf/2308.12966.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.12966)), ([:house:](https://huggingface.co/papers/2308.12966)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/qwen-vl-a-frontier-large-vision-language)), ([:octocat:](https://github.com/qwenlm/qwen-vl)![GitHub Repo stars](https://img.shields.io/github/stars/qwenlm/qwen-vl?style=social))  |
| 8.25 | Eventful Transformers: Leveraging Temporal Redundancy in Vision Transformers ([:x:](https://arxiv.org/abs/2308.13494)), ([:paperclip:](https://arxiv.org/pdf/2308.13494.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.13494)), ([:house:](https://huggingface.co/papers/2308.13494)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/eventful-transformers-leveraging-temporal)), ([:octocat:](https://github.com/WISION-Lab/eventful-transformer)![GitHub Repo stars](https://img.shields.io/github/stars/WISION-Lab/eventful-transformer?style=social))  |
| 8.25 | SoTaNa: The Open-Source Software Development Assistant ([:x:](https://arxiv.org/abs/2308.13416)), ([:paperclip:](https://arxiv.org/pdf/2308.13416.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.13416)), ([:house:](https://huggingface.co/papers/2308.13416)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sotana-the-open-source-software-development)), ([:octocat:](https://github.com/deepsoftwareanalytics/sotana)![GitHub Repo stars](https://img.shields.io/github/stars/deepsoftwareanalytics/sotana?style=social))  |
| 8.24 | Code Llama: Open Foundation Models for Code ([:x:](https://arxiv.org/abs/2308.12950)), ([:book:](https://browse.arxiv.org/pdf/2308.12950.pdf)), ([:paperclip:](https://arxiv.org/pdf/2308.12950.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.12950)), ([:house:](https://huggingface.co/papers/2308.12950)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/code-llama-open-foundation-models-for-code)), ([SS](https://www.semanticscholar.org/paper/Code-Llama%3A-Open-Foundation-Models-for-Code-Rozi%C3%A8re-Gehring/0b0debb710366cdff461938c80763eace1651af6)), ([:octocat:](https://github.com/facebookresearch/codellama)![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/codellama?style=social)) |
| 8.24 | Evaluating large language models on medical evidence summarization (npj Digital Medicine volume 6, [https://doi.org/10.1038/s41746-023-00896-7](https://www.nature.com/articles/s41746-023-00896-7)) |
| 8.24 | Harnessing AI for Psychiatric Use Requires More Nuanced Discussion ([blog](https://psychnews.psychiatryonline.org/doi/10.1176/appi.pn.2023.09.9.37)) |
| 8.24 | Use of Artificial Intelligence Chatbots for Cancer Treatment Information (JAMA Oncol. Published online August 24, 2023. [doi:10.1001/jamaoncol.2023.2954](https://jamanetwork.com/journals/jamaoncology/fullarticle/2808731)) |
| 8.24 | Prompt2Model: Generating Deployable Models from Natural Language Instructions ([:x:](https://arxiv.org/abs/2308.12261)), ([:paperclip:](https://arxiv.org/pdf/2308.12261.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.12261)), ([:house:](https://huggingface.co/papers/2308.12261)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/prompt2model-generating-deployable-models)), ([:octocat:](https://github.com/neulab/prompt2model)![GitHub Repo stars](https://img.shields.io/github/stars/neulab/prompt2model?style=social)) |
| 8.24 | American Stories: A Large-Scale Structured Text Dataset of Historical U.S. Newspapers ([:x:](https://arxiv.org/abs/2308.12477)), ([:paperclip:](https://arxiv.org/pdf/2308.12477.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.12477)), ([:house:](https://huggingface.co/papers/2308.12477)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/american-stories-a-large-scale-structured)), ([:octocat:](https://github.com/dell-research-harvard/americanstories)![GitHub Repo stars](https://img.shields.io/github/stars/dell-research-harvard/americanstories?style=social))  |
| 8.24 | Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities ([:x:](https://arxiv.org/abs/2308.12833)), ([:paperclip:](https://arxiv.org/pdf/2308.12833.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.12833)), ([:house:](https://huggingface.co/papers/2308.12833)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/use-of-llms-for-illicit-purposes-threats)) |
| 8.24 | Problems in using LLMs in commercial products ([blog](https://ehudreiter.com/2023/08/24/llm-problems-in-commercial-products/)) |
| 8.24 | Code LLaMA is now on Perplexity’s LLaMa Chat! ([tweet](https://twitter.com/perplexity_ai/status/1694845231936557437)), ([labs](https://labs.perplexity.ai/?utm_content=first_codellama&s=u&utm_source=twitter&utm_campaign=labs)) |
| 8.24 | Meta AI released Code Llama, a large language model built on top of Llama 2, fine-tuned for coding & state-of-the-art ([tweet](https://twitter.com/MetaAI/status/1694729071325007993)), ([blog](https://ai.meta.com/blog/code-llama-large-language-model-coding/?utm_source=twitter&utm_medium=organic_social&utm_campaign=codellama&utm_content=gif)), ([paper](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)), ([:octocat:](https://github.com/facebookresearch/codellama)![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/codellama?style=social)), ([Model](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)) |
| 8.23 | Efficient Benchmarking (of Language Models) ([:x:](https://arxiv.org/abs/2308.11696)), ([:paperclip:](https://arxiv.org/pdf/2308.11696.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.11696)), ([:house:](https://huggingface.co/papers/2308.11696)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/efficient-benchmarking-of-language-models)) |
| 8.23 | New Study Gives ChatGPT High Marks as a CDS Tool ([news](https://www.healthleadersmedia.com/technology/new-study-gives-chatgpt-high-marks-cds-tool)) |
| 8.23 | OpenAI launched fine-tuning for GPT-3.5 Turbo! Fine-tuning ([tweet](https://twitter.com/OpenAI/status/1694062483462594959)), ([blog](https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates)) |
| 8.23 | Seamless4MT: Massive Multilingual Multimodal Machine Translation ([paper](https://ai.meta.com/research/publications/seamless-m4t/)), ([code](https://github.com/facebookresearch/seamless_communication)), ([blog](https://ai.meta.com/resources/models-and-libraries/seamless-communication/?utm_source=facebook&utm_medium=organic_social&utm_campaign=seamless&utm_content=video)), ([demo](https://seamless.metademolab.com/)), ([tweet](https://twitter.com/MetaAI/status/1694020437532151820)) |
| 8.22 | A Survey on Large Language Model based Autonomous Agents ([:x:](https://arxiv.org/abs/2308.11432)), ([:paperclip:](https://arxiv.org/pdf/2308.11432.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.11432)), ([:house:](https://huggingface.co/papers/2308.11432)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-survey-on-large-language-model-based)), ([:octocat:](https://github.com/paitesanshi/llm-agent-survey)![GitHub Repo stars](https://img.shields.io/github/stars/paitesanshi/llm-agent-survey?style=social))  |
| 8.22 | Comparison of Ophthalmologist and Large Language Model Chatbot Responses to Online Patient Eye Care Questions (JAMA Netw Open. 2023;6(8):e2330320. [doi:10.1001/jamanetworkopen.2023.30320](https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2808557)) |
| 8.22 | Assessing the Utility of ChatGPT Throughout the Entire Clinical Workflow: Development and Usability Study (J Med Internet Res 2023;25:e48659 [doi: 10.2196/48659](https://www.jmir.org/2023/1/e48659/)) |
| 8.22 | Giraffe - 32K Long Context Open-Source LLMs ([tweet](https://twitter.com/bindureddy/status/1694126931174977906)), ([blog](https://blog.abacus.ai/blog/2023/08/22/giraffe-long-context-llms/)), ([Model](https://huggingface.co/abacusai/Giraffe-v2-13b-32k)) |
| 8.22 | Language to rewards for robotic skill synthesis (Google [blog](https://ai.googleblog.com/2023/08/language-to-rewards-for-robotic-skill.html)), ([tweet](https://twitter.com/GoogleAI/status/1694086273689076170)) |
| 8.22 | Stabilizing Unsupervised Environment Design with a Learned Adversary ([:x:](https://arxiv.org/abs/2308.10797)), ([:paperclip:](https://arxiv.org/pdf/2308.10797.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.10797)), ([:house:](https://huggingface.co/papers/2308.10797)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/stabilizing-unsupervised-environment-design)), ([:octocat:](https://github.com/facebookresearch/dcd)![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/dcd?style=social)) |
| 8.21 | Large Language Models for Software Engineering: A Systematic Literature Review ([:x:](https://arxiv.org/abs/2308.10620)), ([:book:](https://browse.arxiv.org/pdf/2308.10620.pdf)), ([:paperclip:](https://arxiv.org/pdf/2308.10620.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.10620)), ([:house:](https://huggingface.co/papers/2308.10620)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-for-software)), ([SS](https://www.semanticscholar.org/paper/Large-Language-Models-for-Software-Engineering%3A-A-Hou-Zhao/000f964393dafe113a8e66734d63b2a145844159)) |
| 8.21 | AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors ([:x:](https://arxiv.org/abs/2308.15504)), ([:book:](https://browse.arxiv.org/pdf/2308.15504.pdf)), ([:paperclip:](https://arxiv.org/pdf/2308.15504.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.15504)), ([:house:](https://huggingface.co/papers/2308.15504)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/agentverse-facilitating-multi-agent)), ([:octocat:](https://github.com/openbmb/agentverse)![GitHub Repo stars](https://img.shields.io/github/stars/openbmb/agentverse?style=social))  |
| 8.21 | Giraffe: Adventures in Expanding Context Lengths in LLMs ([:x:](https://arxiv.org/abs/2308.10882)), ([:paperclip:](https://arxiv.org/pdf/2308.10882.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.10882)), ([:house:](https://huggingface.co/papers/2308.10882)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/giraffe-adventures-in-expanding-context)), ([:octocat:](https://github.com/abacusai/long-context)![GitHub Repo stars](https://img.shields.io/github/stars/abacusai/long-context?style=social)) |
| 8.21 | TADA! Text to Animatable Digital Avatars ([project](https://tada.is.tue.mpg.de/)), ([:x:](https://arxiv.org/abs/2308.10899)), ([:paperclip:](https://arxiv.org/pdf/2308.10899.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.10899)), ([:house:](https://huggingface.co/papers/2308.10899)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tada-text-to-animatable-digital-avatars)), ([tweet](https://twitter.com/HongweiYi2/status/1693985328716198168)) |
| 8.21 | Large Language Models on Wikipedia-Style Survey Generation: an Evaluation in NLP Concepts ([:x:](https://arxiv.org/abs/2308.10410)), ([:paperclip:](https://arxiv.org/pdf/2308.10410.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.10410)), ([:house:](https://huggingface.co/papers/2308.10410)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-on-wikipedia-style)) |
| 8.21 | Instruction Tuning for Large Language Models: A Survey ([:x:](https://arxiv.org/abs/2308.10792)), ([:paperclip:](https://arxiv.org/pdf/2308.10792.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.10792)), ([:house:](https://huggingface.co/papers/2308.10792)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/instruction-tuning-for-large-language-models)) |
| 8.21 | Large Language Models in Hematology Case Solving: A Comparative Study of ChatGPT-3.5, Google Bard, and Microsoft Bing ([DOI: 10.7759/cureus.43861 ](https://www.cureus.com/articles/179601-large-language-models-in-hematology-case-solving-a-comparative-study-of-chatgpt-35-google-bard-and-microsoft-bing#!/)) |
| 8.20 | LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models ([project](https://hazyresearch.stanford.edu/legalbench/)), ([:x:](https://arxiv.org/abs/2308.11462)), ([:paperclip:](https://arxiv.org/pdf/2308.11462.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.11462)), ([:house:](https://huggingface.co/papers/2308.11462)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/legalbench-a-collaboratively-built-benchmark)) |
| 8.19 | HumanLiff: Layer-wise 3D Human Generation with Diffusion Model ([:x:](https://arxiv.org/abs/2308.09712)), ([:paperclip:](https://arxiv.org/pdf/2308.09712.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.09712)), ([:house:](https://huggingface.co/papers/2308.09712)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/humanliff-layer-wise-3d-human-generation-with)) |
| 8.19 | Meet FraudGPT: The Dark Side Twin of ChatGPT ([news](https://www.marktechpost.com/2023/08/19/meet-fraudgpt-the-dark-side-twin-of-chatgpt/)) |
| 8.19 | AI2 Dolma: 3 Trillion Token Open Corpus for Language Model Pretraining ([blog](https://blog.allenai.org/dolma-3-trillion-tokens-open-llm-corpus-9a0ff4b8da64)) |
| 8.19 | AI2 drops biggest open dataset yet for training language models (TechCrunch [news](https://techcrunch.com/2023/08/18/ai2-drops-biggest-open-dataset-yet-for-training-language-models/)) |
| 8.18 | Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies ([:x:](https://arxiv.org/abs/2308.10264)), ([:paperclip:](https://arxiv.org/pdf/2308.10264.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.10264)), ([:house:](https://huggingface.co/papers/2308.10264)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/using-large-language-models-to-simulate)) |
| 8.18 | Graph of Thoughts: Solving Elaborate Problems with Large Language Models ([:x:](https://arxiv.org/abs/2308.09687)), ([:paperclip:](https://arxiv.org/pdf/2308.09687.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.09687)), ([:house:](https://huggingface.co/papers/2308.09687)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/graph-of-thoughts-solving-elaborate-problems)), ([:octocat:](https://github.com/spcl/graph-of-thoughts)![GitHub Repo stars](https://img.shields.io/github/stars/spcl/graph-of-thoughts?style=social)) |
| 8.18 | NYU Langone Health Holds First Generative AI "Prompt-a-Thon" ([tweet](https://twitter.com/nyugrossman/status/1692530308254413063)), ([news](https://nyulangone.org/news/nyu-langone-health-hold-ai-prompt-thon-event)), (Nature [paper](https://nyulangone.org/news/nyu-langone-health-hold-ai-prompt-thon-event)) |
| 8.18 | Autonomous visual information seeking with large language models (Google [blog](https://ai.googleblog.com/2023/08/autonomous-visual-information-seeking.html)) |
| 8.18 | Mind + Machine: ChatGPT as a Basic Clinical Decisions Support Tool ([DOI: 10.7759/cureus.43690](https://www.cureus.com/articles/167241-mind--machine-chatgpt-as-a-basic-clinical-decisions-support-tool#!/)) |
| 8.17 | Reinforced Self-Training for Language Modeling ([:x:](https://arxiv.org/abs/2308.08998)), ([:paperclip:](https://arxiv.org/pdf/2308.08998.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.08998)), ([:house:](https://huggingface.co/papers/2308.08998)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/reinforced-self-training-rest-for-language)) |
| 8.17 | Consciousness in Artificial Intelligence: Insights from the Science of Consciousness ([:x:](https://arxiv.org/abs/2308.08708)), ([:paperclip:](https://arxiv.org/pdf/2308.08708.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.08708)), ([:house:](https://huggingface.co/papers/2308.08708)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/consciousness-in-artificial-intelligence)) |
| 8.17 | OpenAI acquires start-up Global Illumination to work on core products, ChatGPT (Reuters [news](https://www.reuters.com/markets/deals/openai-acquires-start-up-global-illumination-work-core-products-chatgpt-2023-08-16/)) |
| 8.16 | RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models ([:x:](https://arxiv.org/abs/2308.07922)), ([:paperclip:](https://arxiv.org/pdf/2308.07922.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.07922)), ([:house:](https://huggingface.co/papers/2308.07922)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/raven-in-context-learning-with-retrieval)) |
| 8.16 | Atom-by-atom protein generation and beyond with language models ([:x:](https://arxiv.org/abs/2308.09482)), ([:paperclip:](https://arxiv.org/pdf/2308.09482.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.09482)), ([:house:](https://huggingface.co/papers/2308.09482)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/atom-by-atom-protein-generation-and-beyond)) |
| 8.16 | TeCH: Text-guided Reconstruction of Lifelike Clothed Humans ([:x:](https://arxiv.org/abs/2308.08545)), ([:paperclip:](https://arxiv.org/pdf/2308.08545.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.08545)), ([:house:](https://huggingface.co/papers/2308.08545)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tech-text-guided-reconstruction-of-lifelike)) |
| 8.16 | Open challenges in LLM research ([blog](https://huyenchip.com/2023/08/16/llm-research-open-challenges.html)) |
| 8.16 | Microsoft Introduces Azure ChatGPT: A Private Version of ChatGPT Tailored for the Enterprise ([news](https://www.marktechpost.com/2023/08/16/microsoft-introduces-azure-chatgpt-a-private-version-of-chatgpt-tailored-for-the-enterprise/)) |
| 8.15 | GOV.UK - Artificial Intelligence for Decarbonisation innovation programme: Stream 3 ([announcement](https://www.gov.uk/government/publications/artificial-intelligence-for-decarbonisation-innovation-programme-stream-3)) |
| 8.15 | Introducing DeciCoder: The New Gold Standard in Efficient and Accurate Code Generation ([blog](https://deci.ai/blog/decicoder-efficient-and-accurate-code-generation-llm/)), ([project](https://huggingface.co/Deci/DeciCoder-1b)) |
| 8.15 | CALYPSO: LLMs as Dungeon Masters' Assistants ([:x:](https://arxiv.org/abs/2308.07540)), ([:paperclip:](https://arxiv.org/pdf/2308.07540.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.07540)), ([:house:](https://huggingface.co/papers/2308.07540)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/calypso-llms-as-dungeon-masters-assistants)) |
| 8.15 | CoDeF: Content Deformation Fields for Temporally Consistent Video Processing ([project](https://qiuyu96.github.io/CoDeF/)), ([Hires Demo](https://ezioby.github.io/CoDeF_Demo/)), ([:x:](https://arxiv.org/abs/2308.07926)), ([:paperclip:](https://arxiv.org/pdf/2308.07926.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.07926)), ([:house:](https://huggingface.co/papers/2308.07926)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/codef-content-deformation-fields-for)), ([:octocat:](https://github.com/qiuyu96/codef)![GitHub Repo stars](https://img.shields.io/github/stars/qiuyu96/codef?style=social)) |
| 8.15 | Link-Context Learning for Multimodal LLMs ([:x:](https://arxiv.org/abs/2308.07891)), ([:paperclip:](https://arxiv.org/pdf/2308.07891.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.07891)), ([:house:](https://huggingface.co/papers/2308.07891)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/link-context-learning-for-multimodal-llms)), ([:octocat:](https://github.com/isekai-portal/Link-Context-Learning)![GitHub Repo stars](https://img.shields.io/github/stars/isekai-portal/Link-Context-Learning?style=social)) |
| 8.15 | Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification ([:x:](https://arxiv.org/abs/2308.07921)), ([:paperclip:](https://arxiv.org/pdf/2308.07921.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.07921)), ([:house:](https://huggingface.co/papers/2308.07921)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/solving-challenging-math-word-problems-using)) |
| 8.15 | Teach LLMs to Personalize -- An Approach inspired by Writing Education ([:x:](https://arxiv.org/abs/2308.07968)), ([:paperclip:](https://arxiv.org/pdf/2308.07968.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.07968)), ([:house:](https://huggingface.co/papers/2308.07968)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/teach-llms-to-personalize-an-approach)) |
| 8.14 | LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked ([:x:](https://arxiv.org/abs/2308.07308)), ([:paperclip:](https://arxiv.org/pdf/2308.07308.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.07308)), ([:house:](https://huggingface.co/papers/2308.07308)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llm-self-defense-by-self-examination-llms)) |
| 8.14 | GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text ([:x:](https://arxiv.org/abs/2308.06911)), ([:paperclip:](https://arxiv.org/pdf/2308.06911.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.06911)), ([:house:](https://huggingface.co/papers/2308.06911)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/git-mol-a-multi-modal-large-language-model)) |
| 8.14 | Chatbots in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug Development with ChatGPT ([:x:](https://arxiv.org/abs/2308.06920)), ([:paperclip:](https://arxiv.org/pdf/2308.06920.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.06920)), ([:house:](https://huggingface.co/papers/2308.06920)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chatbots-in-drug-discovery-a-case-study-on)) |
| 8.14 | Large Language Models for Information Retrieval: A Survey ([:x:](https://arxiv.org/abs/2308.07107)), ([:paperclip:](https://arxiv.org/pdf/2308.07107.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.07107)), ([:house:](https://huggingface.co/papers/2308.07107)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-for-information)), ([:octocat:](https://github.com/ruc-nlpir/llm4ir-survey)![GitHub Repo stars](https://img.shields.io/github/stars/ruc-nlpir/llm4ir-survey?style=social))  |
| 8.14 | Bayesian Flow Networks ([:x:](https://arxiv.org/abs/2308.07037)), ([:paperclip:](https://arxiv.org/pdf/2308.07037.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.07037)), ([:house:](https://huggingface.co/papers/2308.07037)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/bayesian-flow-networks)) |
| 8.14 | Mind your Language (Model): Fact-Checking LLMs and their Role in NLP Research and Practice ([:x:](https://arxiv.org/abs/2308.07120)), ([:paperclip:](https://arxiv.org/pdf/2308.07120.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.07120)), ([:house:](https://huggingface.co/papers/2308.07120)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mind-your-language-model-fact-checking-llms)) |
| 8.14 | The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation ([:x:](https://arxiv.org/abs/2308.07286)), ([:paperclip:](https://arxiv.org/pdf/2308.07286.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.07286)), ([:house:](https://huggingface.co/papers/2308.07286)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-devil-is-in-the-errors-leveraging-large)) |
| 8.14 | CausalLM is not optimal for in-context learning ([:x:](https://arxiv.org/abs/2308.06912)), ([:paperclip:](https://arxiv.org/pdf/2308.06912.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.06912)), ([:house:](https://huggingface.co/papers/2308.06912)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/causallm-is-not-optimal-for-in-context)) |
| 8.14 | OctoPack: Instruction Tuning Code Large Language Models ([:x:](https://arxiv.org/abs/2308.07124)), ([:paperclip:](https://arxiv.org/pdf/2308.07124.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.07124)), ([:house:](https://huggingface.co/papers/2308.07124)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/octopack-instruction-tuning-code-large)), ([:octocat:](https://github.com/bigcode-project/octopack)![GitHub Repo stars](https://img.shields.io/github/stars/bigcode-project/octopack?style=social)) |
| 8.14 | SpeechX: Neural Codec Language Model as a Versatile Speech Transformer ([:x:](https://arxiv.org/abs/2308.06873)), ([:paperclip:](https://arxiv.org/pdf/2308.06873.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.06873)), ([:house:](https://huggingface.co/papers/2308.06873)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/speechx-neural-codec-language-model-as-a)) |
| 8.13 | What if Generative AI turned out to be a Dud? ([blog](https://garymarcus.substack.com/p/what-if-generative-ai-turned-out)) |
| 8.13 | The most powerful open source instructions dataset: Flan (378 Million samples) ([tweet](https://twitter.com/Yampeleg/status/1690398400775176192)), ([HF](https://huggingface.co/datasets/Open-Orca/FLAN)) |
| 8.13 | VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use ([:x:](https://arxiv.org/abs/2308.06595)), ([:paperclip:](https://arxiv.org/pdf/2308.06595.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.06595)), ([:house:](https://huggingface.co/papers/2308.06595)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/visit-bench-a-benchmark-for-vision-language)) |
| 8.13 | IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models ([:x:](https://arxiv.org/abs/2308.06721)), ([:paperclip:](https://arxiv.org/pdf/2308.06721.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.06721)), ([:house:](https://huggingface.co/papers/2308.06721)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ip-adapter-text-compatible-image-prompt)) |
| 8.12 | A new solution and concrete implementation steps for Artificial General Intelligence ([:x:](https://arxiv.org/abs/2308.09721)), ([:book:](https://browse.arxiv.org/pdf/2308.09721.pdf)), ([:paperclip:](https://arxiv.org/pdf/2308.09721.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.09721)), ([:house:](https://huggingface.co/papers/2308.09721)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-new-solution-and-concrete-implementation)) |
| 8.12 | AI Town - a virtual town where AI characters live, chat and socialize 🏠💻💌 ([:octocat:](https://github.com/a16z-infra/ai-town)![GitHub Repo stars](https://img.shields.io/github/stars/a16z-infra/ai-town?style=social)), ([Live Demo](https://www.convex.dev/ai-town)) |
| 8.12 | GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher ([project](https://llmcipherchat.github.io/)), ([:x:](https://arxiv.org/abs/2308.06463)), ([:paperclip:](https://arxiv.org/pdf/2308.06463.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.06463)), ([:house:](https://huggingface.co/papers/2308.06463)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gpt-4-is-too-smart-to-be-safe-stealthy-chat)), ([:octocat:](https://github.com/robustnlp/cipherchat)![GitHub Repo stars](https://img.shields.io/github/stars/robustnlp/cipherchat?style=social)) |
| 8.12 | Release the Platypus family of finetuned LLMs ([tweet](https://twitter.com/natanielruizg/status/1690048207030493189)), ([project](https://platypus-llm.github.io/)), ([paper](https://platypus-llm.github.io/Platypus.pdf)), ([:octocat:](https://github.com/arielnlee/Platypus)![GitHub Repo stars](https://img.shields.io/github/stars/arielnlee/Platypus?style=social)) |
| 8.12 | Self-Alignment with Instruction Backtranslation ([:x:](https://arxiv.org/abs/2308.06259)), ([:paperclip:](https://arxiv.org/pdf/2308.06259.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.06259)), ([:house:](https://huggingface.co/papers/2308.06259)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/self-alignment-with-instruction)) |
| 8.11 | Detecting and Preventing Hallucinations in Large Vision Language Models ([:x:](https://arxiv.org/abs/2308.06394)), ([:book:](https://browse.arxiv.org/pdf/2308.06394.pdf)), ([:paperclip:](https://arxiv.org/pdf/2308.06394.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.06394)), ([:house:](https://huggingface.co/papers/2308.06394)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/detecting-and-preventing-hallucinations-in)) |
| 8.11 | BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents ([:x:](https://arxiv.org/abs/2308.05960)), ([:paperclip:](https://arxiv.org/pdf/2308.05960.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.05960)), ([:house:](https://huggingface.co/papers/2308.05960)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/bolaa-benchmarking-and-orchestrating-llm)) |
| 8.11 | AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining ([:x:](https://arxiv.org/abs/2308.05734)), ([:paperclip:](https://arxiv.org/pdf/2308.05734.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.05734)), ([:house:](https://huggingface.co/papers/2308.05734)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/audioldm-2-learning-holistic-audio-generation)) |
| 8.11 | Follow Anything: Open-set detection, tracking, and following in real-time ([:x:](https://arxiv.org/abs/2308.05737)), ([:paperclip:](https://arxiv.org/pdf/2308.05737.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.05737)), ([:house:](https://huggingface.co/papers/2308.05737)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/follow-anything-open-set-detection-tracking)) |
| 8.11 | ChatGPT expands its ‘custom instructions’ feature to free users (TechCrunch [news](https://techcrunch.com/2023/08/10/chatgpt-expands-its-custom-instructions-feature-to-free-users)) |
| 8.10 | The Multi-modality Cell Segmentation Challenge: Towards Universal Solutions ([:x:](https://arxiv.org/abs/2308.05864)), ([:paperclip:](https://arxiv.org/pdf/2308.05864.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.05864)), ([:house:](https://huggingface.co/papers/2308.05864)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-multi-modality-cell-segmentation)) |
| 8.10 | DOD Announces Establishment of Generative AI Task Force (U.S. Department of Defense, [Release](https://www.defense.gov/News/Releases/Release/Article/3489803/dod-announces-establishment-of-generative-ai-task-force/)) |
| 8.10 | Metacognitive Prompting Improves Understanding in Large Language Models ([:x:](https://arxiv.org/abs/2308.05342)), ([:paperclip:](https://arxiv.org/pdf/2308.05342.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.05342)), ([:house:](https://huggingface.co/papers/2308.05342)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/metacognitive-prompting-improves)) |
| 8.10 | Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems ([:x:](https://arxiv.org/abs/2308.05713)), ([:paperclip:](https://arxiv.org/pdf/2308.05713.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.05713)), ([:house:](https://huggingface.co/papers/2308.05713)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/testing-gpt-4-with-wolfram-alpha-and-code)) |
| 8.10 | Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment ([:x:](https://arxiv.org/abs/2308.05374)), ([:paperclip:](https://arxiv.org/pdf/2308.05374.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.05374)), ([:house:](https://huggingface.co/papers/2308.05374)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/trustworthy-llms-a-survey-and-guideline-for)) |
| 8.10 | OpenProteinSet: Training data for structural biology at scale ([:x:](https://arxiv.org/abs/2308.05326)), ([:paperclip:](https://arxiv.org/pdf/2308.05326.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.05326)), ([:house:](https://huggingface.co/papers/2308.05326)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/openproteinset-training-data-for-structural)) |
| 8.9 | Inst-Inpaint: Instructing to Remove Objects with Diffusion Models ([project](http://instinpaint.abyildirim.com/)), ([:x:](https://arxiv.org/abs/2304.03246)), ([:paperclip:](https://arxiv.org/pdf/2304.03246.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.03246)), ([:house:](https://huggingface.co/papers/2304.03246)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/inst-inpaint-instructing-to-remove-objects)), ([:octocat:](https://github.com/abyildirim/inst-inpaint)![GitHub Repo stars](https://img.shields.io/github/stars/abyildirim/inst-inpaint?style=social)), ([demo](https://huggingface.co/spaces/abyildirim/inst-inpaint)) |
| 8.9 | A Comparative Study of Open-Source Large Language Models, GPT-4 and Claude 2: Multiple-Choice Test Taking in Nephrology ([:x:](https://arxiv.org/abs/2308.04709)), ([:paperclip:](https://arxiv.org/pdf/2308.04709.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.04709)), ([:house:](https://huggingface.co/papers/2308.04709)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-comparative-study-of-open-source-large)) |
| 8.9 | LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking ([:x:](https://arxiv.org/abs/2308.04945)), ([:paperclip:](https://arxiv.org/pdf/2308.04945.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.04945)), ([:house:](https://huggingface.co/papers/2308.04945)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llmebench-a-flexible-framework-for)), ([:octocat:](https://github.com/qcri/llmebench)![GitHub Repo stars](https://img.shields.io/github/stars/qcri/llmebench?style=social)) |
| 8.9 | Extrapolating Large Language Models to Non-English by Aligning Languages ([:x:](https://arxiv.org/abs/2308.04948)), ([:paperclip:](https://arxiv.org/pdf/2308.04948.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.04948)), ([:house:](https://huggingface.co/papers/2308.04948)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/extrapolating-large-language-models-to-non)) |
| 8.9 | ChatGPT answers more than half of software engineering questions incorrectly (ZDnet ([news](https://www.zdnet.com/article/chatgpt-answers-more-than-half-of-software-engineering-questions-incorrectly/)) |
| 8.9 | Releasing Claude Instant 1.2 ([Blog](https://www.anthropic.com/index/releasing-claude-instant-1-2)) |
| 8.9 | Shepherd: A Critic for Language Model Generation ([:x:](https://arxiv.org/abs/2308.04592)), ([:paperclip:](https://arxiv.org/pdf/2308.04592.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.04592)), ([:house:](https://huggingface.co/papers/2308.04592)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/shepherd-a-critic-for-language-model)) |
| 8.9 | JEN-1: Text-Guided Universal Music Generation with Omnidirectional Diffusion Models ([:x:](https://arxiv.org/abs/2308.04729)), ([:paperclip:](https://arxiv.org/pdf/2308.04729.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.04729)), ([:house:](https://huggingface.co/papers/2308.04729)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/jen-1-text-guided-universal-music-generation)) |
| 8.9 | Accelerating LLM Inference with Staged Speculative Decoding ([:x:](https://arxiv.org/abs/2308.04623)), ([:paperclip:](https://arxiv.org/pdf/2308.04623.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.04623)), ([:house:](https://huggingface.co/papers/2308.04623)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/accelerating-llm-inference-with-staged)) |
| 8.9 | Could a Large Language Model Be Conscious? ([news](https://www.bostonreview.net/articles/could-a-large-language-model-be-conscious/)) |
| 8.9 | 🚀Exciting news! Stability AI has launched StableCode, the revolutionary generative AI LLM for coding! ([tweet](https://twitter.com/StabilityAI/status/1688931312122675200)), ([blog](https://stability.ai/blog/stablecode-llm-generative-ai-coding?utm_source=twitter&utm_medium=website&utm_campaign=announcement)) |
| 8.9 | New research visualizes the political bias of all major AI language models ([tweet](https://twitter.com/AiBreakfast/status/1688939983468453888)) |
| 8.8 | Continual Pre-Training of Large Language Models: How to (re)warm your model? ([:x:](https://arxiv.org/abs/2308.15504)), ([:book:](https://browse.arxiv.org/pdf/2308.15504.pdf)), ([:paperclip:](https://arxiv.org/pdf/2308.15504.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.15504)), ([:house:](https://huggingface.co/papers/2308.15504)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/continual-pre-training-of-large-language)), ([:octocat:](https://github.com/eleutherai/gpt-neox)![GitHub Repo stars](https://img.shields.io/github/stars/eleutherai/gpt-neox?style=social))  |
| 8.8 | Gentopia: A Collaborative Platform for Tool-Augmented LLMs ([:x:](https://arxiv.org/abs/2308.04030)), ([:paperclip:](https://arxiv.org/pdf/2308.04030.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.04030)), ([:house:](https://huggingface.co/papers/2308.04030)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gentopia-a-collaborative-platform-for-tool)), ([:octocat:](https://github.com/gentopia-ai/gentopia)![GitHub Repo stars](https://img.shields.io/github/stars/gentopia-ai/gentopia?style=social)) |
| 8.8 | AgentSims: An Open-Source Sandbox for Large Language Model Evaluation ([:x:](https://arxiv.org/abs/2308.04026)), ([:paperclip:](https://arxiv.org/pdf/2308.04026.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.04026)), ([:house:](https://huggingface.co/papers/2308.04026)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/agentsims-an-open-source-sandbox-for-large)) |
| 8.8 | Empowering Vision-Language Models to Follow Interleaved Vision-Language Instructions ([:x:](https://arxiv.org/abs/2308.04152)), ([:paperclip:](https://arxiv.org/pdf/2308.04152.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.04152)), ([:house:](https://huggingface.co/papers/2308.04152)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/empowering-vision-language-models-to-follow)), ([:octocat:](https://github.com/dcdmllm/cheetah)![GitHub Repo stars](https://img.shields.io/github/stars/dcdmllm/cheetah?style=social)) |
| 8.8 | MedMine: Examining Pre-trained Language Models on Medication Mining ([:x:](https://arxiv.org/abs/2308.03629)), ([:paperclip:](https://arxiv.org/pdf/2308.03629.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.03629)), ([:house:](https://huggingface.co/papers/2308.03629)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/medmine-examining-pre-trained-language-models)) |
| 8.8 | Separate Anything You Describe ([:x:](https://arxiv.org/abs/2308.05037)), ([:paperclip:](https://arxiv.org/pdf/2308.05037.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.05037)), ([:house:](https://huggingface.co/papers/2308.05037)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/separate-anything-you-describe)), ([:octocat:](https://github.com/audio-agi/audiosep)![GitHub Repo stars](https://img.shields.io/github/stars/audio-agi/audiosep?style=social)) |
| 8.8 | AI regulation is taking shape, but startups are being left out (Verge [news](https://www.theverge.com/2023/8/8/23820423/ai-startups-regulation-big-tech)) |
| 8.8 | Accelerating LLM Inference with Staged Speculative Decoding ([:x:](https://arxiv.org/abs/2308.04623)), ([:paperclip:](https://arxiv.org/pdf/2308.04623.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.04623)), ([:house:](https://huggingface.co/papers/2308.04623)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/accelerating-llm-inference-with-staged)) |
| 8.8 | 3D Gaussian Splatting for Real-Time Radiance Field Rendering ([:x:](https://arxiv.org/abs/2308.04079)), ([:paperclip:](https://arxiv.org/pdf/2308.04079.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.04079)), ([:house:](https://huggingface.co/papers/2308.04079)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/3d-gaussian-splatting-for-real-time-radiance)) |
| 8.8 | OpenAI launches webcrawler [GPTBot](https://platform.openai.com/docs/gptbot), and instructions on how to block it (mashable [news](https://mashable.com/article/open-ai-gptbot-crawler-block)) |
| 8.8 | FLIRT: Feedback Loop In-context Red Teaming ([:x:](https://arxiv.org/abs/2308.04265)), ([:paperclip:](https://arxiv.org/pdf/2308.04265.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.04265)), ([:house:](https://huggingface.co/papers/2308.04265)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/flirt-feedback-loop-in-context-red-teaming)) |
| 8.8 | SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore ([:x:](https://arxiv.org/abs/2308.04430)), ([:paperclip:](https://arxiv.org/pdf/2308.04430.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.04430)), ([:house:](https://huggingface.co/papers/2308.04430)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/silo-language-models-isolating-legal-risk-in)) |
| 8.8 | Study Tests Large Language Models’ Ability to Answer Clinical Questions (JAMA. 2023;330(6):496. [doi:10.1001/jama.2023.12553](https://jamanetwork.com/journals/jama/fullarticle/2807649)) |
| 8.8 | Why Are So Many Organizations Banning ChatGPT? (BlackBerry [Blog](https://blogs.blackberry.com/en/2023/08/why-companies-ban-chatgpt-ai)) |
| 8.7 | Coupling Symbolic Reasoning with Language Modeling for Efficient Longitudinal Understanding of Unstructured Electronic Medical Records ([:x:](https://arxiv.org/abs/2308.03360)), ([:paperclip:](https://arxiv.org/pdf/2308.03360.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.03360)), ([:house:](https://huggingface.co/papers/2308.03360)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/coupling-symbolic-reasoning-with-language)) |
| 8.7 | Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue ([:x:](https://arxiv.org/abs/2308.03549)), ([:paperclip:](https://arxiv.org/pdf/2308.03549.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.03549)), ([:house:](https://huggingface.co/papers/2308.03549)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/zhongjing-enhancing-the-chinese-medical)), ([:octocat:](https://github.com/suprityoung/zhongjing)![GitHub Repo stars](https://img.shields.io/github/stars/suprityoung/zhongjing?style=social)) |
| 8.7 | Extracting detailed oncologic history and treatment plan from medical oncology notes with large language models ([:x:](https://arxiv.org/abs/2308.03853)), ([:paperclip:](https://arxiv.org/pdf/2308.03853.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.03853)), ([:house:](https://huggingface.co/papers/2308.03853)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/extracting-detailed-oncologic-history-and)) |
| 8.7 | UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition ([project](https://universal-ner.github.io/#)), ([:x:](https://arxiv.org/abs/2308.03279)), ([:paperclip:](https://arxiv.org/pdf/2308.03279.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.03279)), ([:house:](https://huggingface.co/papers/2308.03279)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/universalner-targeted-distillation-from-large)), ([:octocat:](https://github.com/universal-ner/universal-ner)![GitHub Repo stars](https://img.shields.io/github/stars/universal-ner/universal-ner?style=social)) |
| 8.7 | Studying Large Language Model Generalization with Influence Functions ([:x:](https://arxiv.org/abs/2308.03296)), ([:paperclip:](https://arxiv.org/pdf/2308.03296.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.03296)), ([:house:](https://huggingface.co/papers/2308.03296)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/studying-large-language-model-generalization)) |
| 8.7 | "Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models ([:x:](https://arxiv.org/abs/2308.03825)), ([:paperclip:](https://arxiv.org/pdf/2308.03825.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.03825)), ([:house:](https://huggingface.co/papers/2308.03825)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/do-anything-now-characterizing-and-evaluating)), ([:octocat:](https://github.com/verazuo/jailbreak_llms)![GitHub Repo stars](https://img.shields.io/github/stars/verazuo/jailbreak_llms?style=social)) |
| 8.7 | RecycleGPT: An Autoregressive Language Model with Recyclable Module ([:x:](https://arxiv.org/abs/2308.03421)), ([:paperclip:](https://arxiv.org/pdf/2308.03421.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.03421)), ([:house:](https://huggingface.co/papers/2308.03421)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/recyclegpt-an-autoregressive-language-model)) |
| 8.7 | AgentBench: Evaluating LLMs as Agents ([:x:](https://arxiv.org/abs/2308.03688)), ([:paperclip:](https://arxiv.org/pdf/2308.03688.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.03688)), ([:house:](https://huggingface.co/papers/2308.03688)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/agentbench-evaluating-llms-as-agents)), ([:octocat:](https://github.com/thudm/agentbench)![GitHub Repo stars](https://img.shields.io/github/stars/thudm/agentbench?style=social)) |
| 8.7 | Simple synthetic data reduces sycophancy in large language models  ([:x:](https://arxiv.org/abs/2308.03958)), ([:paperclip:](https://arxiv.org/pdf/2308.03958.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.03958)), ([:house:](https://huggingface.co/papers/2308.03958)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/simple-synthetic-data-reduces-sycophancy-in)), ([:octocat:](https://github.com/google/sycophancy-intervention)![GitHub Repo stars](https://img.shields.io/github/stars/google/sycophancy-intervention?style=social))  |
| 8.7 | Creation and Adoption of Large Language Models in Medicine (Jama [doi:10.1001/jama.2023.14217](https://jamanetwork.com/journals/jama/fullarticle/2808296)) |
| 8.7 | Doctors Vs. ChatGPT: Which Is More Empathetic? (Forbes [news](https://www.forbes.com/sites/robertpearl/2023/08/07/doctors-vs-chatgpt-which-is-more-empathetic)) |
| 8.7 | Criminals Have Created Their Own ChatGPT Clones (Wired [news](https://www.wired.com/story/chatgpt-scams-fraudgpt-wormgpt-crime/)) |
| 8.7 | Large Language Models Answer Medical Questions Accurately, but Can’t Match Clinicians’ Knowledge (Jama [doi:10.1001/jama.2023.14311](https://jamanetwork.com/journals/jama/fullarticle/2808297)) |
| 8.6 | Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology ([:x:](https://arxiv.org/abs/2308.02180)), ([:paperclip:](https://arxiv.org/pdf/2308.02180.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.02180)), ([:house:](https://huggingface.co/papers/2308.02180)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/scaling-clinical-trial-matching-using-large)) |
| 8.6 | Pre-Trained Large Language Models for Industrial Control ([:x:](https://arxiv.org/abs/2308.03028)), ([:paperclip:](https://arxiv.org/pdf/2308.03028.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.03028)), ([:house:](https://huggingface.co/papers/2308.03028)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pre-trained-large-language-models-for)) |
| 8.6 | Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies ([:x:](https://arxiv.org/abs/2308.03188)), ([:paperclip:](https://arxiv.org/pdf/2308.03188.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.03188)), ([:house:](https://huggingface.co/papers/2308.03188)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/automatically-correcting-large-language)), ([:octocat:](https://github.com/teacherpeterpan/self-correction-llm-papers)![GitHub Repo stars](https://img.shields.io/github/stars/teacherpeterpan/self-correction-llm-papers?style=social)) |
| 8.6 | A Simple AI Governance Framework In The Age Of ChatGPT (Forbes [news](https://www.forbes.com/sites/glenngow/2023/08/06/a-simple-ai-governance-framework-in-the-age-of-chatgpt)) |
| 8.5 | ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain Adaptation ([:x:](https://arxiv.org/abs/2308.03793)), ([:paperclip:](https://arxiv.org/pdf/2308.03793.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.03793)), ([:house:](https://huggingface.co/papers/2308.03793)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/reclip-refine-contrastive-language-image-pre)) |
| 8.4 | Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization ([:x:](https://arxiv.org/abs/2308.02151)), ([:paperclip:](https://arxiv.org/pdf/2308.02151.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.02151)), ([:house:](https://huggingface.co/papers/2308.02151)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/retroformer-retrospective-large-language)) |
| 8.4 | Who Answers It Better? An In-Depth Analysis of ChatGPT and Stack Overflow Answers to Software Engineering Questions ([:x:](https://arxiv.org/abs/2308.02312)), ([:paperclip:](https://arxiv.org/pdf/2308.02312.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.02312)), ([:house:](https://huggingface.co/papers/2308.02312)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/who-answers-it-better-an-in-depth-analysis-of)) |
| 8.4 | Towards Generalist Foundation Model for Radiology ([:x:](https://arxiv.org/abs/2308.02463)), ([:paperclip:](https://arxiv.org/pdf/2308.02463.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.02463)), ([:house:](https://huggingface.co/papers/2308.02463)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-generalist-foundation-model-for)), ([:octocat:](https://github.com/chaoyi-wu/radfm)![GitHub Repo stars](https://img.shields.io/github/stars/chaoyi-wu/radfm?style=social)) |
| 8.3 | Emergent Analogical Reasoning in Large Language Models ([:x:](https://arxiv.org/abs/2212.09196)), ([:paperclip:](https://arxiv.org/pdf/2212.09196.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2212.09196)), ([:house:](https://huggingface.co/papers/2212.09196)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/emergent-analogical-reasoning-in-large)), ([:octocat:](https://github.com/taylorwwebb/emergent_analogies_llm)![GitHub Repo stars](https://img.shields.io/github/stars/taylorwwebb/emergent_analogies_llm?style=social)) |
| 8.3 | The Capability of Large Language Models to Measure Psychiatric Functioning ([:x:](https://arxiv.org/abs/2308.01834)), ([:paperclip:](https://arxiv.org/pdf/2308.01834.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.01834)), ([:house:](https://huggingface.co/papers/2308.01834)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-capability-of-large-language-models-to)) |
| 8.3 | Local Large Language Models for Complex Structured Medical Tasks ([:x:](https://arxiv.org/abs/2308.01727)), ([:paperclip:](https://arxiv.org/pdf/2308.01727.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.01727)), ([:house:](https://huggingface.co/papers/2308.01727)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/local-large-language-models-for-complex)) |
| 8.3 | Huge set of ChatGPT updates ([tweet](https://twitter.com/OfficialLoganK/status/1687151401523089408)) |
| 8.3 | Accuracy of Vitreoretinal Disease Information From an Artificial Intelligence Chatbot (JAMA Ophthalmology [doi: 10.1001/jamaophthalmol.2023.3314](https://jamanetwork.com/journals/jamaophthalmology/article-abstract/2807968)) |
| 8.2 | XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models ([:x:](https://arxiv.org/abs/2308.01263)), ([:paperclip:](https://arxiv.org/pdf/2308.01263.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.01263)), ([:house:](https://huggingface.co/papers/2308.01263)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/xstest-a-test-suite-for-identifying)) |
| 8.2 | 4 Charts That Show Why AI Progress Is Unlikely to Slow Down (Time [news](https://time.com/6300942/ai-progress-charts/)) |
| 8.2 | Do Multilingual Language Models Think Better in English? ([:x:](https://arxiv.org/abs/2308.01223)), ([:paperclip:](https://arxiv.org/pdf/2308.01223.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.01223)), ([:house:](https://huggingface.co/papers/2308.01223)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/do-multilingual-language-models-think-better)), ([:octocat:](https://github.com/juletx/self-translate)![GitHub Repo stars](https://img.shields.io/github/stars/juletx/self-translate?style=social)) |
| 8.2 | Exploring the psychology of GPT-4's Moral and Legal Reasoning ([:x:](https://arxiv.org/abs/2308.01264)), ([:paperclip:](https://arxiv.org/pdf/2308.01264.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.01264)), ([:house:](https://huggingface.co/papers/2308.01264)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/exploring-the-psychology-of-gpt-4-s-moral-and)) |
| 8.2 | DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales ([:x:](https://arxiv.org/abs/2308.01320)), ([:paperclip:](https://arxiv.org/pdf/2308.01320.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.01320)), ([:house:](https://huggingface.co/papers/2308.01320)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/deepspeed-chat-easy-fast-and-affordable-rlhf)) |
| 8.2 | Flows: Building Blocks of Reasoning and Collaborating AI ([:x:](https://arxiv.org/abs/2308.01285)), ([:paperclip:](https://arxiv.org/pdf/2308.01285.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.01285)), ([:house:](https://huggingface.co/papers/2308.01285)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/flows-building-blocks-of-reasoning-and)) |
| 8.1 | MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework ([:x:](https://arxiv.org/abs/2308.00352)), ([:book:](https://browse.arxiv.org/pdf/2308.00352.pdf)), ([:paperclip:](https://arxiv.org/pdf/2308.00352.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.00352)), ([:house:](https://huggingface.co/papers/2308.00352)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/metagpt-meta-programming-for-multi-agent)), ([:octocat:](https://github.com/geekan/metagpt)![GitHub Repo stars](https://img.shields.io/github/stars/geekan/metagpt?style=social))  |
| 8.1 | Retrieval Augmented Generation and Representative Vector Summarization for large unstructured textual data in Medical Education ([:x:](https://arxiv.org/abs/2308.00479)), ([:paperclip:](https://arxiv.org/pdf/2308.00479.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.00479)), ([:house:](https://huggingface.co/papers/2308.00479)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/retrieval-augmented-generation-and)) |
| 8.1 | MetaGPT: Meta Programming for Multi-Agent Collaborative Framework ([:x:](https://arxiv.org/abs/2308.00352)), ([:paperclip:](https://arxiv.org/pdf/2308.00352.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.00352)), ([:house:](https://huggingface.co/papers/2308.00352)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/metagpt-meta-programming-for-multi-agent)), ([:octocat:](https://github.com/geekan/metagpt)![GitHub Repo stars](https://img.shields.io/github/stars/geekan/metagpt?style=social))  |
| 8.1 | Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models ([:x:](https://arxiv.org/abs/2308.00675)), ([:paperclip:](https://arxiv.org/pdf/2308.00675.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.00675)), ([:house:](https://huggingface.co/papers/2308.00675)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tool-documentation-enables-zero-shot-tool)) |
| 8.1 | Upstage LLM #1 in Open LLM Leaderboard ([Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)) |
| 8.1 | ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs ([:x:](https://arxiv.org/abs/2307.16789)), ([:paperclip:](https://arxiv.org/pdf/2307.16789.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.16789)), ([:house:](https://huggingface.co/papers/2307.16789)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/toolllm-facilitating-large-language-models-to)), ([:octocat:](https://github.com/openbmb/toolbench)![GitHub Repo stars](https://img.shields.io/github/stars/openbmb/toolbench?style=social)) |
| 8.1 | ChatGPT app for Android is now available in all countries and regions ([tweet](https://twitter.com/OpenAI/status/1686046214519947264)), ([blog](https://help.openai.com/en/articles/7947663-chatgpt-supported-countries)) |
| 7.31 | LLMs4OL: Large Language Models for Ontology Learning ([:x:](https://arxiv.org/abs/2307.16648)), ([:paperclip:](https://arxiv.org/pdf/2307.16648.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2378.16648)), ([:house:](https://huggingface.co/papers/2307.16648)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llms4ol-large-language-models-for-ontology)), ([:octocat:](https://github.com/hamedbabaei/llms4ol)![GitHub Repo stars](https://img.shields.io/github/stars/hamedbabaei/llms4ol?style=social)) |
| 7.31 | Plotting Progress in AI ([blog](https://contextual.ai/plotting-progress-in-ai/)) |
| 7.31 | Getting from Generative AI to Trustworthy AI: What LLMs might learn from Cyc ([:x:](https://arxiv.org/abs/2308.04445)), ([:paperclip:](https://arxiv.org/pdf/2308.04445.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.04445)), ([:house:](https://huggingface.co/papers/2308.04445)), ([:eight_spoked_asterisk:]()) |
| 7.31 | Learning to Model the World with Language ([:x:](https://arxiv.org/abs/2308.01399)), ([:paperclip:](https://arxiv.org/pdf/2308.01399.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2308.01399)), ([:house:](https://huggingface.co/papers/2308.01399)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/learning-to-model-the-world-with-language)) |
| 7.30 | Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models ([:x:](https://arxiv.org/abs/2307.16180)), ([:paperclip:](https://arxiv.org/pdf/2307.16180.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.16180)), ([:house:](https://huggingface.co/papers/2307.16180)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/do-llms-possess-a-personality-making-the-mbti)), ([:octocat:](https://github.com/harderthenharder/transformers_tasks)![GitHub Repo stars](https://img.shields.io/github/stars/harderthenharder/transformers_tasks?style=social)) |
| 7.30 | Unified Model for Image, Video, Audio and Language Tasks ([:x:](https://arxiv.org/abs/2307.16184)), ([:paperclip:](https://arxiv.org/pdf/2307.16184.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.16184)), ([:house:](https://huggingface.co/papers/2307.16184)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/unified-model-for-image-video-audio-and)) |
| 7.29 | The shaky foundations of large language models and foundation models for electronic health records (npj digital medicine, [https://doi.org/10.1038/s41746-023-00879-8](https://www.nature.com/articles/s41746-023-00879-8)), ([PDF](https://www.nature.com/articles/s41746-023-00879-8.pdf)) |
| 7.29 | Uncertainty in Natural Language Generation: From Theory to Applications ([:x:](https://arxiv.org/abs/2307.15703)), ([:paperclip:](https://arxiv.org/pdf/2307.15703.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.15703)), ([:house:](https://huggingface.co/papers/2307.15703)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/uncertainty-in-natural-language-generation)) |
| 7.28 | Exploring Format Consistency for Instruction Tuning ([:x:](https://arxiv.org/abs/2307.15504)), ([:paperclip:](https://arxiv.org/pdf/2307.15504.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.15504)), ([:house:](https://huggingface.co/papers/2307.15504)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/exploring-format-consistency-for-instruction)) |
| 7.28 | ⭐ Med-HALT: Medical Domain Hallucination Test for Large Language Models  ([:x:](https://arxiv.org/abs/2307.15343)), ([:paperclip:](https://arxiv.org/pdf/2307.15343.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.15343)), ([:house:](https://huggingface.co/papers/2307.15343)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/med-halt-medical-domain-hallucination-test)) |
| 7.28 | Med-Flamingo: a Multimodal Medical Few-shot Learner  ([:x:](https://arxiv.org/abs/2307.15189)), ([:paperclip:](https://arxiv.org/pdf/2307.15189.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.15189)), ([:house:](https://huggingface.co/papers/2307.15189)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/med-flamingo-a-multimodal-medical-few-shot)) |
| 7.28 | Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding ([:x:](https://arxiv.org/abs/2307.15337)), ([:paperclip:](https://arxiv.org/pdf/2307.15337.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.15337)), ([:house:](https://huggingface.co/papers/2307.15337)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/skeleton-of-thought-large-language-models-can)) |
| 7.28 | How Good is Google Bard's Visual Understanding? An Empirical Study on Open Challenges ([:x:](https://arxiv.org/abs/2307.15016)), ([:paperclip:](https://arxiv.org/pdf/2307.15016.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.15016)), ([:house:](https://huggingface.co/papers/2307.15016)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/how-good-is-google-bard-s-visual)) |
| 7.27 | Generative AI for Medical Imaging: extending the MONAI Framework ([:x:](https://arxiv.org/abs/2307.15208)), ([:paperclip:](https://arxiv.org/pdf/2307.15208.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.15208)), ([:house:](https://huggingface.co/papers/2307.15208)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/generative-ai-for-medical-imaging-extending)), ([:octocat:](https://github.com/warvito/generative_monai)![GitHub Repo stars](https://img.shields.io/github/stars/warvito/generative_monai?style=social))  |
| 7.27 | Guidance for Authors, Peer Reviewers, and Editors on Use of AI, Language Models, and Chatbots (Jama [doi:10.1001/jama.2023.12500](https://jamanetwork.com/journals/jama/fullarticle/2807956)) |
| 7.27 | Chatbots, Artificial Intelligence, and the Future of Scientific Reporting (JAMA Ophthalmology [doi: 10.1001/jamaophthalmol.2023.3344](https://jamanetwork.com/journals/jamaophthalmology/article-abstract/2807443)) |
| 7.27 | Matching Patients to Clinical Trials with Large Language Models ([:x:](https://arxiv.org/abs/2307.15051)), ([:paperclip:](https://arxiv.org/pdf/2307.15051.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.15051)), ([:house:](https://huggingface.co/papers/2307.15051)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/matching-patients-to-clinical-trials-with)) |
| 7.27 | Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback  ([:x:](https://arxiv.org/abs/2307.15217)), ([:paperclip:](https://arxiv.org/pdf/2307.15217.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.15217)), ([:house:](https://huggingface.co/papers/2307.15217)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/open-problems-and-fundamental-limitations-of)) |
| 7.27 | Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition ([:x:](https://arxiv.org/abs/2307.14535)), ([:paperclip:](https://arxiv.org/pdf/2307.14535.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.14535)), ([:house:](https://huggingface.co/papers/2307.14535)), ([:eight_spoked_asterisk:]()) |
| 7.27 | NeurIPS 2023 Large Language Model Efficiency Challenge: 1 LLM + 1GPU + 1Day ([site](https://llm-efficiency-challenge.github.io/)) |
| 7.27 | Google DeepMind RT-2: Vision-Language-Action Models ([tweet](https://twitter.com/GoogleDeepMind/status/1684903412834447360)), ([blog](https://www.deepmind.com/blog/rt-2-new-model-translates-vision-and-language-into-action)), ([project](https://robotics-transformer2.github.io/)), ([PDF](https://robotics-transformer2.github.io/assets/rt2.pdf)) |
| 7.27 | Multilingual Code Co-Evolution Using Large Language Models ([:x:](https://arxiv.org/abs/2307.14991)), ([:paperclip:](https://arxiv.org/pdf/2307.14991.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.14991)), ([:house:](https://huggingface.co/papers/2307.14991)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/multilingual-code-co-evolution-using-large)) |
| 7.27 | The Guardian's updated editorial code guidance now includes a section on generative AI ([PDF](https://uploads.guim.co.uk/2023/07/27/GNM_editorial_code_of_practice_and_guidance_2023.pdf)) |
| 7.27 | Training Data Extraction From Pre-trained Language Models: A Survey ([report](https://aclanthology.org/2023.trustnlp-1.23/)), ([PDF](https://aclanthology.org/2023.trustnlp-1.23.pdf)) |
| 7.27 | ⭐ Universal and Transferable Adversarial Attacks on Aligned Language Models ([project](https://llm-attacks.org/)), ([:x:](https://arxiv.org/abs/2307.15043)), ([:paperclip:](https://arxiv.org/pdf/2307.15043.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.15043)), ([:house:](https://huggingface.co/papers/2307.15043)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/universal-and-transferable-adversarial)), ([:octocat:](https://github.com/llm-attacks/llm-attacks)![GitHub Repo stars](https://img.shields.io/github/stars/llm-attacks/llm-attacks?style=social)), ([SS](https://www.semanticscholar.org/paper/Universal-and-Transferable-Adversarial-Attacks-on-Zou-Wang/47030369e97cc44d4b2e3cf1be85da0fd134904a)) |
| 7.27 | NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection ([:x:](https://arxiv.org/abs/2307.14620)), ([:paperclip:](https://arxiv.org/pdf/2307.14620.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.14620)), ([:house:](https://huggingface.co/papers/2307.14620)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/nerf-det-learning-geometry-aware-volumetric)) |
| 7.27 | PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback ([:x:](https://arxiv.org/abs/2307.14936)), ([:paperclip:](https://arxiv.org/pdf/2307.14936.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.14936)), ([:house:](https://huggingface.co/papers/2307.14936)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pangu-coder2-boosting-large-language-models)) |
| 7.27 | WavJourney: Compositional Audio Creation with Large Language Models ([:x:](https://arxiv.org/abs/2307.14335)), ([:paperclip:](https://arxiv.org/pdf/2307.14335.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.14335)), ([:house:](https://huggingface.co/papers/2307.14335)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/wavjourney-compositional-audio-creation-with)), ([:octocat:](https://github.com/audio-agi/wavjourney)![GitHub Repo stars](https://img.shields.io/github/stars/audio-agi/wavjourney?style=social)) |
| 7.26 | Supporting Open Source and Open Science in the EU AI Act ([Blog](https://creativecommons.org/2023/07/26/supporting-open-source-and-open-science-in-the-eu-ai-act/)), ([PDF](https://creativecommons.org/wp-content/uploads/2023/07/SupportingOpenSourceAndOpenScienceInTheEUAIAct.pdf)) |
| 7.26 | Points-to-3D: Bridging the Gap between Sparse Points and Shape-Controllable Text-to-3D Generation ([:x:](https://arxiv.org/abs/2307.13908)), ([:paperclip:](https://arxiv.org/pdf/2307.13908.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.13908)), ([:house:](https://huggingface.co/papers/2307.13908)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/points-to-3d-bridging-the-gap-between-sparse)) |
| 7.26 | Tracking Anything in High Quality ([:x:](https://arxiv.org/abs/2307.13974)), ([:paperclip:](https://arxiv.org/pdf/2307.13974.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.13974)), ([:house:](https://huggingface.co/papers/2307.13974)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tracking-anything-in-high-quality)), ([:octocat:](https://github.com/jiawen-zhu/hqtrack)![GitHub Repo stars](https://img.shields.io/github/stars/jiawen-zhu/hqtrack?style=social)) |
| 7.26 | Stability AI Announces Stable Diffusion XL 1.0, Featured on Amazon Bedrock ([blog](https://stability.ai/press-articles/stable-diffusion-xl-1-featured-amazon-aws-bedrock)), ([SD-XL 1.0-base Model Card](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)), ([SD-XL 1.0-refiner Model Card](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)), ([:octocat:](https://github.com/Stability-AI/generative-models)![GitHub Repo stars](https://img.shields.io/github/stars/Stability-AI/generative-models?style=social)) |
| 7.26 | Towards Generalist Biomedical AI  ([:x:](https://arxiv.org/abs/2307.14334)), ([:paperclip:](https://arxiv.org/pdf/2307.14334.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.14334)), ([:house:](https://huggingface.co/papers/2307.14334)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-generalist-biomedical-ai)) |
| 7.26 | ⭐ Microsoft, Anthropic, Google, and OpenAI launch Frontier Model Forum ([Microsoft](https://blogs.microsoft.com/on-the-issues/2023/07/26/anthropic-google-microsoft-openai-launch-frontier-model-forum/?fbclid=IwAR3-9xKx9lLczutZ5q7ZDyXjGXYfLqM8FvZ_7saCI-EqLIl97w--ETcpKV8)), [Google](https://blog.google/outreach-initiatives/public-policy/google-microsoft-openai-anthropic-frontier-model-forum/?fbclid=IwAR3lv1xCrjWbkyetFYTdYQuZEbJ0yfqGWybIuY5v9rN0EW2cPC7jj6WrhKg), [OpenAI](https://openai.com/blog/frontier-model-forum?fbclid=IwAR3PMnEemOsvJm2loxHbI90ZO5R-_GdKVJ0lS8nEuOFCSf5cVeip-EpNI-4), [anthropic](https://www.anthropic.com/index/frontier-threats-red-teaming-for-ai-safety)) | 
| 7.26 | Evaluating the Moral Beliefs Encoded in LLMs ([:x:](https://arxiv.org/abs/2307.14324)), ([:paperclip:](https://arxiv.org/pdf/2307.14324.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.14324)), ([:house:](https://huggingface.co/papers/2307.14324)), ([:eight_spoked_asterisk:]()) |
| 7.26 | WebArena: A Realistic Web Environment for Building Autonomous Agents ([project](https://webarena.dev/)),  ([:paperclip:](https://webarena.dev/static/paper.pdf)), ([:octocat:](https://github.com/web-arena-x/webarena)![GitHub Repo stars](https://img.shields.io/github/stars/web-arena-x/webarena?style=social) |
| 7.26 | ARB: Advanced Reasoning Benchmark for Large Language Models  ([:x:](https://arxiv.org/abs/2307.13692)), ([:paperclip:](https://arxiv.org/pdf/2307.13692.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.13692)), ([:house:](https://huggingface.co/papers/2307.13692)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/arb-advanced-reasoning-benchmark-for-large)) |
| 7.26 | OpenAI scuttles AI-written text detector over ‘low rate of accuracy’ ([news](https://techcrunch.com/2023/07/25/openai-scuttles-ai-written-text-detector-over-low-rate-of-accuracy/)) |
| 7.25 | Foundational Models Defining a New Era in Vision: A Survey and Outlook ([:x:](https://arxiv.org/abs/2307.13721)), ([:paperclip:](https://arxiv.org/pdf/2307.13721.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.13721)), ([:house:](https://huggingface.co/papers/2307.13721)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/foundational-models-defining-a-new-era-in)), ([:octocat:](https://github.com/awaisrauf/awesome-cv-foundational-models)![GitHub Repo stars](https://img.shields.io/github/stars/awaisrauf/awesome-cv-foundational-models?style=social)) |
| 7.25 | Evaluating Large Language Models for Radiology Natural Language Processing ([:x:](https://arxiv.org/abs/2307.13693)), ([:paperclip:](https://arxiv.org/pdf/2307.13693.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.13693)), ([:house:](https://huggingface.co/papers/2307.13693)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evaluating-large-language-models-for)) |
| 7.25 | LLM-Rec: Personalized Recommendation via Prompting Large Language Models ([:x:](https://arxiv.org/abs/2307.15780)), ([:paperclip:](https://arxiv.org/pdf/2307.15780.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.15780)), ([:house:](https://huggingface.co/papers/2307.15780)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llm-rec-personalized-recommendation-via)) |
| 7.25 | How Can Large Language Models Help Humans in Design and Manufacturing? ([:x:](https://arxiv.org/abs/2307.14377)), ([:paperclip:](https://arxiv.org/pdf/2307.14377.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.14377)), ([:house:](https://huggingface.co/papers/2307.14377)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/how-can-large-language-models-help-humans-in)) |
| 7.25 | UK House of Lords Announces Inquiry into Large Language Models ([news](https://www.lexology.com/library/detail.aspx?g=ae522c40-cdb5-48ca-aa5d-58c6f5b32c1f)) |
| 7.25 | FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios ([:x:](https://arxiv.org/abs/2307.13528)), ([:paperclip:](https://arxiv.org/pdf/2307.13528.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.13528)), ([:house:](https://huggingface.co/papers/2307.13528)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/factool-factuality-detection-in-generative-ai)), ([:octocat:](https://github.com/gair-nlp/factool)![GitHub Repo stars](https://img.shields.io/github/stars/gair-nlp/factool?style=social)) |
| 7.25 | LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition ([:x:](https://arxiv.org/abs/2307.13269)), ([:paperclip:](https://arxiv.org/pdf/2307.13269.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.13269)), ([:house:](https://huggingface.co/papers/2307.13269)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lorahub-efficient-cross-task-generalization)) |
| 7.25 | ChatGPT is a black box: how AI research can break it open (Nature [doi: https://doi.org/10.1038/d41586-023-02366-2](https://www.nature.com/articles/d41586-023-02366-2)) |
| 7.25 | ChatGPT broke the Turing test — the race is on for new ways to assess AI (Nature [doi: https://doi.org/10.1038/d41586-023-02361-7](https://www.nature.com/articles/d41586-023-02361-7)), ([PDF](https://www.nature.com/articles/d41586-023-02361-7.pdf?pdf=button%20sticky)) |
| 7.25 | Evaluating the Ripple Effects of Knowledge Editing in Language Models ([:x:](https://arxiv.org/abs/2307.12976)), ([:paperclip:](https://arxiv.org/pdf/2307.12976.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.12976)), ([:house:](https://huggingface.co/papers/2307.12976)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evaluating-the-ripple-effects-of-knowledge)) |
| 7.25 | 3D-LLM: Injecting the 3D World into Large Language Models ([:x:](https://arxiv.org/abs/2307.12981)), ([:paperclip:](https://arxiv.org/pdf/2307.12981.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.12981)), ([:house:](https://huggingface.co/papers/2307.12981)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/3d-llm-injecting-the-3d-world-into-large)) |
| 7.25 | RLCD: Reinforcement Learning from Contrast Distillation for Language Model Alignment ([:x:](https://arxiv.org/abs/2307.12950)), ([:paperclip:](https://arxiv.org/pdf/2307.12950.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.12950)), ([:house:](https://huggingface.co/papers/2307.12950)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rlcd-reinforcement-learning-from-contrast)) |
| 7.24 | A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models ([:x:](https://arxiv.org/abs/2307.12980)), ([:paperclip:](https://arxiv.org/pdf/2307.12980.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.12980)), ([:house:](https://huggingface.co/papers/2307.12980)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-systematic-survey-of-prompt-engineering-on)), ([:octocat:](https://github.com/JindongGu/Awesome-Prompting-on-Vision-Language-Model)![GitHub Repo stars](https://img.shields.io/github/stars/JindongGu/Awesome-Prompting-on-Vision-Language-Model?style=social))  |
| 7.24 | ⭐ Aligning Large Language Models with Human: A Survey  ([:x:](https://arxiv.org/abs/2307.12966)), ([:paperclip:](https://arxiv.org/pdf/2307.12966.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.12966)), ([:house:](https://huggingface.co/papers/2307.12966)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/aligning-large-language-models-with-human-a)), ([:octocat:](https://github.com/garyyufei/alignllmhumansurvey)![GitHub Repo stars](https://img.shields.io/github/stars/garyyufei/alignllmhumansurvey?style=social)) |
| 7.24 | A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis  ([:x:](https://arxiv.org/abs/2307.12856)), ([:paperclip:](https://arxiv.org/pdf/2307.12856.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.12856)), ([:house:](https://huggingface.co/papers/2307.12856)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-real-world-webagent-with-planning-long)) |
| 7.24 | LLMs get a medical education (Nature [DOI: 10.1038/d41591-023-00064-0](https://www.nature.com/articles/d41591-023-00064-0)) |
| 7.24 | MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features ([:x:](https://arxiv.org/abs/2307.12698)), ([:paperclip:](https://arxiv.org/pdf/2307.12698.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.12698)), ([:house:](https://huggingface.co/papers/2307.12698)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mc-jepa-a-joint-embedding-predictive)) |
| 7.24 | Interpolating between Images with Diffusion Models ([:x:](https://arxiv.org/abs/2307.12560)), ([:paperclip:](https://arxiv.org/pdf/2307.12560.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.12560)), ([:house:](https://huggingface.co/papers/2307.12560)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/interpolating-between-images-with-diffusion))  |
| 7.24 | PUMA: Secure Inference of LLaMA-7B in Five Minutes  ([:x:](https://arxiv.org/abs/2307.12533)), ([:paperclip:](https://arxiv.org/pdf/2307.12533.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.12533)), ([:house:](https://huggingface.co/papers/2307.12533)), ([:eight_spoked_asterisk:](https://cs.paperswithcode.com/paper/puma-secure-inference-of-llama-7b-in-five)), ([:octocat:](https://github.com/secretflow/spu)![GitHub Repo stars](https://img.shields.io/github/stars/secretflow/spu?style=social))  |
| 7.24 | A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis ([:x:](https://arxiv.org/abs/2307.12856)), ([:paperclip:](https://arxiv.org/pdf/2307.12856.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.12856)), ([:house:](https://huggingface.co/papers/2307.12856)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-real-world-webagent-with-planning-long)) |
| 7.23 | GitHub repo for Generative Agents: Interactive Simulacra of Human Behavior ([:octocat:](https://github.com/joonspk-research/generative_agents)![GitHub Repo stars](https://img.shields.io/github/stars/joonspk-research/generative_agents?style=social)) |
| 7.23 | Optimized Network Architectures for Large Language Model Training with Billions of Parameters ([:x:](https://arxiv.org/abs/2307.12169)), ([:paperclip:](https://arxiv.org/pdf/2307.12169.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.12169)), ([:house:](https://huggingface.co/papers/2307.12169)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/optimized-network-architectures-for-large)) |
| 7.22 | A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks ([:x:](https://arxiv.org/abs/2307.12114)), ([:paperclip:](https://arxiv.org/pdf/2307.12114.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.12114)), ([:house:](https://huggingface.co/papers/2307.12114)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-zero-shot-and-few-shot-study-of-instruction)) |
| 7.22 | Introducing FreeWilly1 and FreeWilly2 - The latest groundbreaking LLMs from Stability AI's and  @carperai lab! ⭐ ([tweet](https://twitter.com/StabilityAI/status/1682474968393609216)) |
| 7.22 | llama2-webui: Run Llama 2 locally with gradio UI on GPU or CPU from anywhere (Linux/Windows/Mac) ([:octocat:](https://github.com/liltom-eth/llama2-webui)![GitHub Repo stars](https://img.shields.io/github/stars/liltom-eth/llama2-webui?style=social)) |
| 7.22 | ChatGPT for Android launches next week ([news](https://www.theverge.com/2023/7/21/23803482/chatgpt-android-artificial-intelligence-chatbot-app)) |
| 7.22 | Expedia launches ChatGPT travel planning tool ([news](https://globetrender.com/2023/07/22/expedia-launches-chatgpt-travel-planning-tool/)) |
| 7.21 | CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study ([:x:](https://arxiv.org/abs/2307.11346)), ([:paperclip:](https://arxiv.org/pdf/2307.11346.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.11346)), ([:house:](https://huggingface.co/papers/2307.11346)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/cohortgpt-an-enhanced-gpt-for-participant)) |
| 7.21 | FACT SHEET: Biden-⁠Harris Administration Secures Voluntary Commitments from Leading Artificial Intelligence Companies to Manage the Risks Posed by AI (White House [news](https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/)) |
| 7.21 | Prompting Large Language Models with Speech Recognition Abilities  ([:x:](https://arxiv.org/abs/2307.11795)), ([:paperclip:](https://arxiv.org/pdf/2307.11795.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.11795)), ([:house:](https://huggingface.co/papers/2307.11795)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/prompting-large-language-models-with-speech)) |
| 7.21 | L-Eval: Instituting Standardized Evaluation for Long Context Language Models  ([:x:](https://arxiv.org/abs/2307.11088)), ([:paperclip:](https://arxiv.org/pdf/2307.11088.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.11088)), ([:house:](https://huggingface.co/papers/2307.11088)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evaluating-superhuman-models-with-consistency)) |
| 7.21 | CopyRNeRF: Protecting the CopyRight of Neural Radiance Fields ([:x:](https://arxiv.org/abs/2307.11526)), ([:paperclip:](https://arxiv.org/pdf/2307.11526.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.11526)), ([:house:](https://huggingface.co/papers/2307.11526)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/copyrnerf-protecting-the-copyright-of-neural)) |
| 7.21 | FaceCLIPNeRF: Text-driven 3D Face Manipulation using Deformable Neural Radiance Fields ([:x:](https://arxiv.org/abs/2307.11418)), ([:paperclip:](https://arxiv.org/pdf/2307.11418.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.11418)), ([:house:](https://huggingface.co/papers/2307.11418)), ([:eight_spoked_asterisk:]()) |
| 7.21 | Subject-Diffusion:Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning ([:x:](https://arxiv.org/abs/2307.11410)), ([:paperclip:](https://arxiv.org/pdf/2307.11410.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.11410)), ([:house:](https://huggingface.co/papers/2307.11410)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/subject-diffusion-open-domain-personalized)), ([:octocat:](https://github.com/OPPO-Mente-Lab/Subject-Diffusion)![GitHub Repo stars](https://img.shields.io/github/stars/OPPO-Mente-Lab/Subject-Diffusion?style=social)) |
| 7.21 | Meet FreeWilly, Our Large And Mighty Instruction Fine-Tuned Models (stability.ai [announcement](https://stability.ai/blog/freewilly-large-instruction-fine-tuned-models)) |
| 7.21 | WormGPT: ChatGPT For Cybercriminals ([news](https://medium.datadriveninvestor.com/wormgpt-chatgpt-for-cybercriminals-db81d2b0a1fc)) |
| 7.21 | Brain2Music: Reconstructing Music from Human Brain Activity ([:x:](https://arxiv.org/abs/2307.11078)), ([:paperclip:](https://arxiv.org/pdf/2307.11078.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.11078)), ([:house:](https://huggingface.co/papers/2307.11078)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/brain2music-reconstructing-music-from-human)) |
| 7.21 | OpenAI launches customized instructions for ChatGPT ([news](https://techcrunch.com/2023/07/20/openai-launches-customized-instructions-for-chatgpt/)) |
| 7.20 | L-Eval: Instituting Standardized Evaluation for Long Context Language Models  ([:x:](https://arxiv.org/abs/2307.11088)), ([:paperclip:](https://arxiv.org/pdf/2307.11088.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.11088)), ([:house:](https://huggingface.co/papers/2307.11088)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/l-eval-instituting-standardized-evaluation)), ([:octocat:](https://github.com/openlmlab/leval)![GitHub Repo stars](https://img.shields.io/github/stars/openlmlab/leval?style=social)) |
| 7.20 | DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-centric Rendering ([:x:](https://arxiv.org/abs/2307.10173)), ([:paperclip:](https://arxiv.org/pdf/2307.10173.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.10173)), ([:house:](https://huggingface.co/papers/2307.10173)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dna-rendering-a-diverse-neural-actor)), ([:octocat:](https://github.com/DNA-Rendering/DNA-Rendering)![GitHub Repo stars](https://img.shields.io/github/stars/DNA-Rendering/DNA-Rendering?style=social))  |
| 7.20 | LLMs as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with LLMs ([:x:](https://arxiv.org/abs/2307.10168)), ([:paperclip:](https://arxiv.org/pdf/2307.10168.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.10168)), ([:house:](https://huggingface.co/papers/2307.10168)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llms-as-workers-in-human-computational)) |
| 7.20 | DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI ([:x:](https://arxiv.org/abs/2307.10172)), ([:paperclip:](https://arxiv.org/pdf/2307.10172.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.10172)), ([:house:](https://huggingface.co/papers/2307.10172)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dialogstudio-towards-richest-and-most-diverse)), ([:octocat:](https://github.com/salesforce/DialogStudio)![GitHub Repo stars](https://img.shields.io/github/stars/salesforce/DialogStudio?style=social)) |
| 7.20 | FABRIC: Personalizing Diffusion Models with Iterative Feedback  ([:x:](https://arxiv.org/abs/2307.10159)), ([:paperclip:](https://arxiv.org/pdf/2307.10159.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.10159)), ([:house:](https://huggingface.co/papers/2307.10159)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fabric-personalizing-diffusion-models-with)), ([:octocat:](https://github.com/sd-fabric/fabric)![GitHub Repo stars](https://img.shields.io/github/stars/sd-fabric/fabric?style=social)) |
| 7.20 | ⭐ FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets ([:x:](https://arxiv.org/abs/2307.10928)), ([:paperclip:](https://arxiv.org/pdf/2307.10928.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.10928)), ([:house:](https://huggingface.co/papers/2307.10928)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/flask-fine-grained-language-model-evaluation)) |
| 7.20 | Instruction-following Evaluation through Verbalizer Manipulation ([:x:](https://arxiv.org/abs/2307.10558)), ([:paperclip:](https://arxiv.org/pdf/2307.10558.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.10558)), ([:house:](https://huggingface.co/papers/2307.10558)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/instruction-following-evaluation-through)) |
| 7.20 | PASTA: Pretrained Action-State Transformer Agents ([:x:](https://arxiv.org/abs/2307.10936)), ([:paperclip:](https://arxiv.org/pdf/2307.10936.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.10936)), ([:house:](https://huggingface.co/papers/2307.10936)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pasta-pretrained-action-state-transformer)) |
| 7.20 | TokenFlow: Consistent Diffusion Features for Consistent Video Editing ([:x:](https://arxiv.org/abs/2307.10373)), ([:paperclip:](https://arxiv.org/pdf/2307.10373.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.10373)), ([:house:](https://huggingface.co/papers/2307.10373)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tokenflow-consistent-diffusion-features-for)) |
| 7.20 | ⭐ SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models ([:x:](https://arxiv.org/abs/2307.10635)), ([:paperclip:](https://arxiv.org/pdf/2307.10635.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.10635)), ([:house:](https://huggingface.co/papers/2307.10635)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/scibench-evaluating-college-level-scientific)), ([:octocat:](https://github.com/mandyyyyii/scibench)![GitHub Repo stars](https://img.shields.io/github/stars/mandyyyyii/scibench?style=social)) |
| 7.20 | Artificial intelligence is making the union movement’s case–and even ChatGPT knows it ([news](https://fortune.com/2023/07/20/artificial-intelligence-making-union-movements-caseand-even-chatgpt-labor-strikes-edward-smith/)) |
| 7.20 | Apple is testing a ChatGPT-like AI chatbot ([news](https://techcrunch.com/2023/07/19/apple-is-testing-chatgpt-like-ai-chatbot/)) |
| 7.20 | Someone Used ChatGPT to Finish the Game of Thrones Book Series ([news](https://www.ign.com/articles/someone-used-chatgpt-to-finish-the-game-of-thrones-book-series)) |
| 7.20 | ⭐ Meta-Transformer: A Unified Framework for Multimodal Learning ([:x:](https://arxiv.org/abs/2307.10802)), ([:paperclip:](https://arxiv.org/pdf/2307.10802.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.10802)), ([:house:](https://huggingface.co/papers/2307.10802)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/meta-transformer-a-unified-framework-for)), ([:octocat:](https://github.com/invictus717/MetaTransformer)![GitHub Repo stars](https://img.shields.io/github/stars/invictus717/MetaTransformer?style=social)) |
| 7.19 | PharmacyGPT: The AI Pharmacist ([:x:](https://arxiv.org/abs/2307.10432)), ([:paperclip:](https://arxiv.org/pdf/2307.10432.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.10432)), ([:house:](https://huggingface.co/papers/2307.10432)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pharmacygpt-the-ai-pharmacist)) |
| 7.19 | IvyGPT: InteractiVe Chinese pathwaY language model in medical domain ([:x:](https://arxiv.org/abs/2307.10512)), ([:paperclip:](https://arxiv.org/pdf/2307.10512.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.10512)), ([:house:](https://huggingface.co/papers/2307.10512)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ivygpt-interactive-chinese-pathway-language)) |
| 7.19 | Study Tests Large Language Models’ Ability to Answer Clinical Questions (Jama [doi: 10.1001/jama.2023.12553](https://jamanetwork.com/journals/jama/fullarticle/2807649)) |
| 7.19 | (Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs ([:x:](https://arxiv.org/abs/2307.10490)), ([:paperclip:](https://arxiv.org/pdf/2307.10490.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.10490)), ([:house:](https://huggingface.co/papers/2307.10490)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ab-using-images-and-sounds-for-indirect)), ([:octocat:](https://github.com/ebagdasa/multimodal_injection)![GitHub Repo stars](https://img.shields.io/github/stars/ebagdasa/multimodal_injection?style=social)) |
| 7.19 | Text2Layer: Layered Image Generation using Latent Diffusion Model ([:x:](https://arxiv.org/abs/2307.09781)), ([:paperclip:](https://arxiv.org/pdf/2307.09781.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.09781)), ([:house:](https://huggingface.co/papers/2307.09781)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/text2layer-layered-image-generation-using)) |
| 7.19 | Towards A Unified Agent with Foundation Models ([:x:](https://arxiv.org/abs/2307.09668)), ([:paperclip:](https://arxiv.org/pdf/2307.09668.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.09668)), ([:house:](https://huggingface.co/papers/2307.09668)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-a-unified-agent-with-foundation)) |
| 7.19 | ⭐ Challenges and Applications of Large Language Models ([:x:](https://arxiv.org/abs/2307.10169)), ([:paperclip:](https://arxiv.org/pdf/2307.10169.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.10169)), ([:house:](https://huggingface.co/papers/2307.10169)), ([:eight_spoked_asterisk:]()) |
| 7.19 | On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models ([:x:](https://arxiv.org/abs/2307.09793)), ([:paperclip:](https://arxiv.org/pdf/2307.09793.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.09793)), ([:house:](https://huggingface.co/papers/2307.09793)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/on-the-origin-of-llms-an-evolutionary-tree)), ([Constellation](https://constellation.sites.stanford.edu/)) |
| 7.18 | AnyDoor: Zero-shot Object-level Image Customization ([project](https://damo-vilab.github.io/AnyDoor-Page/)), ([:x:](https://arxiv.org/abs/2307.09481)), ([:book:](https://browse.arxiv.org/pdf/2307.09481.pdf)), ([:paperclip:](https://arxiv.org/pdf/2307.09481.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.09481)), ([:house:](https://huggingface.co/papers/2307.09481)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/anydoor-zero-shot-object-level-image)), ([:octocat:](https://github.com/modelscope/modelscope)![GitHub Repo stars](https://img.shields.io/github/stars/modelscope/modelscope?style=social)), ([demo](https://huggingface.co/spaces/xichenhku/AnyDoor-online))  |
| 7.18 | Augmenting CLIP with Improved Visio-Linguistic Reasoning  ([:x:](https://arxiv.org/abs/2307.09233)), ([:paperclip:](https://arxiv.org/pdf/2307.09233.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.09233)), ([:house:](https://huggingface.co/papers/2307.09233)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/augmenting-clip-with-improved-visio)) |
| 7.18 | How generative AI will reshape the enterprise ([report](https://www.databricks.com/resources/ebook/mit-cio-generative-ai-report?utm_source=mit&utm_medium=press-release&utm_campaign=7018Y000001FhXLQA0)) |
| 7.18 | How is ChatGPT's behavior changing over time? ([:x:](https://arxiv.org/abs/2307.09009)), ([:paperclip:](https://arxiv.org/pdf/2307.09009.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.09009)), ([:house:](https://huggingface.co/papers/2307.09009)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/how-is-chatgpt-s-behavior-changing-over-time)), ([:octocat:](https://github.com/lchen001/llmdrift)![GitHub Repo stars](https://img.shields.io/github/stars/lchen001/llmdrift?style=social)) |
| 7.18 | NU-MCC: Multiview Compressive Coding with Neighborhood Decoder and Repulsive UDF ([:x:](https://arxiv.org/abs/2307.09112)), ([:paperclip:](https://arxiv.org/pdf/2307.09112.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.09112)), ([:house:](https://huggingface.co/papers/2307.09112)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/nu-mcc-multiview-compressive-coding-with)) |
| 7.18 | Measuring Faithfulness in Chain-of-Thought Reasoning ([PDF](https://www-files.anthropic.com/production/files/measuring-faithfulness-in-chain-of-thought-reasoning.pdf)) |
| 7.18 | 🦙 Llama 2 and Claude 2 are now live on Chatbot Arena! ([arena](https://huggingface.co/spaces/lmsys/Chat-and-Battle-with-Open-LLMs)) |
| 7.18 | Statement of Support for Meta’s Open Approach to Today’s AI ([blog](https://about.fb.com/news/2023/07/llama-2-statement-of-support/)) |
| 7.18 | Llama 2: Open Foundation and Fine-Tuned Chat Models ([paper](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)), ([PDF](https://scontent-ssn1-1.xx.fbcdn.net/v/t39.2365-6/10000000_663429262362723_1696968207443577320_n.pdf?_nc_cat=101&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=5ol-jUSglG4AX-46yHU&_nc_ht=scontent-ssn1-1.xx&oh=00_AfCR0oNWkc6kxTrD1j6ODAZq38M7xzwDzu8Fz-Z_h-v1kg&oe=64BBB691)), ([:octocat:](https://github.com/facebookresearch/llama)![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/llama?style=social)) |
| 7.18 | Meta and Microsoft Introduce the Next Generation of Llama ([tweet](https://twitter.com/ylecun/status/1681336284453781505)), ([news](https://about.fb.com/news/2023/07/llama-2/)), ([Llama2](https://ai.meta.com/llama/)), ([download](https://ai.meta.com/llama/#download-the-model)) |
| 7.18 | Retentive Network: A Successor to Transformer for Large Language Models ([:x:](https://arxiv.org/abs/2307.08621)), ([:paperclip:](https://arxiv.org/pdf/2307.08621.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.08621)), ([:house:](https://huggingface.co/papers/2307.08621)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/retentive-network-a-successor-to-transformer)), ([:octocat:](https://github.com/microsoft/unilm)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/unilm?style=social))  |
| 7.18 | Diffusion Models Beat GANs on Image Classification ([:x:](https://arxiv.org/abs/2307.08702)), ([:paperclip:](https://arxiv.org/pdf/2307.08702.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.08702)), ([:house:](https://huggingface.co/papers/2307.08702)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/diffusion-models-beat-gans-on-image)) |
| 7.18 | BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs ([:x:](https://arxiv.org/abs/2307.08581)), ([:paperclip:](https://arxiv.org/pdf/2307.08581.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.08581)), ([:house:](https://huggingface.co/papers/2307.08581)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/bubogpt-enabling-visual-grounding-in-multi)) |
| 7.18 | TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT ([:x:](https://arxiv.org/abs/2307.08674)), ([:paperclip:](https://arxiv.org/pdf/2307.08674.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.08674)), ([:house:](https://huggingface.co/papers/2307.08674)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tablegpt-towards-unifying-tables-nature)) |
| 7.17 | Abductive Reasoning with the GPT-4 Language Model: Case studies from criminal investigation, medical practice, scientific research ([:x:](https://arxiv.org/abs/2307.10250)), ([:paperclip:](https://arxiv.org/pdf/2307.10250.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.10250)), ([:house:](https://huggingface.co/papers/2307.10250)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/abductive-reasoning-with-the-gpt-4-language)) |
| 7.17 | Performance of a Large Language Model on Practice Questions for the Neonatal Board Examination (Jama [doi: 10.1001/jamapediatrics.2023.2373](https://jamanetwork.com/journals/jamapediatrics/fullarticle/2807329)) |
| 7.17 | Large language models in medicine (nature medicine [https://doi.org/10.1038/s41591-023-02448-8](https://www.nature.com/articles/s41591-023-02448-8)) |
| 7.17 | Chatbot vs Medical Student Performance on Free-Response Clinical Reasoning Examinations (JAMA, [doi:10.1001/jamainternmed.2023.2909](https://jamanetwork.com/journals/jamainternalmedicine/article-abstract/2806980)) |
| 7.17 | AlpaGasus: Training A Better Alpaca with Fewer Data ([:x:](https://arxiv.org/abs/2307.08701)), ([:paperclip:](https://arxiv.org/pdf/2307.08701.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.08701)), ([:house:](https://huggingface.co/papers/2307.08701)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/alpagasus-training-a-better-alpaca-with-fewer)) |
| 7.16 | Communicative Agents for Software Development ([:x:](https://arxiv.org/abs/2307.07924)), ([:paperclip:](https://arxiv.org/pdf/2307.07924.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.07924)), ([:house:](https://huggingface.co/papers/2307.07924)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/communicative-agents-for-software-development)) |
| 7.16 | Planting a SEED of Vision in Large Language Model ([:x:](https://arxiv.org/abs/2307.08041)), ([:paperclip:](https://arxiv.org/pdf/2307.08041.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.08041)), ([:house:](https://huggingface.co/papers/2307.08041)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/planting-a-seed-of-vision-in-large-language)), ([:octocat:](https://github.com/ailab-cvc/seed)![GitHub Repo stars](https://img.shields.io/github/stars/ailab-cvc/seed?style=social)) |
| 7.15 | DreamTeacher: Pretraining Image Backbones with Deep Generative Models ([:x:](https://arxiv.org/abs/2307.07487)), ([:paperclip:](https://arxiv.org/pdf/2307.07487.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.07487)), ([:house:](https://huggingface.co/papers/2307.07487)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dreamteacher-pretraining-image-backbones-with)) |
| 7.15 | INVE: Interactive Neural Video Editing ([:x:](https://arxiv.org/abs/2307.07663)), ([:paperclip:](https://arxiv.org/pdf/2307.07663.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.07663)), ([:house:](https://huggingface.co/papers/2307.07663)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/inve-interactive-neural-video-editing)) |
| 7.14 | Software Testing with Large Language Model: Survey, Landscape, and Vision ([:x:](https://arxiv.org/abs/2307.07221)), ([:book:](https://browse.arxiv.org/pdf/2307.07221.pdf)), ([:paperclip:](https://arxiv.org/pdf/2307.07221.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.07221)), ([:house:](https://huggingface.co/papers/2307.07221)), ([:eight_spoked_asterisk:]()), ([SS](https://www.semanticscholar.org/paper/Software-Testing-with-Large-Language-Model%3A-Survey%2C-Wang-Huang/f43b8a87a96f8abc2467b90538b643a6061416e9)) |
| 7.14 | Are Large Language Models a Threat to Digital Public Goods? Evidence from Activity on Stack Overflow ([:x:](https://arxiv.org/abs/2307.07367)), ([:paperclip:](https://arxiv.org/pdf/2307.07367.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.07367)), ([:house:](https://huggingface.co/papers/2307.07367)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/are-large-language-models-a-threat-to-digital)) |
| 7.14 | China takes major step in regulating generative AI services like ChatGPT ([news](https://edition.cnn.com/2023/07/14/tech/china-ai-regulation-intl-hnk/index.html)), ([生成式人工智能服务管理暂行办法](http://www.cac.gov.cn/2023-07/13/c_1690898327029107.htm)) |
| 7.14 | What Happens When You Ask a Chinese Chatbot About Taiwan? ([news](https://www.nytimes.com/2023/07/14/business/baidu-ernie-openai-chatgpt-chinese.html)) |
| 7.14 | In-context Autoencoder for Context Compression in a Large Language Model ([:x:](https://arxiv.org/abs/2307.06945)), ([:paperclip:](https://arxiv.org/pdf/2307.06945.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.06945)), ([:house:](https://huggingface.co/papers/2307.06945)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/in-context-autoencoder-for-context)) |
| 7.14 | Mega-TTS 2: Zero-Shot Text-to-Speech with Arbitrary Length Speech Prompts ([:x:](https://arxiv.org/abs/2307.07218)), ([:paperclip:](https://arxiv.org/pdf/2307.07218.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.07218)), ([:house:](https://huggingface.co/papers/2307.07218)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mega-tts-2-zero-shot-text-to-speech-with)) |
| 7.14 | Learning to Retrieve In-Context Examples for Large Language Models ([:x:](https://arxiv.org/abs/2307.07164)), ([:paperclip:](https://arxiv.org/pdf/2307.07164.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.07164)), ([:house:](https://huggingface.co/papers/2307.07164)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/learning-to-retrieve-in-context-examples-for)) |
| 7.14 | Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation ([:x:](https://arxiv.org/abs/2307.07954)), ([:paperclip:](https://arxiv.org/pdf/2307.07954.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.07954)), ([:house:](https://huggingface.co/papers/2307.07954)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rerender-a-video-zero-shot-text-guided-video)) |
| 7.14 | CoTracker: It is Better to Track Together ([:x:](https://arxiv.org/abs/2307.07635)), ([:paperclip:](https://arxiv.org/pdf/2307.07635.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.07635)), ([:house:](https://huggingface.co/papers/2307.07635)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/cotracker-it-is-better-to-track-together)) |
| 7.14 | The Practical Guides for Large Language Models ([:octocat:](https://github.com/Mooler0410/LLMsPracticalGuide)![GitHub Repo stars](https://img.shields.io/github/stars/Mooler0410/LLMsPracticalGuide?style=social)) |
| 7.14 | Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning ([paper](https://ai.meta.com/research/publications/scaling-autoregressive-multi-modal-models-pretraining-and-instruction-tuning/)), ([PDF](https://scontent-ssn1-1.xx.fbcdn.net/v/t39.2365-6/358725877_789390529544546_1176484804732743296_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=_diQr9c6Ru8AX9jOhWR&_nc_ht=scontent-ssn1-1.xx&oh=00_AfDtxreR8dzOFPvXAZOSeIhGGGuztehyXcPvY6lcF_BaRA&oe=64B94F32)) |
| 7.14 | Introducing CM3leon, a more efficient, state-of-the-art generative model for text and images ([blog](https://ai.meta.com/blog/generative-ai-text-images-cm3leon/?utm_source=twitter&utm_medium=organic_social&utm_campaign=blog&utm_content=image)) |
| 7.14 | Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation ([:x:](https://arxiv.org/abs/2307.06940)), ([:paperclip:](https://arxiv.org/pdf/2307.06940.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.06940)), ([:house:](https://huggingface.co/papers/2307.06940)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/animate-a-story-storytelling-with-retrieval)), ([:octocat:](https://github.com/videocrafter/animate-a-story)![GitHub Repo stars](https://img.shields.io/github/stars/videocrafter/animate-a-story?style=social)) |
| 7.14 | Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models  ([:x:](https://arxiv.org/abs/2307.06925)), ([:paperclip:](https://arxiv.org/pdf/2307.06925.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.06925)), ([:house:](https://huggingface.co/papers/2307.06925)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/domain-agnostic-tuning-encoder-for-fast)) |
| 7.14 | Generating Benchmarks for Factuality Evaluation of Language Models ([:x:](https://arxiv.org/abs/2307.06908)), ([:paperclip:](https://arxiv.org/pdf/2307.06908.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.06908)), ([:house:](https://huggingface.co/papers/2307.06908)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/generating-benchmarks-for-factuality)) |
| 7.13 | F.T.C. Opens Investigation Into ChatGPT Maker Over Technology’s Potential Harms ([news](https://www.nytimes.com/2023/07/13/technology/chatgpt-investigation-ftc-openai.html)) |
| 7.13 | Instruction Mining: High-Quality Instruction Data Selection for Large Language Models ([:x:](https://arxiv.org/abs/2307.06290)), ([:paperclip:](https://arxiv.org/pdf/2307.06290.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.06290)), ([:house:](https://huggingface.co/papers/2307.06290)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/instruction-mining-high-quality-instruction)) |
| 7.13 | Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution ([:x:](https://arxiv.org/abs/2307.06304)), ([:paperclip:](https://arxiv.org/pdf/2307.06304.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.06304)), ([:house:](https://huggingface.co/papers/2307.06304)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/patch-n-pack-navit-a-vision-transformer-for)) |
| 7.13 | T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation ([:x:](https://arxiv.org/abs/2307.06350)), ([:paperclip:](https://arxiv.org/pdf/2307.06350.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.06350)), ([:house:](https://huggingface.co/papers/2307.06350)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/t2i-compbench-a-comprehensive-benchmark-for)), ([:octocat:](https://github.com/Karine-Huang/T2I-CompBench)![GitHub Repo stars](https://img.shields.io/github/stars/Karine-Huang/T2I-CompBench?style=social)) |
| 7.13 | Distilling Large Language Models for Biomedical Knowledge Extraction: A Case Study on Adverse Drug Events ([:x:](https://arxiv.org/abs/2307.06439)), ([:paperclip:](https://arxiv.org/pdf/2307.06439.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.06439)), ([:house:](https://huggingface.co/papers/2307.06439)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/distilling-large-language-models-for)) |
| 7.13 | DIALGEN: Collaborative Human-LM Generated Dialogues for Improved Understanding of Human-Human Conversations ([:x:](https://arxiv.org/abs/2307.07047)), ([:paperclip:](https://arxiv.org/pdf/2307.07047.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.07047)), ([:house:](https://huggingface.co/papers/2307.07047)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dialgen-collaborative-human-lm-generated)) |
| 7.13 | Copy Is All You Need ([:x:](https://arxiv.org/abs/2307.06962)), ([:paperclip:](https://arxiv.org/pdf/2307.06962.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.06962)), ([:house:](https://huggingface.co/papers/2307.06962)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/copy-is-all-you-need)), ([:octocat:](https://github.com/gmftbygmftby/copyisallyouneed)![GitHub Repo stars](https://img.shields.io/github/stars/gmftbygmftby/copyisallyouneed?style=social)) |
| 7.13 | AniFaceDrawing: Anime Portrait Exploration during Your Sketching ([:x:](https://arxiv.org/abs/2307.07476)), ([:paperclip:](https://arxiv.org/pdf/2307.07476.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.07476)), ([:house:](https://huggingface.co/papers/2307.07476)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/anifacedrawing-anime-portrait-exploration)) |
| 7.13 | HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models ([project](https://hyperdreambooth.github.io/)), ([:x:](https://arxiv.org/abs/2307.06949)), ([:paperclip:](https://arxiv.org/pdf/2307.06949.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.06949)), ([:house:](https://huggingface.co/papers/2307.06949)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hyperdreambooth-hypernetworks-for-fast)) |
| 7.13 | Stability AI releases Stable Doodle, a sketch-to-image tool ([news](https://techcrunch.com/2023/07/13/stability-ai-releases-stable-doodle-a-sketch-to-image-tool/)), ([announcement](https://stability.ai/blog/clipdrop-launches-stable-doodle)) |
| 7.12 | Efficient 3D Articulated Human Generation with Layered Surface Volumes ([:x:](https://arxiv.org/abs/2307.05462)), ([:paperclip:](https://arxiv.org/pdf/2307.05462.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.05462)), ([:house:](https://huggingface.co/papers/2307.05462)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/efficient-3d-articulated-human-generation)) |
| 7.12 | SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning ([:x:](https://arxiv.org/abs/2307.06135)), ([:paperclip:](https://arxiv.org/pdf/2307.06135.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.06135)), ([:house:](https://huggingface.co/papers/2307.06135)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sayplan-grounding-large-language-models-using)) |
| 7.12 | Stack More Layers Differently: High-Rank Training Through Low-Rank Updates ([:x:](https://arxiv.org/abs/2307.05695)), ([:paperclip:](https://arxiv.org/pdf/2307.05695.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.05695)), ([:house:](https://huggingface.co/papers/2307.05695)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/stack-more-layers-differently-high-rank)), ([:octocat:](https://github.com/guitaricet/peft_pretraining)![GitHub Repo stars](https://img.shields.io/github/stars/guitaricet/peft_pretraining?style=social)) |
| 7.12 | PolyLM: An Open Source Polyglot Large Language Model  ([:x:](https://arxiv.org/abs/2307.06018)), ([:paperclip:](https://arxiv.org/pdf/2307.06018.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.06018)), ([:house:](https://huggingface.co/papers/2307.06018)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/polylm-an-open-source-polyglot-large-language)) |
| 7.12 | Today we announce the formation of xAI ([announcement](https://x.ai/)) |
| 7.12 | Large language models encode clinical knowledge (Nature, [https://doi.org/10.1038/s41586-023-06291-2](https://www.nature.com/articles/s41586-023-06291-2)), ([PDF](https://www.nature.com/articles/s41586-023-06291-2.pdf)) |
| 7.12 | Google's NotebookLM ([waitlist](https://notebooklm.google.com/about)) |
| 7.12 | 27% of jobs at high risk from AI revolution, says OECD ([news](https://www.reuters.com/technology/27-jobs-high-risk-ai-revolution-says-oecd-2023-07-11/)) |
| 7.12 | Objaverse-XL: A Universe of 10M+ 3D Objects ([PDF](https://objaverse.allenai.org/objaverse-xl-paper.pdf)) |
| 7.12 | EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone ([:x:](https://arxiv.org/abs/2307.05463)), ([:paperclip:](https://arxiv.org/pdf/2307.05463.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.05463)), ([:house:](https://huggingface.co/papers/2307.05463)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/egovlpv2-egocentric-video-language-pre)) |
| 7.12 | Differentiable Blocks World: Qualitative 3D Decomposition by Rendering Primitives ([:x:](https://arxiv.org/abs/2307.05473)), ([:paperclip:](https://arxiv.org/pdf/2307.05473.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.05473)), ([:house:](https://huggingface.co/papers/2307.05473)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/differentiable-blocks-world-qualitative-3d)) |
| 7.11 | AmadeusGPT: a natural language interface for interactive animal behavioral analysis ([:x:](https://arxiv.org/abs/2307.04858)), ([:paperclip:](https://arxiv.org/pdf/2307.04858.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.04858)), ([:house:](https://huggingface.co/papers/2307.04858)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/amadeusgpt-a-natural-language-interface-for)), ([:octocat:](https://github.com/adaptivemotorcontrollab/amadeusgpt)![GitHub Repo stars](https://img.shields.io/github/stars/adaptivemotorcontrollab/amadeusgpt?style=social)) |
| 7.11 | 3 principles for regulatory-grade large language model application (CIO [news](https://www.cio.com/article/645602/3-principles-for-regulatory-grade-large-language-model-application.html)) |
| 7.11 | Generative Pretraining in Multimodality ([:x:](https://arxiv.org/abs/2307.05222)), ([:paperclip:](https://arxiv.org/pdf/2307.05222.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.05222)), ([:house:](https://huggingface.co/papers/2307.05222)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/generative-pretraining-in-multimodality)), ([:octocat:](https://github.com/baaivision/emu)![GitHub Repo stars](https://img.shields.io/github/stars/baaivision/emu?style=social)) |
| 7.11 | DNAGPT: A Generalized Pretrained Tool for Multiple DNA Sequence Analysis Tasks ([:x:](https://arxiv.org/abs/2307.05628)), ([:paperclip:](https://arxiv.org/pdf/2307.05628.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.05628)), ([:house:](https://huggingface.co/papers/2307.05628)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dnagpt-a-generalized-pretrained-tool-for)) |
| 7.11 | VampNet: Music Generation via Masked Acoustic Token Modeling  ([:x:](https://arxiv.org/abs/2307.04686)), ([:paperclip:](https://arxiv.org/pdf/2307.04686.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.04686)), ([:house:](https://huggingface.co/papers/2307.04686)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vampnet-music-generation-via-masked-acoustic)) |
| 7.11 | Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal Rearrangement  ([:x:](https://arxiv.org/abs/2307.04751)), ([:paperclip:](https://arxiv.org/pdf/2307.04751.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.04751)), ([:house:](https://huggingface.co/papers/2307.04751)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/shelving-stacking-hanging-relational-pose)) |
| 7.11 | International Institutions for Advanced AI ([:x:](https://arxiv.org/abs/2307.04699)), ([:paperclip:](https://arxiv.org/pdf/2307.04699.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.04699)), ([:house:](https://huggingface.co/papers/2307.04699)), ([:eight_spoked_asterisk:]()), ([SS](https://www.semanticscholar.org/paper/International-Institutions-for-Advanced-AI-Ho-Barnhart/df8e10ba7f67779cfed0d3653fdc68a33f75bb14)) |
| 7.11 | Semantic-SAM: Segment and Recognize Anything at Any Granularity  ([:x:](https://arxiv.org/abs/2307.04767)), ([:paperclip:](https://arxiv.org/pdf/2307.04767.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.04767)), ([:house:](https://huggingface.co/papers/2307.04767)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/semantic-sam-segment-and-recognize-anything)), ([:octocat:](https://github.com/ux-decoder/semantic-sam)![GitHub Repo stars](https://img.shields.io/github/stars/ux-decoder/semantic-sam?style=social)) |
| 7.11 | AI tools are designing entirely new proteins that could transform medicine (Nature, [doi: https://doi.org/10.1038/d41586-023-02227-y](https://www.nature.com/articles/d41586-023-02227-y)), ([PDF](https://www.nature.com/articles/d41586-023-02227-y.pdf?pdf=button%20sticky)) |
| 7.11 | Shutterstock expands deal with OpenAI to build generative AI tools ([news](https://techcrunch.com/2023/07/11/shutterstock-expands-deal-with-openai-to-build-generative-ai-tools/)) |
| 7.11 | Generative Pretraining in Multimodality ([:x:](https://arxiv.org/abs/2307.05222)), ([:paperclip:](https://arxiv.org/pdf/2307.05222.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.05222)), ([:house:](https://huggingface.co/papers/2307.05222)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/generative-pretraining-in-multimodality)) |
| 7.11 | Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration  ([:x:](https://arxiv.org/abs/2307.05300)), ([:paperclip:](https://arxiv.org/pdf/2307.05300.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.05300)), ([:house:](https://huggingface.co/papers/2307.05300)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/unleashing-cognitive-synergy-in-large)) |
| 7.11 | Secrets of RLHF in Large Language Models Part I: PPO ([:x:](https://arxiv.org/abs/2307.04964)), ([:paperclip:](https://arxiv.org/pdf/2307.04964.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.04964)), ([:house:](https://huggingface.co/papers/2307.04964)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/secrets-of-rlhf-in-large-language-models-part)),  ([:octocat:](https://github.com/OpenLMLab/MOSS-RLHF)![GitHub Repo stars](https://img.shields.io/github/stars/OpenLMLab/MOSS-RLHF?style=social)) |
| 7.11 | Anthropic's Claude-2 was just released ([blog](https://www.anthropic.com/index/claude-2)), ([claude](https://claude.ai/login)) |
| 7.11 | Large Language Models as General Pattern Machines ([:x:](https://arxiv.org/abs/2307.04721)), ([:paperclip:](https://arxiv.org/pdf/2307.04721.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.04721)), ([:house:](https://huggingface.co/papers/2307.04721)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-as-general-pattern)) |
| 7.10 | Self-Diagnosis and Large Language Models: A New Front for Medical Misinformation ([:x:](https://arxiv.org/abs/2307.04910)), ([:paperclip:](https://arxiv.org/pdf/2307.04910.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.04910)), ([:house:](https://huggingface.co/papers/2307.04910)), ([:eight_spoked_asterisk:]()) |
| 7.10 | Google is testing its medical AI chatbot at the Mayo Clinic ([news](https://www.engadget.com/google-is-testing-its-medical-ai-chatbot-at-the-mayo-clinic-102055669.html?guccounter=1)) |
| 7.10 | RLTF: Reinforcement Learning from Unit Test Feedback ([:x:](https://arxiv.org/abs/2307.04349)), ([:paperclip:](https://arxiv.org/pdf/2307.04349.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.04349)), ([:house:](https://huggingface.co/papers/2307.04349)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rltf-reinforcement-learning-from-unit-test)) |
| 7.10 | AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning ([:x:](https://arxiv.org/abs/2307.04725)), ([:paperclip:](https://arxiv.org/pdf/2307.04725.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.04725)), ([:house:](https://huggingface.co/papers/2307.04725)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/animatediff-animate-your-personalized-text-to)), ([:octocat:](https://github.com/guoyww/animatediff)![GitHub Repo stars](https://img.shields.io/github/stars/guoyww/animatediff?style=social)) |
| 7.10 | GPT Researcher - GPT based autonomous agent that does online comprehensive research on any given topic ([:octocat:](https://github.com/assafelovic/gpt-researcher)![GitHub Repo stars](https://img.shields.io/github/stars/assafelovic/gpt-researcher?style=social))  |
| 7.9 | Chapyter: ChatGPT Code Interpreter in Jupyter Notebooks ([:octocat:](https://github.com/chapyter/chapyter)![GitHub Repo stars](https://img.shields.io/github/stars/chapyter/chapyter?style=social)) |
| 7.9 | DragGAN - Drag Your GAN - Face Inversion: Interactive Point-based Manipulation on the Generative Image Manifold ([tweet](https://twitter.com/radamar/status/1677924592915206144)), ([HF demo](https://huggingface.co/spaces/DragGan/DragGan-Inversion)) |
| 7.8 | Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators ([:x:](https://arxiv.org/abs/2307.05532)), ([:paperclip:](https://arxiv.org/pdf/2307.05532.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.05532)), ([:house:](https://huggingface.co/papers/2307.05532)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/opening-up-chatgpt-tracking-openness)), ([:octocat:](https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io)![GitHub Repo stars](https://img.shields.io/github/stars/opening-up-chatgpt/opening-up-chatgpt.github.io?style=social)) |
| 7.8 | Google’s medical AI chatbot is already being tested in hospitals ([news](https://www.theverge.com/2023/7/8/23788265/google-med-palm-2-mayo-clinic-chatbot-bard-chatgpt)) |
| 7.8 | Large Language Models for Supply Chain Optimization ([:x:](https://arxiv.org/abs/2307.03875)), ([:paperclip:](https://arxiv.org/pdf/2307.03875.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.03875)), ([:house:](https://huggingface.co/papers/2307.03875)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-for-supply-chain)) |
| 7.8 | Sketch-A-Shape: Zero-Shot Sketch-to-3D Shape Generation ([:x:](https://arxiv.org/abs/2307.03869)), ([:paperclip:](https://arxiv.org/pdf/2307.03869.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.03869)), ([:house:](https://huggingface.co/papers/2307.03869)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sketch-a-shape-zero-shot-sketch-to-3d-shape)) |
| 7.8 | AutoDecoding Latent 3D Diffusion Models ([:x:](https://arxiv.org/abs/2307.05445)), ([:paperclip:](https://arxiv.org/pdf/2307.05445.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.05445)), ([:house:](https://huggingface.co/papers/2307.05445)), ([:eight_spoked_asterisk:]()) |
| 7.8 | Awesome Generative AI Techniques: a curated list of Generative AI Techniques ([:octocat:](https://github.com/hollobit/awesome-GenAITech)![GitHub Repo stars](https://img.shields.io/github/stars/hollobit/awesome-GenAITech?style=social))  |
| 7.8 | Robots say they won't steal jobs, rebel against humans ([news](https://www.reuters.com/technology/robots-say-they-wont-steal-jobs-rebel-against-humans-2023-07-07/)) |
| 7.7 | CheXmask: a large-scale dataset of anatomical segmentation masks for multi-center chest x-ray images ([:x:](https://arxiv.org/abs/2307.03293)), ([:paperclip:](https://arxiv.org/pdf/2307.03293.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.03293)), ([:house:](https://huggingface.co/papers/2307.03293)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chexmask-a-large-scale-dataset-of-anatomical)), ([:octocat:](https://github.com/ngaggion/chexmask-database)![GitHub Repo stars](https://img.shields.io/github/stars/ngaggion/chexmask-database?style=social)) |
| 7.7 | Teaching Arithmetic to Small Transformers ([:x:](https://arxiv.org/abs/2307.03381)), ([:paperclip:](https://arxiv.org/pdf/2307.03381.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.03381)), ([:house:](https://huggingface.co/papers/2307.03381)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/teaching-arithmetic-to-small-transformers)) |
| 7.7 | GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest ([:x:](https://arxiv.org/abs/2307.03601)), ([:paperclip:](https://arxiv.org/pdf/2307.03601.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.03601)), ([:house:](https://huggingface.co/papers/2307.03601)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gpt4roi-instruction-tuning-large-language)) |
| 7.7 | Lost in the Middle: How Language Models Use Long Contexts ([:x:](https://arxiv.org/abs/2307.03172)), ([:paperclip:](https://arxiv.org/pdf/2307.03172.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.03172)), ([:house:](https://huggingface.co/papers/2307.03172)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lost-in-the-middle-how-language-models-use)) |
| 7.6 | What Should Data Science Education Do with Large Language Models? ([:x:](https://arxiv.org/abs/2307.02792)), ([:paperclip:](https://arxiv.org/pdf/2307.02792.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.02792)), ([:house:](https://huggingface.co/papers/2307.02792)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/what-should-data-science-education-do-with)) |
| 7.6 | A.I. Will Change Medicine but Not What It Means to Be a Doctor (NYT, [news](https://archive.is/nJtCC)) |
| 7.6 | Frontier AI Regulation: Managing Emerging Risks to Public Safety ([:x:](https://arxiv.org/abs/2307.03718)), ([:paperclip:](https://arxiv.org/pdf/2307.03718.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.03718)), ([:house:](https://huggingface.co/papers/2307.03718)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/frontier-ai-regulation-managing-emerging)), ([SS](https://www.semanticscholar.org/paper/Frontier-AI-Regulation%3A-Managing-Emerging-Risks-to-Anderljung-Barnhart/494b043fce4da2ecc7f87bc96f7c29a5278cca61)) |
| 7.6 | The imperative for regulatory oversight of large language models (or generative AI) in healthcare (npj Digital Medicine, [https://doi.org/10.1038/s41746-023-00873-0](https://www.nature.com/articles/s41746-023-00873-0)), ([PDF](https://www.nature.com/articles/s41746-023-00873-0.pdf?pdf=button%20sticky)) |
| 7.6 | OpenAI launches ChatGTP code interpreter for better coding using only natural language ([tweet](https://twitter.com/OpenAI/status/1677015057316872192)), ([blog](https://the-decoder.com/openai-launches-chatgtp-code-interpreter-for-better-coding-using-only-natural-language/)), ([news](https://www.searchenginejournal.com/code-interpreter-chatgpt-plus/490980/#close)) |
| 7.6 | Jailbroken: How Does LLM Safety Training Fail? ([:x:](https://arxiv.org/abs/2307.02483)), ([:paperclip:](https://arxiv.org/pdf/2307.02483.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.02483)), ([:house:](https://huggingface.co/papers/2307.02483)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/jailbroken-how-does-llm-safety-training-fail)) |
| 7.6 | Building Cooperative Embodied Agents Modularly with Large Language Models ([:x:](https://arxiv.org/abs/2307.02485)), ([:paperclip:](https://arxiv.org/pdf/2307.02485.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.02485)), ([:house:](https://huggingface.co/papers/2307.02485)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/building-cooperative-embodied-agents)) |
| 7.6 | What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?  ([:x:](https://arxiv.org/abs/2307.02469)), ([:paperclip:](https://arxiv.org/pdf/2307.02469.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.02469)), ([:house:](https://huggingface.co/papers/2307.02469)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/what-matters-in-training-a-gpt4-style)) |
| 7.6 | DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models ([:x:](https://arxiv.org/abs/2307.02421)), ([:paperclip:](https://arxiv.org/pdf/2307.02421.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.02421)), ([:house:](https://huggingface.co/papers/2307.02421)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dragondiffusion-enabling-drag-style)), ([:octocat:](https://github.com/mc-e/dragondiffusion)![GitHub Repo stars](https://img.shields.io/github/stars/mc-e/dragondiffusion?style=social)) |
| 7.6 | Elastic Decision Transformer ([:x:](https://arxiv.org/abs/2307.02484)), ([:paperclip:](https://arxiv.org/pdf/2307.02484.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.02484)), ([:house:](https://huggingface.co/papers/2307.02484)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/elastic-decision-transformer)) |
| 7.6 | Releasing 🚀 CodeGen2.5 🚀, a small but mighty LLM for code ([tweet](https://twitter.com/erik_nijkamp/status/1677055271104045056)), ([blog](https://blog.salesforceairesearch.com/codegen25/)), ([:octocat:](https://github.com/salesforce/CodeGen)![GitHub Repo stars](https://img.shields.io/github/stars/salesforce/CodeGen?style=social)) |
| 7.6 | Training Models to Generate, Recognize, and Reframe Unhelpful Thoughts ([:x:](https://arxiv.org/abs/2307.02768)), ([:paperclip:](https://arxiv.org/pdf/2307.02768.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.02768)), ([:house:](https://huggingface.co/papers/2307.02768)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/training-models-to-generate-recognize-and)) |
| 7.6 | Lost in the Middle: How Language Models Use Long Contexts ([:x:](https://arxiv.org/abs/2307.03172)), ([:paperclip:](https://arxiv.org/pdf/2307.03172.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.03172)), ([:house:](https://huggingface.co/papers/2307.03172)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lost-in-the-middle-how-language-models-use)) |
| 7.6 | Artificial Intelligence in Clinical Diagnosis Opportunities, Challenges, and Hype (JAMA, [doi:10.1001/jama.2023.11440](https://jamanetwork.com/journals/jama/fullarticle/2807166)) |
| 7.6 | AI Chatbots, Health Privacy, and Challenges to HIPAA Compliance (JAMA, [doi:10.1001/jama.2023.9458](https://jamanetwork.com/journals/jama/fullarticle/2807170)) |
| 7.6 | Health Care Privacy Risks of AI Chatbots (JAMA, [doi:10.1001/jama.2023.9618](https://jamanetwork.com/journals/jama/fullarticle/2807169)) |
| 7.6 | Generative AI in Health Care and Liability Risks for Physicians and Safety Concerns for Patients (JAMA, [doi:10.1001/jama.2023.9630](https://jamanetwork.com/journals/jama/fullarticle/2807168)) |
| 7.6 | The Challenges for Regulating Medical Use of ChatGPT and Other Large Language Models (JAMA, [doi:10.1001/jama.2023.9651](https://jamanetwork.com/journals/jama/fullarticle/2807167)) |
| 7.6 | A Survey on Evaluation of Large Language Models ([:x:](https://arxiv.org/abs/2307.03109)), ([:paperclip:](https://arxiv.org/pdf/2307.03109.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.03109)), ([:house:](https://huggingface.co/papers/2307.03109)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-survey-on-evaluation-of-large-language)), ([:octocat:](https://github.com/MLGroupJLU/LLM-eval-survey)![GitHub Repo stars](https://img.shields.io/github/stars/MLGroupJLU/LLM-eval-survey?style=social)) , ([SS](https://www.semanticscholar.org/paper/A-Survey-on-Evaluation-of-Large-Language-Models-Chang-Wang/5530f6f57e3e1bc7ba1910b1c4274ec5a7a0a44f)) |
| 7.5 | Collaborative Score Distillation for Consistent Visual Synthesis ([:x:](https://arxiv.org/abs/2307.04787)), ([:paperclip:](https://arxiv.org/pdf/2307.04787.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.04787)), ([:house:](https://huggingface.co/papers/2307.04787)), ([:eight_spoked_asterisk:]()) |
| 7.5 | Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning ([:x:](https://arxiv.org/abs/2307.03692)), ([:paperclip:](https://arxiv.org/pdf/2307.03692.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.03692)), ([:house:](https://huggingface.co/papers/2307.03692)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/becoming-self-instruct-introducing-early)) |
| 7.5 | OpenAI - Introducing Superalignment ([blog](https://openai.com/blog/introducing-superalignment)) |
| 7.5 | Open-Source Large Language Models Outperform Crowd Workers and Approach ChatGPT in Text-Annotation Tasks ([:x:](https://arxiv.org/abs/2307.02179)), ([:paperclip:](https://arxiv.org/pdf/2307.02179.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.02179)), ([:house:](https://huggingface.co/papers/2307.02179)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/open-source-large-language-models-outperform)) |
| 7.5 | Embodied Task Planning with Large Language Models ([:x:](https://arxiv.org/abs/2307.01928)), ([:paperclip:](https://arxiv.org/pdf/2307.01928.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.01928)), ([:house:](https://huggingface.co/papers/2307.01928)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/embodied-task-planning-with-large-language)) |
| 7.5 | Flacuna: Unleashing the Problem Solving Power of Vicuna using FLAN Fine-Tuning ([:x:](https://arxiv.org/abs/2307.02053)), ([:paperclip:](https://arxiv.org/pdf/2307.02053.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.02053)), ([:house:](https://huggingface.co/papers/2307.02053)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/flacuna-unleashing-the-problem-solving-power)), ([:octocat:](https://github.com/declare-lab/flacuna)![GitHub Repo stars](https://img.shields.io/github/stars/declare-lab/flacuna?style=social))  |
| 7.5 | Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners  ([:x:](https://arxiv.org/abs/2307.01848)), ([:paperclip:](https://arxiv.org/pdf/2307.01848.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.01848)), ([:house:](https://huggingface.co/papers/2307.01848)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/robots-that-ask-for-help-uncertainty)) |
| 7.5 | Physics-based Motion Retargeting from Sparse Inputs  ([:x:](https://arxiv.org/abs/2307.01938)), ([:paperclip:](https://arxiv.org/pdf/2307.01938.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.01938)), ([:house:](https://huggingface.co/papers/2307.01938)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/physics-based-motion-retargeting-from-sparse)) |
| 7.5 | MSViT: Dynamic Mixed-Scale Tokenization for Vision Transformers  ([:x:](https://arxiv.org/abs/2307.02321)), ([:paperclip:](https://arxiv.org/pdf/2307.02321.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.02321)), ([:house:](https://huggingface.co/papers/2307.02321)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/msvit-dynamic-mixed-scale-tokenization-for)) |
| 7.5 | Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks ([:x:](https://arxiv.org/abs/2307.02477)), ([:paperclip:](https://arxiv.org/pdf/2307.02477.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.02477)), ([:house:](https://huggingface.co/papers/2307.02477)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/reasoning-or-reciting-exploring-the)) |
| 7.5 | All about the generative tasks in the Generative Medical AI ([blog](https://link.medium.com/be79VOaheBb)) |
| 7.5 | LongNet: Scaling Transformers to 1,000,000,000 Tokens ([:x:](https://arxiv.org/abs/2307.02486)), ([:paperclip:](https://arxiv.org/pdf/2307.02486.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.02486)), ([:house:](https://huggingface.co/papers/2307.02486)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/longnet-scaling-transformers-to-1000000000)), ([:octocat:](https://github.com/kyegomez/LongNet)![GitHub Repo stars](https://img.shields.io/github/stars/kyegomez/LongNet?style=social)) |
| 7.4 | PULSAR at MEDIQA-Sum 2023: Large Language Models Augmented by Synthetic Dialogue Convert Patient Dialogues to Medical Records ([:x:](https://arxiv.org/abs/2307.02006)), ([:paperclip:](https://arxiv.org/pdf/2307.02006.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.02006)), ([:house:](https://huggingface.co/papers/2307.02006)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pulsar-at-mediqa-sum-2023-large-language)) |
| 7.4 | A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image Diagnosis ([:x:](https://arxiv.org/abs/2307.01981)), ([:paperclip:](https://arxiv.org/pdf/2307.01981.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.01981)), ([:house:](https://huggingface.co/papers/2307.01981)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-chatgpt-aided-explainable-framework-for)) |
| 7.4 | Segment Anything Meets Point Tracking ([:x:](https://arxiv.org/abs/2307.01197)), ([:paperclip:](https://arxiv.org/pdf/2307.01197.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.01197)), ([:house:](https://huggingface.co/papers/2307.01197)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/segment-anything-meets-point-tracking)), ([:octocat:](https://github.com/SysCV/sam-pt)![GitHub Repo stars](https://img.shields.io/github/stars/SysCV/sam-pt?style=social)) |
| 7.4 | Career Essentials in Generative AI by Microsoft and LinkedIn ([learning](https://www.linkedin.com/learning/paths/career-essentials-in-generative-ai-by-microsoft-and-linkedin)) |
| 7.4 | SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis ([PDF](https://github.com/Stability-AI/generative-models/blob/main/assets/sdxl_report.pdf)), ([:x:](https://arxiv.org/abs/2307.01952)), ([:paperclip:](https://arxiv.org/pdf/2307.01952.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.01952)), ([:house:](https://huggingface.co/papers/2307.01952)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sdxl-improving-latent-diffusion-models-for)), ([:octocat:](https://github.com/stability-ai/generative-models)![GitHub Repo stars](https://img.shields.io/github/stars/stability-ai/generative-models?style=social)) |
| 7.4 | Real-time Monocular Full-body Capture in World Space via Sequential Proxy-to-Motion Learning ([:x:](https://arxiv.org/abs/2307.01200)), ([:paperclip:](https://arxiv.org/pdf/2307.01200.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.01200)), ([:house:](https://huggingface.co/papers/2307.01200)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/real-time-monocular-full-body-capture-in)) |
| 7.3 | Motion-X: A Large-scale 3D Expressive Whole-body Human Motion Dataset ([:x:](https://arxiv.org/abs/2307.00818)), ([:paperclip:](https://arxiv.org/pdf/2307.00818.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.00818)), ([:house:](https://huggingface.co/papers/2307.00818)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/motion-x-a-large-scale-3d-expressive-whole)), ([:octocat:](https://github.com/idea-research/motion-x)![GitHub Repo stars](https://img.shields.io/github/stars/idea-research/motion-x?style=social)) |
| 7.3 | EmoGen: Eliminating Subjective Bias in Emotional Music Generation ([:x:](https://arxiv.org/abs/2307.01229)), ([:paperclip:](https://arxiv.org/pdf/2307.01229.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.01229)), ([:house:](https://huggingface.co/papers/2307.01229)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/emogen-eliminating-subjective-bias-in)) |
| 7.3 | SketchMetaFace: A Learning-based Sketching Interface for High-fidelity 3D Character Face Modeling ([:x:](https://arxiv.org/abs/2307.00804)), ([:paperclip:](https://arxiv.org/pdf/2307.00804.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.00804)), ([:house:](https://huggingface.co/papers/2307.00804)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sketchmetaface-a-learning-based-sketching)) |
| 7.2 | LEDITS: Real Image Editing with DDPM Inversion and Semantic Guidance ([:x:](https://arxiv.org/abs/2307.00522)), ([:paperclip:](https://arxiv.org/pdf/2307.00522.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.00522)), ([:house:](https://huggingface.co/papers/2307.00522)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ledits-real-image-editing-with-ddpm-inversion)), ([demo](https://huggingface.co/spaces/editing-images/ledits)) |
| 7.1 | Global Mental Health Services and the Impact of Artificial Intelligence–Powered Large Language Models (Jama [doi:10.1001/jamapediatrics.2023.2373](https://jamanetwork.com/journals/jamapsychiatry/fullarticle/2804646)) |
| 7.1 | Personality Traits in Large Language Models ([:x:](https://arxiv.org/abs/2307.00184)), ([:paperclip:](https://arxiv.org/pdf/2307.00184.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.00184)), ([:house:](https://huggingface.co/papers/2307.00184)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/personality-traits-in-large-language-models)) |
| 7.1 | DisCo: Disentangled Control for Referring Human Dance Generation in Real World ([:x:](https://arxiv.org/abs/2307.00040)), ([:paperclip:](https://arxiv.org/pdf/2307.00040.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.00040)), ([:house:](https://huggingface.co/papers/2307.00040)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/disco-disentangled-control-for-referring)), ([:octocat:](https://github.com/Wangt-CN/DisCo)![GitHub Repo stars](https://img.shields.io/github/stars/Wangt-CN/DisCo?style=social)) |
| 7.1 | BatGPT: A Bidirectional Autoregessive Talker from Generative Pre-trained Transformer ([:x:](https://arxiv.org/abs/2307.00360)), ([:paperclip:](https://arxiv.org/pdf/2307.00360.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2307.00360)), ([:house:](https://huggingface.co/papers/2307.00360)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/batgpt-a-bidirectional-autoregessive-talker)) |
| 7.1 | Improve ChatGPT with Knowledge Graphs ([blog](https://mlabonne.github.io/blog/posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html)) |
| 7.1 | The Rise of the AI Engineer ([Blog](https://www.latent.space/p/ai-engineer)) |  
| 6.30 | DrugGPT: A GPT-based Strategy for Designing Potential Ligands Targeting Specific Proteins  ([:x:](https://www.biorxiv.org/content/10.1101/2023.06.29.543848v1)), ([:paperclip:](https://www.biorxiv.org/content/10.1101/2023.06.29.543848v1.full.pdf)) |
| 6.30 | Doctor Chatbot: The EUʼs Regulatory Prescription for Generative Medical AI (Oslo Law Review, [https://doi.org/10.18261/olr.10.1.1](https://www.idunn.no/doi/10.18261/olr.10.1.1)), ([PDF](https://www.idunn.no/doi/epdf/10.18261/olr.10.1.1)) |
| 6.30 | Preference Ranking Optimization for Human Alignment ([:x:](https://arxiv.org/abs/2306.17492)), ([:paperclip:](https://arxiv.org/pdf/2306.17492.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.17492)), ([:house:](https://huggingface.co/papers/2306.17492)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/preference-ranking-optimization-for-human)) |
| 6.30 | Reliability of Medical Information Provided by ChatGPT: Assessment Against Clinical Guidelines and Patient Information Quality Instrument (JMIR, [doi: 10.2196/47479](https://www.jmir.org/2023/1/e47479)), ([PDF](https://www.jmir.org/2023/1/e47479/PDF)) |
| 6.30 | Large language model AI chatbots require approval as medical devices (Nature Medicine, [https://doi.org/10.1038/s41591-023-02412-6](https://www.nature.com/articles/s41591-023-02412-6)) | 
| 6.30 | LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding ([:x:](https://arxiv.org/abs/2306.17107)), ([:paperclip:](https://arxiv.org/pdf/2306.17107.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.17107)), ([:house:](https://huggingface.co/papers/2306.17107)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llavar-enhanced-visual-instruction-tuning-for)) |
| 6.30 | Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors ([:x:](https://arxiv.org/abs/2306.17156)), ([:paperclip:](https://arxiv.org/pdf/2306.17156.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.17156)), ([:house:](https://huggingface.co/papers/2306.17156)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/generative-ai-for-programming-education)) |
| 6.30 | Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation ([:x:](https://arxiv.org/abs/2306.17115)), ([:paperclip:](https://arxiv.org/pdf/2306.17115.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.17115)), ([:house:](https://huggingface.co/papers/2306.17115)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/michelangelo-conditional-3d-shape-generation)) |
| 6.30 | Generate Anything Anywhere in Any Scene ([:x:](https://arxiv.org/abs/2306.17154)), ([:paperclip:](https://arxiv.org/pdf/2306.17154.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.17154)), ([:house:](https://huggingface.co/papers/2306.17154)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/generate-anything-anywhere-in-any-scene)) |
| 6.30 | Benchmarking Large Language Model Capabilities for Conditional Generation ([:x:](https://arxiv.org/abs/2306.16793)), ([:paperclip:](https://arxiv.org/pdf/2306.16793.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.16793)), ([:house:](https://huggingface.co/papers/2306.16793)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/benchmarking-large-language-model)) |
| 6.29 | End-to-end Autonomous Driving: Challenges and Frontiers ([:x:](https://arxiv.org/abs/2306.16927)), ([:paperclip:](https://arxiv.org/pdf/2306.16927.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.16927)), ([:house:](https://huggingface.co/papers/2306.16927)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/end-to-end-autonomous-driving-challenges-and)), ([:octocat:](https://github.com/opendrivelab/end-to-end-autonomous-driving)![GitHub Repo stars](https://img.shields.io/github/stars/opendrivelab/end-to-end-autonomous-driving?style=social)) |
| 6.29 | UMASS_BioNLP at MEDIQA-Chat 2023: Can LLMs generate high-quality synthetic note-oriented doctor-patient conversations?  ([:x:](https://arxiv.org/abs/2306.16931)), ([:paperclip:](https://arxiv.org/pdf/2306.16931.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.16931)), ([:house:](https://huggingface.co/papers/2306.16931)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/umass-bionlp-at-mediqa-chat-2023-can-llms)) |
| 6.29 | ⭐ A Survey of Large Language Models - version 11 ([:x:](https://arxiv.org/abs/2303.18223)), ([:paperclip:](https://arxiv.org/pdf/2303.18223.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.18223)), ([:house:](https://huggingface.co/papers/2303.18223)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-survey-of-large-language-models)), ([:octocat:](https://github.com/rucaibox/llmsurvey)![GitHub Repo stars](https://img.shields.io/github/stars/rucaibox/llmsurvey?style=social)), ([SS](https://www.semanticscholar.org/paper/A-Survey-of-Large-Language-Models-Zhao-Zhou/1d29334cfbe9a1a943082058876f0c22d44c62fd))  |
| 6.29 | June 2023, A Stage Review of Instruction Tuning ([notion](https://yaofu.notion.site/June-2023-A-Stage-Review-of-Instruction-Tuning-f59dbfc36e2d4e12a33443bd6b2012c2)) |
| 6.29 | Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language ([:x:](https://arxiv.org/abs/2306.16410)), ([:paperclip:](https://arxiv.org/pdf/2306.16410.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.16410)), ([:house:](https://huggingface.co/papers/2306.16410)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-language-models-that-can-see-computer)), ([:octocat:](https://github.com/contextualai/lens)![GitHub Repo stars](https://img.shields.io/github/stars/contextualai/lens?style=social)) |
| 6.29 | Towards Measuring the Representation of Subjective Global Opinions in Language Models ([:x:](https://arxiv.org/abs/2306.16388)), ([:paperclip:](https://arxiv.org/pdf/2306.16388.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.16388)), ([:house:](https://huggingface.co/papers/2306.16388)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-measuring-the-representation-of)) |
| 6.29 | REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction ([:x:](https://arxiv.org/abs/2306.15724)), ([:paperclip:](https://arxiv.org/pdf/2306.15724.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.15724)), ([:house:](https://huggingface.co/papers/2306.15724)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/reflect-summarizing-robot-experiences-for)) |
| 6.29 | One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization ([:x:](https://arxiv.org/abs/2306.16928)), ([:paperclip:](https://arxiv.org/pdf/2306.16928.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.16928)), ([:house:](https://huggingface.co/papers/2306.16928)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/one-2-3-45-any-single-image-to-3d-mesh-in-45)) |
| 6.29 | DreamDiffusion: Generating High-Quality Images from Brain EEG Signals ([:x:](https://arxiv.org/abs/2306.16934)), ([:paperclip:](https://arxiv.org/pdf/2306.16934.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.16934)), ([:house:](https://huggingface.co/papers/2306.16934)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dreamdiffusion-generating-high-quality-images)) |
| 6.28 | Regulations to govern use of AI in health records could come later this year ([news](https://fedscoop.com/hhs-health-it-division-carving-out-artificial-intelligence-niche/)) |
| 6.28 | On the Exploitability of Instruction Tuning ([:x:](https://arxiv.org/abs/2306.17194)), ([:paperclip:](https://arxiv.org/pdf/2306.17194.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.17194)), ([:house:](https://huggingface.co/papers/2306.17194)), ([:eight_spoked_asterisk:]()) |
| 6.28 | ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases  ([:x:](https://arxiv.org/abs/2306.16092)), ([:paperclip:](https://arxiv.org/pdf/2306.16092.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.16092)), ([:house:](https://huggingface.co/papers/2306.16092)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chatlaw-open-source-legal-l)), ([:octocat:](https://github.com/pku-yuangroup/chatlaw)![GitHub Repo stars](https://img.shields.io/github/stars/pku-yuangroup/chatlaw?style=social)) |
| 6.28 | RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model ([:x:](https://arxiv.org/abs/2306.16269)), ([:paperclip:](https://arxiv.org/pdf/2306.16269.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.16269)), ([:house:](https://huggingface.co/papers/2306.16269)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rsprompter-learning-to-prompt-for-remote)), ([demo](https://huggingface.co/spaces/KyanChen/RSPrompter)) |
| 6.28 | Extending Context Window of Large Language Models via Positional Interpolation ([:x:](https://arxiv.org/abs/2306.15595)), ([:paperclip:](https://arxiv.org/pdf/2306.15595.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.15595)), ([:house:](https://huggingface.co/papers/2306.15595)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/extending-context-window-of-large-language)) |
| 6.28 | CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy within a \10,000 Budget; An Extra 4,000 Unlocks 81.8% Accuracy  ([:x:](https://arxiv.org/abs/2306.15658)), ([:paperclip:](https://arxiv.org/pdf/2306.15658.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.15658)), ([:house:](https://huggingface.co/papers/2306.15658)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/clipa-v2-scaling-clip-training-with-81-1-zero)), ([:octocat:](https://github.com/ucsc-vlaa/clipa)![GitHub Repo stars](https://img.shields.io/github/stars/ucsc-vlaa/clipa?style=social)) |
| 6.28 | Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision ([:x:](https://arxiv.org/abs/2306.16564)), ([:paperclip:](https://arxiv.org/pdf/2306.16564.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.16564)), ([:house:](https://huggingface.co/papers/2306.16564)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/automatic-calibration-and-error-correction) |
| 6.28 | PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment ([project](https://posediffusion.github.io/)), ([:paperclip:](https://posediffusion.github.io/resources/pose_diffusion.pdf)), ([:octocat:](https://github.com/facebookresearch/PoseDiffusion)![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/PoseDiffusion?style=social)) |
| 6.28 | BrainGPT - A Large Language Model tool to assist neuroscientific research ([home](https://braingpt.org/)) |
| 6.28 | Toward Actionable Generative AI - LAMs: From Large Language Models to Large Action Models ([blog](https://blog.salesforceairesearch.com/large-action-models/)) |
| 6.28 | The official #DragGAN app and code ([tweet](https://twitter.com/OpenMMLab/status/1673884887768784896)), ([application](https://openxlab.org.cn/apps/detail/XingangPan/DragGAN)), ([:octocat:](https://github.com/XingangPan/DragGAN)![GitHub Repo stars](https://img.shields.io/github/stars/XingangPan/DragGAN?style=social)) |
| 6.27 | Introducing ERNIE 3.5: Baidu’s Knowledge-Enhanced Foundation Model Takes a Giant Leap Forward ([blog](http://research.baidu.com/Blog/index-view?id=185)) |
| 6.27 | Beyond the Hype: Assessing the Performance, Trustworthiness, and Clinical Suitability of GPT3.5 ([:x:](https://arxiv.org/abs/2306.15887)), ([:paperclip:](https://arxiv.org/pdf/2306.15887.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.15887)), ([:house:](https://huggingface.co/papers/2306.15887)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/beyond-the-hype-assessing-the-performance)) |
| 6.27 | Vision Augmented Language Models: Computer vision through the LENS of natural language ([blog](https://contextual.ai/introducing-lens/)), ([demo](https://lens.contextual.ai/#intro)), ([:octocat:](https://github.com/ContextualAI/lens)![GitHub Repo stars](https://img.shields.io/github/stars/ContextualAI/lens?style=social)) |
| 6.27 | Restart Sampling for Improving Generative Processes  ([:x:](https://arxiv.org/abs/2306.14878)), ([:paperclip:](https://arxiv.org/pdf/2306.14878.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.14878)), ([:house:](https://huggingface.co/papers/2306.14878)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/restart-sampling-for-improving-generative)), ([:octocat:](https://github.com/newbeeer/diffusion_restart_sampling)![GitHub Repo stars](https://img.shields.io/github/stars/newbeeer/diffusion_restart_sampling?style=social)) |
| 6.27 | 3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and Multi-Dialect Corpus for Speech Representation Disentanglement  ([:x:](https://arxiv.org/abs/2306.15354)), ([:paperclip:](https://arxiv.org/pdf/2306.15354.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.15354)), ([:house:](https://huggingface.co/papers/2306.15354)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/3d-speaker-a-large-scale-multi-device-multi)), ([:octocat:](https://github.com/alibaba-damo-academy/3D-Speaker)![GitHub Repo stars](https://img.shields.io/github/stars/alibaba-damo-academy/3D-Speaker?style=social)) |
| 6.27 | MIMIC: Masked Image Modeling with Image Correspondences ([:x:](https://arxiv.org/abs/2306.15128)), ([:paperclip:](https://arxiv.org/pdf/2306.15128.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.15128)), ([:house:](https://huggingface.co/papers/2306.15128)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mimic-masked-image-modeling-with-image)), ([:octocat:](https://github.com/raivnlab/mimic)![GitHub Repo stars](https://img.shields.io/github/stars/raivnlab/mimic?style=social)) |
| 6.27 | LeanDojo: Theorem Proving with Retrieval-Augmented Language Models ([project](https://leandojo.org/)), ([:x:](https://arxiv.org/abs/2306.15626)), ([:paperclip:](https://arxiv.org/pdf/2306.15626.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.15626)), ([:house:](https://huggingface.co/papers/2306.15626)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/leandojo-theorem-proving-with-retrieval)), ([:octocat:](https://github.com/lean-dojo/leandojo)![GitHub Repo stars](https://img.shields.io/github/stars/lean-dojo/leandojo?style=social)) |
| 6.27 | Any Image to 3D ([blog](https://csm.ai/any-image-to-3d)) |
| 6.27 | ⭐️LangChain Integrations⭐️ Hub ([link](https://integrations.langchain.com/)) |
| 6.27 | MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion ([project](https://mvdiffusion.github.io/)), ([demo](https://huggingface.co/spaces/tangshitao/MVDiffusion)) |
| 6.27 | Extending Context Window of Large Language Models via Positional Interpolation ([:x:](https://arxiv.org/abs/2306.15595)), ([:paperclip:](https://arxiv.org/pdf/2306.15595.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.15595)), ([:house:](https://huggingface.co/papers/2306.15595)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/extending-context-window-of-large-language)) |
| 6.27 | Salesforce open-source LLMs with 8k sequence length - Xgen 7B ([tweet](https://twitter.com/CaimingXiong/status/1674123308177178624)), ([blog](https://blog.salesforceairesearch.com/xgen/)), ([:octocat:](https://github.com/salesforce/xgen)![GitHub Repo stars](https://img.shields.io/github/stars/salesforce/xgen?style=social)) |
| 6.27 | Embracing change and resetting expectations ([blog](https://unlocked.microsoft.com/ai-anthology/terence-tao/)) | 
| 6.27 | Baby steps in evaluating the capacities of large language models (Nature Reviews Psychology, [https://doi.org/10.1038/s44159-023-00211-x](https://www.nature.com/articles/s44159-023-00211-x)), ([preview](https://www.nature.com/articles/s44159-023-00211-x.epdf?sharing_token=PYbU8twpfLCX_0iUnZ5uHdRgN0jAjWel9jnR3ZoTv0PTYDivHgU9XA-WV7YjPPGbQEAeKTPDC7dr9mwqTIpkLUsmlJssgvX6OrpHW0tUqyl6eOBgbVyX3hTm3yuWSHL8TstCrNpVavi8oMDsWvz2M2PcFa-YYEJruKabaEqbDMo%3D)) |
| 6.26 | MedLSAM: Localize and Segment Anything Model for 3D Medical Images ([:x:](https://arxiv.org/abs/2306.14752)), ([:paperclip:](https://arxiv.org/pdf/2306.14752.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.14752)), ([:house:](https://huggingface.co/papers/2306.14752)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/medlsam-localize-and-segment-anything-model)), ([:octocat:](https://github.com/openmedlab/medlsam)![GitHub Repo stars](https://img.shields.io/github/stars/openmedlab/medlsam?style=social)) |
| 6.26 | MotionGPT: Human Motion as a Foreign Language ([:x:](https://arxiv.org/abs/2306.14795)), ([:paperclip:](https://arxiv.org/pdf/2306.14795.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.14795)), ([:house:](https://huggingface.co/papers/2306.14795)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/motiongpt-human-motion-as-a-foreign-language)), ([:octocat:](https://github.com/openmotionlab/motiongpt)![GitHub Repo stars](https://img.shields.io/github/stars/openmotionlab/motiongpt?style=social)) |
| 6.26 | Faster Segment Anything: Towards Lightweight SAM for Mobile Applications ([:x:](https://arxiv.org/abs/2306.14289)), ([:paperclip:](https://arxiv.org/pdf/2306.14289.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.14289)), ([:house:](https://huggingface.co/papers/2306.14289)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/faster-segment-anything-towards-lightweight)), ([:octocat:](https://github.com/chaoningzhang/mobilesam)![GitHub Repo stars](https://img.shields.io/github/stars/chaoningzhang/mobilesam?style=social)) |
| 6.26 | Aligning Large Multi-Modal Model with Robust Instruction Tuning ([:x:](https://arxiv.org/abs/2306.14565)), ([:paperclip:](https://arxiv.org/pdf/2306.14565.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.14565)), ([:house:](https://huggingface.co/papers/2306.14565)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/aligning-large-multi-modal-model-with-robust)) |
| 6.26 | InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback ([project](https://intercode-benchmark.github.io/)), ([:x:](https://arxiv.org/abs/2306.14898)), ([:paperclip:](https://arxiv.org/pdf/2306.14898.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.14898)), ([:house:](https://huggingface.co/papers/2306.14898)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/intercode-standardizing-and-benchmarking)), ([:octocat:](https://github.com/princeton-nlp/intercode)![GitHub Repo stars](https://img.shields.io/github/stars/princeton-nlp/intercode?style=social)) |
| 6.26 | LongCoder: A Long-Range Pre-trained Language Model for Code Completion ([:x:](https://arxiv.org/abs/2306.14893)), ([:paperclip:](https://arxiv.org/pdf/2306.14893.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.14893)), ([:house:](https://huggingface.co/papers/2306.14893)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/longcoder-a-long-range-pre-trained-language)) |
| 6.26 | Kosmos-2: Grounding Multimodal Large Language Models to the World ([:x:](https://arxiv.org/abs/2306.14824)), ([:paperclip:](https://arxiv.org/pdf/2306.14824.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.14824)), ([:house:](https://huggingface.co/papers/2306.14824)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/kosmos-2-grounding-multimodal-large-language)) |
| 6.26 | ViNT: A Foundation Model for Visual Navigation ([project](https://visualnav-transformer.github.io/)), ([:x:](https://arxiv.org/abs/2306.14846)), ([:paperclip:](https://arxiv.org/pdf/2306.14846.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.14846)), ([:house:](https://huggingface.co/papers/2306.14846)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vint-a-foundation-model-for-visual-navigation)), ([video](https://www.youtube.com/watch?v=6kNex5dJ5sQ)) |
| 6.26 | DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing ([:x:](https://arxiv.org/abs/2306.14435)), ([:paperclip:](https://arxiv.org/pdf/2306.14435.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.14435)), ([:house:](https://huggingface.co/papers/2306.14435)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dragdiffusion-harnessing-diffusion-models-for)) |
| 6.25 | Generative AI — LLMOps Architecture Patterns ([blog](https://medium.datadriveninvestor.com/generative-ai-llmops-deployment-architecture-patterns-6d45d1668aba)) |
| 6.25 | DomainStudio: Fine-Tuning Diffusion Models for Domain-Driven Image Generation using Limited Data ([:x:](https://arxiv.org/abs/2306.14153)), ([:paperclip:](https://arxiv.org/pdf/2306.14153.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.14153)), ([:house:](https://huggingface.co/papers/2306.14153)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/domainstudio-fine-tuning-diffusion-models-for)) |
| 6.25 | H_2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models ([:x:](https://arxiv.org/abs/2306.14048)), ([:paperclip:](https://arxiv.org/pdf/2306.14048.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.14048)), ([:house:](https://huggingface.co/papers/2306.14048)), ([:eight_spoked_asterisk:]()) |
| 6.25 | Thinking Like an Annotator: Generation of Dataset Labeling Instructions ([:x:](https://arxiv.org/abs/2306.14035)), ([:paperclip:](https://arxiv.org/pdf/2306.14035.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.14035)), ([:house:](https://huggingface.co/papers/2306.14035)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/thinking-like-an-annotator-generation-of)) |
| 6.25 | Language models are weak learners ([:x:](https://arxiv.org/abs/2306.14101)), ([:paperclip:](https://arxiv.org/pdf/2306.14101.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.14101)), ([:house:](https://huggingface.co/papers/2306.14101)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/language-models-are-weak-learners)) |
| 6.25 | Let's Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning ([:x:](https://arxiv.org/abs/2306.14308)), ([:paperclip:](https://arxiv.org/pdf/2306.14308.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.14308)), ([:house:](https://huggingface.co/papers/2306.14308)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/let-s-do-a-thought-experiment-using)) |
| 6.25 | Chat with Hacker News in real-time using natural language ([demo](https://chathn.vercel.app/)) |
| 6.24 | Zero-shot spatial layout conditioning for text-to-image diffusion models ([:x:](https://arxiv.org/abs/2306.13754)), ([:paperclip:](https://arxiv.org/pdf/2306.13754.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.13754)), ([:house:](https://huggingface.co/papers/2306.13754)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/zero-shot-spatial-layout-conditioning-for)) |
| 6.24 | Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data  ([:x:](https://arxiv.org/abs/2306.13840)), ([:paperclip:](https://arxiv.org/pdf/2306.13840.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.13840)), ([:house:](https://huggingface.co/papers/2306.13840)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/beyond-scale-the-diversity-coefficient-as-a)) |
| 6.24 | On the paper “Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models” ([MIT](https://people.csail.mit.edu/asolar/CoursesPaperStatement.pdf)) |
| 6.24 | A critical analysis of “Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models” ([blog](https://flower-nutria-41d.notion.site/No-GPT4-can-t-ace-MIT-b27e6796ab5a48368127a98216c76864)) |
| 6.24 | System-Level Natural Language Feedback  ([:x:](https://arxiv.org/abs/2306.13588)), ([:paperclip:](https://arxiv.org/pdf/2306.13588.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.13588)), ([:house:](https://huggingface.co/papers/2306.13588)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/system-level-natural-language-feedback)) |
| 6.24 | OpenMask3D: Open-Vocabulary 3D Instance Segmentation ([:x:](https://arxiv.org/abs/2306.13631)), ([:paperclip:](https://arxiv.org/pdf/2306.13631.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.13631)), ([:house:](https://huggingface.co/papers/2306.13631)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/openmask3d-open-vocabulary-3d-instance)) |
| 6.24 | Scaling MLPs: A Tale of Inductive Bias ([:x:](https://arxiv.org/abs/2306.13575)), ([:paperclip:](https://arxiv.org/pdf/2306.13575.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.13575)), ([:house:](https://huggingface.co/papers/2306.13575)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/scaling-mlps-a-tale-of-inductive-bias)) |
| 6.23 | MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models ([:x:](https://arxiv.org/abs/2306.13394)), ([:paperclip:](https://arxiv.org/pdf/2306.13394.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.13394)), ([:house:](https://huggingface.co/papers/2306.13394)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mme-a-comprehensive-evaluation-benchmark-for)), ([:octocat:](https://github.com/awesome-multimodal-large-language-models)![GitHub Repo stars](https://img.shields.io/github/stars/awesome-multimodal-large-language-models?style=social)) |
| 6.23 | What's going on with the Open LLM Leaderboard? ([blog](https://huggingface.co/blog/evaluating-mmlu-leaderboard)) |
| 6.23 | A Survey on Multimodal Large Language Models ([:x:](https://arxiv.org/abs/2306.13549)), ([:paperclip:](https://arxiv.org/pdf/2306.13549.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.13549)), ([:house:](https://huggingface.co/papers/2306.13549)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-survey-on-multimodal-large-language-models)), ([:octocat:](https://github.com/bradyfu/awesome-multimodal-large-language-models)![GitHub Repo stars](https://img.shields.io/github/stars/bradyfu/awesome-multimodal-large-language-models?style=social)) |
| 6.23 | LLM Powered Autonomous Agents ([blog](https://lilianweng.github.io/posts/2023-06-23-agent/)) |
| 6.23 | DreamEditor: Text-Driven 3D Scene Editing with Neural Fields ([:x:](https://arxiv.org/abs/2306.13455)), ([:paperclip:](https://arxiv.org/pdf/2306.13455.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.13455)), ([:house:](https://huggingface.co/papers/2306.13455)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dreameditor-text-driven-3d-scene-editing-with)) |
| 6.23 | Long-range Language Modeling with Self-retrieval  ([:x:](https://arxiv.org/abs/2306.13421)), ([:paperclip:](https://arxiv.org/pdf/2306.13421.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.13421)), ([:house:](https://huggingface.co/papers/2306.13421)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/long-range-language-modeling-with-self)) |
| 6.23 | Bring Your Own Data! Self-Supervised Evaluation for Large Language Models ([:x:](https://arxiv.org/abs/2306.13651)), ([:paperclip:](https://arxiv.org/pdf/2306.13651.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.13651)), ([:house:](https://huggingface.co/papers/2306.13651)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/bring-your-own-data-self-supervised)), ([:octocat:](https://github.com/neelsjain/byod)![GitHub Repo stars](https://img.shields.io/github/stars/neelsjain/byod?style=social)) |
| 6.22 | Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs ([:x:](https://arxiv.org/abs/2306.13063)), ([:book:](https://browse.arxiv.org/pdf/2306.13063.pdf)), ([:paperclip:](https://arxiv.org/pdf/2306.13063.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.13063)), ([:house:](https://huggingface.co/papers/2306.13063)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/can-llms-express-their-uncertainty-an)), ([SS](https://www.semanticscholar.org/paper/Can-LLMs-Express-Their-Uncertainty-An-Empirical-of-Xiong-Hu/8f7297454d7f44365b9bcda5ebb9439a43daf5e6)) |
| 6.22 | reliableGPT: Stop OpenAI Errors in Production ([:octocat:](https://github.com/BerriAI/reliableGPT)![GitHub Repo stars](https://img.shields.io/github/stars/BerriAI/reliableGPT?style=social)) |
| 6.22 | Lit-GPT : Implementation of Falcon, StableLM, Pythia, INCITE language models based on nanoGPT ([:octocat:](https://github.com/Lightning-AI/lit-gpt)![GitHub Repo stars](https://img.shields.io/github/stars/Lightning-AI/lit-gpt?style=social)) |
| 6.22 | Perspective Fields for Single Image Camera Calibration ([project page](https://jinlinyi.github.io/PerspectiveFields/)), ([video](https://www.youtube.com/watch?v=sN5B_ZvMva8)), ([demo](https://huggingface.co/spaces/jinlinyi/PerspectiveFields)), ([:x:](https://arxiv.org/abs/2212.03239)), ([:paperclip:](https://arxiv.org/pdf/2212.03239.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2212.03239)), ([:house:](https://huggingface.co/papers/2212.03239)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/perspective-fields-for-single-image-camera)), ([:octocat:](https://github.com/jinlinyi/PerspectiveFields)![GitHub Repo stars](https://img.shields.io/github/stars/jinlinyi/PerspectiveFields?style=social)), (CVPR 2023) |
| 6.22 | Event Stream GPT (ESGPT), for "event stream" datasets, particularly Electronic Health Record (EHR) datasets ([tweet](https://twitter.com/MattBMcDermott/status/1671912624366166018)), ([:octocat:](https://github.com/mmcdermott/EventStreamGPT)![GitHub Repo stars](https://img.shields.io/github/stars/mmcdermott/EventStreamGPT?style=social)) |
| 6.22 | MPT-30B is here ([tweet](https://twitter.com/jefrankle/status/1671897555435913220)), ([blog](https://www.mosaicml.com/blog/mpt-30b)), ([HF](https://huggingface.co/mosaicml/mpt-30b)), ([MosaicML MPT-30B-Chat](https://huggingface.co/spaces/mosaicml/mpt-30b-chat)) |
| 6.22 | How continuous batching enables 23x throughput in LLM inference while reducing p50 latency ([blog](https://www.anyscale.com/blog/continuous-batching-llm-inference)) |
| 6.22 | DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation ([:x:](https://arxiv.org/abs/2306.12422)), ([:paperclip:](https://arxiv.org/pdf/2306.12422.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.12422)), ([:house:](https://huggingface.co/papers/2306.12422)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dreamtime-an-improved-optimization-strategy)) |
| 6.22 | Stability AI launches SDXL 0.9: A Leap Forward in AI Image Generation ([news](https://stability.ai/blog/sdxl-09-stable-diffusion)) |
| 6.21 | ChatGPT Poses New Regulatory Questions for FDA, Medical Industry (Bloomber [news](https://news.bloomberglaw.com/health-law-and-business/chatgpt-poses-new-regulatory-questions-for-fda-medical-industry)), [Youtube](https://www.youtube.com/watch?v=ZLCfasjTWvY)) |
| 6.21 | Understanding Social Reasoning in Language Models with Language Models ([:x:](https://arxiv.org/abs/2306.15448)), ([:paperclip:](https://arxiv.org/pdf/2306.15448.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.15448)), ([:house:](https://huggingface.co/papers/2306.15448)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/understanding-social-reasoning-in-language)) |
| 6.21 | Opportunities and Risks of LLMs for Scalable Deliberation with Polis ([:x:](https://arxiv.org/abs/2306.11932)), ([:paperclip:](https://arxiv.org/pdf/2306.11932.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.11932)), ([:house:](https://huggingface.co/papers/2306.11932)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/opportunities-and-risks-of-llms-for-scalable)) |
| 6.21 | Training Transformers with 4-bit Integers ([:x:](https://arxiv.org/abs/2306.11987)), ([:paperclip:](https://arxiv.org/pdf/2306.11987.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.11987)), ([:house:](https://huggingface.co/papers/2306.11987)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/training-transformers-with-4-bit-integers)) |
| 6.21 | Fast Segment Anything ([:x:](https://arxiv.org/abs/2306.12156)), ([:paperclip:](https://arxiv.org/pdf/2306.12156.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.12156)), ([:house:](https://huggingface.co/papers/2306.12156)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fast-segment-anything)), ([:octocat:](https://github.com/casia-iva-lab/fastsam)![GitHub Repo stars](https://img.shields.io/github/stars/casia-iva-lab/fastsam?style=social)) |
| 6.21 | DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models ([:x:](https://arxiv.org/abs/2306.11698)), ([:paperclip:](https://arxiv.org/pdf/2306.11698.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.11698)), ([:house:](https://huggingface.co/papers/2306.11698)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/decodingtrust-a-comprehensive-assessment-of)) |
| 6.21 | LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models ([:x:](https://arxiv.org/abs/2306.12420)), ([:book:](https://browse.arxiv.org/pdf/2306.12420.pdf)), ([:paperclip:](https://arxiv.org/pdf/2306.12420.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.12420)), ([:house:](https://huggingface.co/papers/2306.12420)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lmflow-an-extensible-toolkit-for-finetuning)), ([:octocat:](https://github.com/optimalscale/lmflow)![GitHub Repo stars](https://img.shields.io/github/stars/optimalscale/lmflow?style=social))  |
| 6.20 | Visual Foundation Models for Medical Image Analysis ([blog](https://developer.nvidia.com/blog/visual-foundation-models-for-medical-image-analysis/)) |
| 6.20 | Learning to Generate Better Than Your LLM ([:x:](https://arxiv.org/abs/2306.11816)), ([:paperclip:](https://arxiv.org/pdf/2306.11816.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.11816)), ([:house:](https://huggingface.co/papers/2306.11816)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/learning-to-generate-better-than-your-llm)) |
| 6.20 | Sound reconstruction from human brain activity via a generative model with brain-like auditory features ([:x:](https://arxiv.org/abs/2306.11629)), ([:paperclip:](https://arxiv.org/pdf/2306.11629.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.11629)), ([:house:](https://huggingface.co/papers/2306.11629)), ([:eight_spoked_asterisk:](https://cs.paperswithcode.com/paper/sound-reconstruction-from-human-brain)), ([:octocat:](https://github.com/KamitaniLab/DeepImageReconstruction)![GitHub Repo stars](https://img.shields.io/github/stars/KamitaniLab/DeepImageReconstruction?style=social)) |
| 6.20 | A Simple and Effective Pruning Approach for Large Language Models ([:x:](https://arxiv.org/abs/2306.11695)), ([:paperclip:](https://arxiv.org/pdf/2306.11695.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.11695)), ([:house:](https://huggingface.co/papers/2306.11695)), ([:eight_spoked_asterisk:]([https://paperswithcode.com/paper/fast-segment-anything](https://paperswithcode.com/paper/a-simple-and-effective-pruning-approach-for)), ([:octocat:](https://github.com/locuslab/wanda)![GitHub Repo stars](https://img.shields.io/github/stars/locuslab/wanda?style=social)) |
| 6.20 | Radiology Report Expert Evaluation (ReXVal) Dataset (PhysioNet [https://doi.org/10.13026/2fp8-qr71](https://physionet.org/content/rexval-dataset/1.0.0/)) |
| 6.20 | RoboCat: A Self-Improving Foundation Agent for Robotic Manipulation ([:x:](https://arxiv.org/abs/2306.11706)), ([:paperclip:](https://arxiv.org/pdf/2306.11706.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.11706)), ([:house:](https://huggingface.co/papers/2306.11706)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/robocat-a-self-improving-foundation-agent-for)), ([:octocat:](https://github.com/kyegomez/RoboCAT)![GitHub Repo stars](https://img.shields.io/github/stars/kyegomez/RoboCAT?style=social)) |
| 6.20 | Segment Anything Model (SAM) for Radiation Oncology ([:x:](https://arxiv.org/abs/2306.11730)), ([:paperclip:](https://arxiv.org/pdf/2306.11730.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.11730)), ([:house:](https://huggingface.co/papers/2306.11730)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/segment-anything-model-sam-for-radiation)) |
| 6.20 | RepoFusion: Training Code Models to Understand Your Repository  ([:x:](https://arxiv.org/abs/2306.10998)), ([:paperclip:](https://arxiv.org/pdf/2306.10998.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.10998)), ([:house:](https://huggingface.co/papers/2306.10998)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/repofusion-training-code-models-to-understand)) |
| 6.20 | Textbooks Are All You Need ([:x:](https://arxiv.org/abs/2306.11644)), ([:paperclip:](https://arxiv.org/pdf/2306.11644.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.11644)), ([:house:](https://huggingface.co/papers/2306.11644)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/textbooks-are-all-you-need)) |
| 6.19 | Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost ([:x:](https://arxiv.org/abs/2306.10765)), ([:paperclip:](https://arxiv.org/pdf/2306.10765.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.10765)), ([:house:](https://huggingface.co/papers/2306.10765)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/path-to-medical-agi-unify-domain-specific)), ([:octocat:](https://github.com/joshuachou2018/medagi)![GitHub Repo stars](https://img.shields.io/github/stars/joshuachou2018/medagi?style=social)) |
| 6.19 | CounselGPT - Korean psychological counseling dataset ([:octocat:](https://github.com/MrBananaHuman/CounselGPT)![GitHub Repo stars](https://img.shields.io/github/stars/MrBananaHuman/CounselGPT?style=social)) |
| 6.19 | MotionGPT: Finetuned LLMs are General-Purpose Motion Generators ([:x:](https://arxiv.org/abs/2306.10900)), ([:paperclip:](https://arxiv.org/pdf/2306.10900.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.10900)), ([:house:](https://huggingface.co/papers/2306.10900)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/motiongpt-finetuned-llms-are-general-purpose)) |
| 6.18 | Point-Cloud Completion with Pretrained Text-to-image Diffusion Models ([:x:](https://arxiv.org/abs/2306.10533)), ([:paperclip:](https://arxiv.org/pdf/2306.10533.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.10533)), ([:house:](https://huggingface.co/papers/2306.10533)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/point-cloud-completion-with-pretrained-text)) |
| 6.18 | Mercedes-Benz Installs ChatGPT Artificial Intelligence in 900,000 Cars ([Newsweek](https://www.newsweek.com/mercedes-benz-installs-chatgpt-artificial-intelligence-900000-cars-1807384)), ([Mercedes Benz](https://media.mercedes-benz.com/article/323212b5-1b56-458a-9324-20b25cc176cb)) |
| 6.18 | OpenLLaMA-13B released ([tweet](https://twitter.com/hardmaru/status/1670628627057197059)), ([:octocat:](https://github.com/openlm-research/open_llama)![GitHub Repo stars](https://img.shields.io/github/stars/openlm-research/open_llama?style=social)) |
| 6.17 | Generation of Radiology Findings in Chest X-Ray by Leveraging Collaborative Knowledge ([:x:](https://arxiv.org/abs/2306.10448)), ([:paperclip:](https://arxiv.org/pdf/2306.10448.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.10448)), ([:house:](https://huggingface.co/papers/2306.10448)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/generation-of-radiology-findings-in-chest-x)) |
| 6.17 | Beware of Unreliable Data in Model Evaluation: A LLM Prompt Selection case study with Flan-T5 ([blog](https://towardsdatascience.com/beware-of-unreliable-data-in-model-evaluation-a-llm-prompt-selection-case-study-with-flan-t5-88cfd469d058)) | 
| 6.17 | GPT Engineer - specify what you want it to build, the AI asks for clarification, and then builds it ([:octocat:](https://github.com/AntonOsika/gpt-engineer)![GitHub Repo stars](https://img.shields.io/github/stars/AntonOsika/gpt-engineer?style=social)) |
| 6.17 | Demystifying GPT Self-Repair for Code Generation ([:x:](https://arxiv.org/abs/2306.09896)), ([:paperclip:](https://arxiv.org/pdf/2306.09896.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.09896)), ([:house:](https://huggingface.co/papers/2306.09896)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/demystifying-gpt-self-repair-for-code)) |
| 6.17 | Introducing GAIA-1: A Cutting-Edge Generative AI Model for Autonomy ([blog](https://wayve.ai/thinking/introducing-gaia1/)) |
| 6.17 | Understanding Encoder And Decoder LLMs ([blog](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder)) | 
| 6.16 | Evaluating Superhuman Models with Consistency Checks ([:x:](https://arxiv.org/abs/2306.09983)), ([:paperclip:](https://arxiv.org/pdf/2306.09983.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.09983)), ([:house:](https://huggingface.co/papers/2306.09983)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evaluating-superhuman-models-with-consistency)) |
| 6.16 | AvatarBooth: High-Quality and Customizable 3D Human Avatar Generation ([project](https://zeng-yifei.github.io/avatarbooth_page/)), ([:x:](https://arxiv.org/abs/2306.09864)), ([:paperclip:](https://arxiv.org/pdf/2306.09864.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.09864)), ([:house:](https://huggingface.co/papers/2306.09864)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/avatarbooth-high-quality-and-customizable-3d)) |
| 6.16 | Gradient is All You Need? ([:x:](https://arxiv.org/abs/2306.09778)), ([:paperclip:](https://arxiv.org/pdf/2306.09778.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.09778)), ([:house:](https://huggingface.co/papers/2306.09778)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gradients-are-not-all-you-need)), ([:octocat:](https://github.com/google/learned_optimization)![GitHub Repo stars](https://img.shields.io/github/stars/google/learned_optimization?style=social)) |
| 6.16 | LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning ([:x:](https://arxiv.org/abs/2306.09910)), ([:paperclip:](https://arxiv.org/pdf/2306.09910.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.09910)), ([:house:](https://huggingface.co/papers/2306.09910)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/labelbench-a-comprehensive-framework-for)), ([:octocat:](https://github.com/efficienttraining/labelbench)![GitHub Repo stars](https://img.shields.io/github/stars/efficienttraining/labelbench?style=social)) |
| 6.16 | AD-AutoGPT: An Autonomous GPT for Alzheimer's Disease Infodemiology ([:x:](https://arxiv.org/abs/2306.10095)), ([:paperclip:](https://arxiv.org/pdf/2306.10095.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.10095)), ([:house:](https://huggingface.co/papers/2306.10095)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ad-autogpt-an-autonomous-gpt-for-alzheimer-s)) |
| 6.16 | Meta - Introducing Voicebox: The Most Versatile AI for Speech Generation ([news](https://about.fb.com/news/2023/06/introducing-voicebox-ai-for-speech-generation/)) |
| 6.16 | Explore, Establish, Exploit: Red Teaming Language Models from Scratch  ([:x:](https://arxiv.org/abs/2306.09442)), ([:paperclip:](https://arxiv.org/pdf/2306.09442.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.09442)), ([:house:](https://huggingface.co/papers/2306.09442)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/explore-establish-exploit-red-teaming)) |
| 6.16 | Full Parameter Fine-tuning for Large Language Models with Limited Resources  ([:x:](https://arxiv.org/abs/2306.09782)), ([:paperclip:](https://arxiv.org/pdf/2306.09782.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.09782)), ([:house:](https://huggingface.co/papers/2306.09782)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/full-parameter-fine-tuning-for-large-language)), ([:octocat:](https://github.com/openlmlab/lomo)![GitHub Repo stars](https://img.shields.io/github/stars/openlmlab/lomo?style=social)) |
| 6.16 | ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation ([:x:](https://arxiv.org/abs/2306.09968)), ([:paperclip:](https://arxiv.org/pdf/2306.09968.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.09968)), ([:house:](https://huggingface.co/papers/2306.09968)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/clinicalgpt-large-language-models-finetuned)) |
| 6.16 | CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models ([:x:](https://arxiv.org/abs/2306.09635)), ([:paperclip:](https://arxiv.org/pdf/2306.09635.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.09635)), ([:house:](https://huggingface.co/papers/2306.09635)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/clipsonic-text-to-audio-synthesis-with)) |
| 6.16 | Language-Guided Music Recommendation for Video via Prompt Analogies ([:x:](https://arxiv.org/abs/2306.09327)), ([:paperclip:](https://arxiv.org/pdf/2306.09327.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.09327)), ([:house:](https://huggingface.co/papers/2306.09327)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/language-guided-music-recommendation-for-1)) |
| 6.16 | QR Code AI Art Generator ([tweet](https://twitter.com/radamar/status/1669549592470499328)), ([Hugging face](https://huggingface.co/spaces/huggingface-projects/QR-code-AI-art-generator)), ([SD art](https://stable-diffusion-art.com/qr-code/)) |
| 6.16 | Standford CRFM - Transparency Index for Foundation Model Provider's Compliance measurement with the Draft EU AI Act ([tweet](https://twitter.com/RishiBommasani/status/1669463873869709313)), ([:octocat:](https://github.com/stanford-crfm/TransparencyIndex)![GitHub Repo stars](https://img.shields.io/github/stars/stanford-crfm/TransparencyIndex?style=social)) |
| 6.16 | The economic potential of generative AI: The next productivity frontier (McKinsey & Company. [report](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier)) |
| 6.15 | Med-MMHL: A Multi-Modal Dataset for Detecting Human- and LLM-Generated Misinformation in the Medical Domain ([:x:](https://arxiv.org/abs/2306.08871)), ([:paperclip:](https://arxiv.org/pdf/2306.08871.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.08871)), ([:house:](https://huggingface.co/papers/2306.08871)), ([:eight_spoked_asterisk:](https://cs.paperswithcode.com/paper/med-mmhl-a-multi-modal-dataset-for-detecting)) |
| 6.15 | Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health ([:x:](https://arxiv.org/abs/2306.10070)), ([:paperclip:](https://arxiv.org/pdf/2306.10070.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.10070)), ([:house:](https://huggingface.co/papers/2306.10070)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/opportunities-and-challenges-for-chatgpt-and)), ([SS](https://www.semanticscholar.org/paper/Opportunities-and-Challenges-for-ChatGPT-and-Large-Tian-Jin/8772847b9b87a7babd3aa0b5fe5bf55f8c9e028b)) |
| 6.15 | Introducing the ElevenLabs AI Speech Classifier: Elevating Safety Standards for AI-generated Audio Content ([news](https://beta.elevenlabs.io/blog/ai-speech-classifier/)) |
| 6.15 | ChatGPT AI Shines in Challenging Medical Cases ([news](https://neurosciencenews.com/chatgpt-medical-ai-23480/)) |
| 6.15 | Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge (JAMA [doi:10.1001/jama.2023.8288](https://jamanetwork.com/journals/jama/fullarticle/2806457)) |
| 6.15 | LOVM: Language-Only Vision Model Selection ([:x:](https://arxiv.org/abs/2306.08893)), ([:paperclip:](https://arxiv.org/pdf/2306.08893.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.08893)), ([:house:](https://huggingface.co/papers/2306.08893)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lovm-language-only-vision-model-selection)) |
| 6.15 | WizardCoder: Empowering Code Large Language Models with Evol-Instruct ([:x:](https://arxiv.org/abs/2306.08568)), ([:paperclip:](https://arxiv.org/pdf/2306.08568.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.08568)), ([:house:](https://huggingface.co/papers/2306.08568)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/wizardcoder-empowering-code-large-language)), ([:octocat:](https://github.com/nlpxucan/wizardlm)![GitHub Repo stars](https://img.shields.io/github/stars/nlpxucan/wizardlm?style=social)) |
| 6.15 | Segment Any Point Cloud Sequences by Distilling Vision Foundation Models ([:x:](https://arxiv.org/abs/2306.09347)), ([:paperclip:](https://arxiv.org/pdf/2306.09347.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.09347)), ([:house:](https://huggingface.co/papers/2306.09347)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/segment-any-point-cloud-sequences-by)), ([:octocat:](https://github.com/youquanl/segment-any-point-cloud)![GitHub Repo stars](https://img.shields.io/github/stars/youquanl/segment-any-point-cloud?style=social)) |
| 6.15 | Seeing the World through Your Eyes ([:x:](https://arxiv.org/abs/2306.09348)), ([:paperclip:](https://arxiv.org/pdf/2306.09348.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.09348)), ([:house:](https://huggingface.co/papers/2306.09348)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/seeing-the-world-through-your-eyes)) |
| 6.15 | Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models ([:x:](https://arxiv.org/abs/2306.08997)), ([:paperclip:](https://arxiv.org/pdf/2306.08997.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.08997)), ([:house:](https://huggingface.co/papers/2306.08997)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/exploring-the-mit-mathematics-and-eecs)) |
| 6.15 | Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Theory of Mind ([:x:](https://arxiv.org/abs/2306.09299)), ([:paperclip:](https://arxiv.org/pdf/2306.09299.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.09299)), ([:house:](https://huggingface.co/papers/2306.09299)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/can-language-models-teach-weaker-agents)), ([:octocat:](https://github.com/swarnahub/explanationintervention)![GitHub Repo stars](https://img.shields.io/github/stars/swarnahub/explanationintervention?style=social)) |
| 6.15 | Segment Any Point Cloud Sequences by Distilling Vision Foundation Models ([:x:](https://arxiv.org/abs/2306.09347)), ([:paperclip:](https://arxiv.org/pdf/2306.09347.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.09347)), ([:house:](https://huggingface.co/papers/2306.09347)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/segment-any-point-cloud-sequences-by)), ([:octocat:](https://github.com/youquanl/segment-any-point-cloud)![GitHub Repo stars](https://img.shields.io/github/stars/youquanl/segment-any-point-cloud?style=social)) |
| 6.15 | ChessGPT: Bridging Policy Learning and Language Modeling ([:x:](https://arxiv.org/abs/2306.09200)), ([:paperclip:](https://arxiv.org/pdf/2306.09200.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.09200)), ([:house:](https://huggingface.co/papers/2306.09200)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chessgpt-bridging-policy-learning-and)), ([:octocat:](https://github.com/waterhorse1/chessgpt)![GitHub Repo stars](https://img.shields.io/github/stars/waterhorse1/chessgpt?style=social)) |
| 6.15 | Top Use Cases and Cutting-Edge Solutions with Generative AI in Healthcare ([blog](https://emorphis.health/blogs/generative-ai-in-healthcare-top-use-cases-and-solutions/)) | 
| 6.15 | [SCIENCE] Art and the science of generative AI, Vol 380, Issue 6650, ([DOI: 10.1126/science.adh4451](https://www.science.org/doi/full/10.1126/science.adh4451)) |
| 6.14 | Radiology-GPT: A Large Language Model for Radiology ([demo](https://huggingface.co/spaces/allen-eric/radiology-gpt)), ([:x:](https://arxiv.org/abs/2306.08666)), ([:paperclip:](https://arxiv.org/pdf/2306.08666.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.08666)), ([:house:](https://huggingface.co/papers/2306.08666)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/radiology-gpt-a-large-language-model-for)) |
| 6.14 | Unifying Large Language Models and Knowledge Graphs: A Roadmap ([:x:](https://arxiv.org/abs/2306.08302)), ([:paperclip:](https://arxiv.org/pdf/2306.08302.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.08302)), ([:house:](https://huggingface.co/papers/2306.08302)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/unifying-large-language-models-and-knowledge)) |
| 6.14 | Knowledge Distillation of Large Language Models ([:x:](https://arxiv.org/abs/2306.08543)), ([:paperclip:](https://arxiv.org/pdf/2306.08543.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.08543)), ([:house:](https://huggingface.co/papers/2306.08543)), ([:eight_spoked_asterisk:]()) |
| 6.14 | TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement ([:x:](https://arxiv.org/abs/2306.08637)), ([:paperclip:](https://arxiv.org/pdf/2306.08637.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.08637)), ([:house:](https://huggingface.co/papers/2306.08637)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tapir-tracking-any-point-with-per-frame)) |
| 6.14 | EU MEPs ready to negotiate first-ever rules for safe and transparent AI ([news](https://www.europarl.europa.eu/news/en/press-room/20230609IPR96212/meps-ready-to-negotiate-first-ever-rules-for-safe-and-transparent-ai)) |
| 6.14 | TryOnDiffusion: A Tale of Two UNets ([:x:](https://arxiv.org/abs/2306.08276)), ([:paperclip:](https://arxiv.org/pdf/2306.08276.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.08276)), ([:house:](https://huggingface.co/papers/2306.08276)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tryondiffusion-a-tale-of-two-unets)) |
| 6.14 | AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn ([:x:](https://arxiv.org/abs/2306.08640)), ([:paperclip:](https://arxiv.org/pdf/2306.08640.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.08640)), ([:house:](https://huggingface.co/papers/2306.08640)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/assistgpt-a-general-multi-modal-assistant)) |
| 6.14 | Stable Diffusion with Core ML on Apple Silicon  ([tweet](https://twitter.com/atiorh/status/1669009755191537664)), ([:octocat:](https://github.com/apple/ml-stable-diffusion)![GitHub Repo stars](https://img.shields.io/github/stars/apple/ml-stable-diffusion?style=social)) |
| 6.13 | How AI Responds to Common Lung Cancer Questions: ChatGPT vs Google Bard (RSNA Radiology, [https://doi.org/10.1148/radiol.230922](https://pubs.rsna.org/doi/full/10.1148/radiol.230922)) |
| 6.13 | Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks ([:x:](https://arxiv.org/abs/2306.07899)), ([:paperclip:](https://arxiv.org/pdf/2306.07899.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.07899)), ([:house:](https://huggingface.co/papers/2306.07899)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/artificial-artificial-artificial-intelligence)), ([:octocat:](https://github.com/epfl-dlab/gpturk)![GitHub Repo stars](https://img.shields.io/github/stars/epfl-dlab/gpturk?style=social)) |
| 6.13 | Scalable 3D Captioning with Pretrained Models ([:x:](https://arxiv.org/abs/2306.07279)), ([:paperclip:](https://arxiv.org/pdf/2306.07279.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.07279)), ([:house:](https://huggingface.co/papers/2306.07279)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/scalable-3d-captioning-with-pretrained-models)) |
| 6.13 | Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation ([:x:](https://arxiv.org/abs/2306.07954)), ([:paperclip:](https://arxiv.org/pdf/2306.07954.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.07954)), ([:house:](https://huggingface.co/papers/2306.07954)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rerender-a-video-zero-shot-text-guided-video)) |
| 6.13 | arXiVeri: Automatic table verification with GPT ([:x:](https://arxiv.org/abs/2306.07968)), ([:paperclip:](https://arxiv.org/pdf/2306.07968.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.07968)), ([:house:](https://huggingface.co/papers/2306.07968)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/arxiveri-automatic-table-verification-with)) |
| 6.13 | AVIS: Autonomous Visual Information Seeking with Large Language Models ([:x:](https://arxiv.org/abs/2306.08129)), ([:paperclip:](https://arxiv.org/pdf/2306.08129.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.08129)), ([:house:](https://huggingface.co/papers/2306.08129)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/avis-autonomous-visual-information-seeking)) |
| 6.13 | AniFaceDrawing: Anime Portrait Exploration during Your Sketching ([:x:](https://arxiv.org/abs/2306.07476)), ([:paperclip:](https://arxiv.org/pdf/2306.07476.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.07476)), ([:house:](https://huggingface.co/papers/2306.07476)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/anifacedrawing-anime-portrait-exploration)) |
| 6.13 | h2oGPT: Democratizing Large Language Models ([:x:](https://arxiv.org/abs/2306.08161)), ([:paperclip:](https://arxiv.org/pdf/2306.08161.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.08161)), ([:house:](https://huggingface.co/papers/2306.08161)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/h2ogpt-democratizing-large-language-models)), [:octocat:](https://github.com/h2oai/h2ogpt)![GitHub Repo stars](https://img.shields.io/github/stars/h2oai/h2ogpt?style=social)) |
| 6.13 | 3D molecule generation by denoising voxel grids [:x:](https://arxiv.org/abs/2306.07473)), ([:paperclip:](https://arxiv.org/pdf/2306.07473.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.07473)), ([:house:](https://huggingface.co/papers/2306.07473)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/3d-molecule-generation-by-denoising-voxel)) |
| 6.13 | GeneCIS: A Benchmark for General Conditional Image Similarity ([project page](https://sgvaze.github.io/genecis/)), ([:x:](https://arxiv.org/abs/2306.07969)), ([:paperclip:](https://arxiv.org/pdf/2306.07969.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.07969)), ([:house:](https://huggingface.co/papers/2306.07969)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/genecis-a-benchmark-for-general-conditional-1)), ([:octocat:](https://github.com/facebookresearch/genecis)![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/genecis?style=social)), (CVPR 2023) |
| 6.13 | Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration  ([:octocat:](https://github.com/lyuchenyang/Macaw-LLM)![GitHub Repo stars](https://img.shields.io/github/stars/lyuchenyang/Macaw-LLM?style=social)) |
| 6.13 | One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning ([:x:](https://arxiv.org/abs/2306.07967)), ([:paperclip:](https://arxiv.org/pdf/2306.07967.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.07967)), ([:house:](https://huggingface.co/papers/2306.07967)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/one-for-all-generalized-lora-for-parameter)), ([:octocat:](https://github.com/arnav0400/vit-slim)![GitHub Repo stars](https://img.shields.io/github/stars/arnav0400/vit-slim?style=social)) |
| 6.13 | GitHub survey result - 92% of U.S.-based developers are already using AI coding tools both in and outside of work ([blog](https://github.blog/2023-06-13-survey-reveals-ais-impact-on-the-developer-experience/)) |
| 6.13 | ChatGPT Workspaces - Upcoming ChatGPT features: file uploading, profiles, organizations and workspaces ([reddit](https://www.reddit.com/r/ChatGPT/comments/144cfzg/upcoming_chatgpt_features_file_uploading_profiles/?utm_source=share&utm_medium=web2x&context=3)) |
| 6.12 | Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow ([:x:](https://arxiv.org/abs/2306.07209)), ([:paperclip:](https://arxiv.org/pdf/2306.07209.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.07209)), ([:house:](https://huggingface.co/papers/2306.07209)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/data-copilot-bridging-billions-of-data-and)), ([:octocat:](https://github.com/zwq2018/data-copilot)![GitHub Repo stars](https://img.shields.io/github/stars/zwq2018/data-copilot?style=social)) |
| 6.12 | Transformers learn through gradual rank increase ([:x:](https://arxiv.org/abs/2306.07042)), ([:paperclip:](https://arxiv.org/pdf/2306.07042.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.07042)), ([:house:](https://huggingface.co/papers/2306.07042)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/transformers-learn-through-gradual-rank)) |
| 6.12 | Large Language Models as Tax Attorneys: A Case Study in Legal Capabilities Emergence ([:x:](https://arxiv.org/abs/2306.07075)), ([:paperclip:](https://arxiv.org/pdf/2306.07075.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.07075)), ([:house:](https://huggingface.co/papers/2306.07075)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-as-tax-attorneys-a-case)) |
| 6.12 | Augmenting Language Models with Long-Term Memory ([:x:](https://arxiv.org/abs/2306.07174)), ([:paperclip:](https://arxiv.org/pdf/2306.07174.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.07174)), ([:house:](https://huggingface.co/papers/2306.07174)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/augmenting-language-models-with-long-term)), ([:octocat:](https://github.com/Victorwz/LongMem)![GitHub Repo stars](https://img.shields.io/github/stars/Victorwz/LongMem?style=social)) |
| 6.12 | Yann LeCun and Geoffrrey Hinton's Consensus on a number of questions about AI and catastrophic risks ([tweet](https://twitter.com/ylecun/status/1667947166764023808)) |
| 6.12 | Conversation of Andrew Ng and Geoffrey Hinton about AI and catastrophic risks ([tweet](https://twitter.com/AndrewYNg/status/1667920020587020290)) |
| 6.12 | Benchmarking Neural Network Training Algorithms ([:x:](https://arxiv.org/abs/2306.07179)), ([:paperclip:](https://arxiv.org/pdf/2306.07179.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.07179)), ([:house:](https://huggingface.co/papers/2306.07179)), ([:eight_spoked_asterisk:]()) |
| 6.12 | Lit-llama - Implementation of the LLaMA language model based on nanoGPT ([:octocat:](https://github.com/Lightning-AI/lit-llama)![GitHub Repo stars](https://img.shields.io/github/stars/Lightning-AI/lit-llama?style=social)) |
| 6.12 | OpenAI, DeepMind will open up models to UK government ([news](https://www.politico.eu/article/openai-deepmind-will-open-up-models-to-uk-government/)) |
| 6.12 | WizardLM: An Instruction-following LLM Using Evol-Instruct ([:octocat:](https://github.com/nlpxucan/WizardLM)![GitHub Repo stars](https://img.shields.io/github/stars/nlpxucan/WizardLM?style=social)) |
| 6.11 | The Impact of ChatGPT and LLMs on Medical Imaging Stakeholders: Perspectives and Use Cases  ([:x:](https://arxiv.org/abs/2306.06767)), ([:paperclip:](https://arxiv.org/pdf/2306.06767.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.06767)), ([:house:](https://huggingface.co/papers/2306.06767)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-impact-of-chatgpt-and-llms-on-medical)) |
| 6.11 | Face0: Instantaneously Conditioning a Text-to-Image Model on a Face ([:x:](https://arxiv.org/abs/2306.06638)), ([:paperclip:](https://arxiv.org/pdf/2306.06638.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.06638)), ([:house:](https://huggingface.co/papers/2306.06638)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/face0-instantaneously-conditioning-a-text-to)) |
| 6.11 | A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks ([:x:](https://arxiv.org/abs/2306.07303)), ([:paperclip:](https://arxiv.org/pdf/2306.07303.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.07303)), ([:house:](https://huggingface.co/papers/2306.07303)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-comprehensive-survey-on-applications-of)) |
| 6.10 | Medical Data Augmentation via ChatGPT: A Case Study on Medication Identification and Medication Event Classification ([:x:](https://arxiv.org/abs/2306.07297)), ([:paperclip:](https://arxiv.org/pdf/2306.07297.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.07297)), ([:house:](https://huggingface.co/papers/2306.07297)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/medical-data-augmentation-via-chatgpt-a-case)) |
| 6.10 | Large Language Model Evaluation in 2023: 5 Methods ([blog](https://research.aimultiple.com/large-language-model-evaluation/)) | 
| 6.9 | How Can Recommender Systems Benefit from Large Language Models: A Survey ([:x:](https://arxiv.org/abs/2306.05817)), ([:paperclip:](https://arxiv.org/pdf/2306.05817.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.05817)), ([:house:](https://huggingface.co/papers/2306.05817)),  ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/how-can-recommender-systems-benefit-from)), ([:octocat:](https://github.com/chiangel/awesome-llm-for-recsys)![GitHub Repo stars](https://img.shields.io/github/stars/chiangel/awesome-llm-for-recsys?style=social)) | 
| 6.9 | On the Challenges and Perspectives of Foundation Models for Medical Image Analysis ([:x:](https://arxiv.org/abs/2306.05705)), ([:paperclip:](https://arxiv.org/pdf/2306.05705.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.05705)), ([:house:](https://huggingface.co/papers/2306.05705)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/on-the-challenges-and-perspectives-of)), ([🎓](https://www.semanticscholar.org/paper/On-the-Challenges-and-Perspectives-of-Foundation-Zhang-Metaxas/fed150a219f9c31bdb4920e615c7c9264c634736)) |
| 6.9 | Chat Generative Pretrained Transformer Fails the Multiple-Choice American College of Gastroenterology Self-Assessment Test (The American Journal of Gastroenterology, [DOI: 10.14309/ajg.0000000000002320](https://journals.lww.com/ajg/Abstract/9900/Chat_Generative_Pretrained_Transformer_Fails_the.751.aspx)) |
| 6.9 | Aladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions ([:x:](https://arxiv.org/abs/2306.06212)), ([:paperclip:](https://arxiv.org/pdf/2306.06212.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.06212)), ([:house:](https://huggingface.co/papers/2306.06212)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/aladdin-zero-shot-hallucination-of-stylized)), ([:octocat:](https://github.com/ianhuang0630/aladdin)![GitHub Repo stars](https://img.shields.io/github/stars/ianhuang0630/aladdin?style=social)) |
| 6.9 | Judging LLM-as-a-judge with MT-Bench and Chatbot Arena ([:x:](https://arxiv.org/abs/2306.05685)), ([:paperclip:](https://arxiv.org/pdf/2306.05685.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.05685)), ([:house:](https://huggingface.co/papers/2306.05685)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/judging-llm-as-a-judge-with-mt-bench-and)), ([:octocat:](https://github.com/lm-sys/fastchat)![GitHub Repo stars](https://img.shields.io/github/stars/lm-sys/fastchat?style=social)) |
| 6.9 | Evaluating the Social Impact of Generative AI Systems in Systems and Society ([:x:](https://arxiv.org/abs/2306.05949)), ([:paperclip:](https://arxiv.org/pdf/2306.05949.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.05949)), ([:house:](https://huggingface.co/papers/2306.05949)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evaluating-the-social-impact-of-generative-ai)) |
| 6.9 | Can Large Language Models Infer Causation from Correlation? ([:x:](https://arxiv.org/abs/2306.05836)), ([:paperclip:](https://arxiv.org/pdf/2306.05836.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.05836)), ([:house:](https://huggingface.co/papers/2306.05836)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/can-large-language-models-infer-causation)), ([:octocat:](https://github.com/causalNLP/corr2cause)![GitHub Repo stars](https://img.shields.io/github/stars/causalNLP/corr2cause?style=social)) |
| 6.9 | FinGPT: Open-Source Financial Large Language Models ([:x:](https://arxiv.org/abs/2306.06031)), ([:paperclip:](https://arxiv.org/pdf/2306.06031.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.06031)), ([:house:](https://huggingface.co/papers/2306.06031)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fingpt-open-source-financial-large-language)), ([:octocat:](https://github.com/AI4Finance-Foundation/FinGPT)![GitHub Repo stars](https://img.shields.io/github/stars/AI4Finance-Foundation/FinGPT?style=social)) |
| 6.8 | Interpretable Medical Diagnostics with Structured Data Extraction by Large Language Models ([:x:](https://arxiv.org/abs/2306.05052)), ([:paperclip:](https://arxiv.org/pdf/2306.05052.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.05052)), ([:house:](https://huggingface.co/papers/2306.05052)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/interpretable-medical-diagnostics-with)) |
| 6.8 | Customizing General-Purpose Foundation Models for Medical Report Generation ([:x:](https://arxiv.org/abs/2306.05642)), ([:paperclip:](https://arxiv.org/pdf/2306.05642.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.05642)), ([:house:](https://huggingface.co/papers/2306.05642)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/customizing-general-purpose-foundation-models)) |
| 6.8 | Regulators Face Novel Challenges as Artificial Intelligence Tools Enter Medical Practice (JAMA Health Forum [doi: 10.1001/jamahealthforum.2023.2300](https://jamanetwork.com/journals/jama-health-forum/fullarticle/2806091)) |
| 6.8 | Artificial General Intelligence for Medical Imaging ([:x:](https://arxiv.org/abs/2306.05480)), ([:paperclip:](https://arxiv.org/pdf/2306.05480.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.05480)), ([:house:](https://huggingface.co/papers/2306.05480)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/artificial-general-intelligence-for-medical)) |
| 6.8 | On the Reliability of Watermarks for Large Language Models ([:x:](https://arxiv.org/abs/2306.04634)), ([:paperclip:](https://arxiv.org/pdf/2306.04634.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.04634)), ([:house:](https://huggingface.co/papers/2306.04634)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/on-the-reliability-of-watermarks-for-large)), ([:octocat:](https://github.com/jwkirchenbauer/lm-watermarking)![GitHub Repo stars](https://img.shields.io/github/stars/jwkirchenbauer/lm-watermarking?style=social)) |
| 6.8 | PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization ([:x:](https://arxiv.org/abs/2306.05087)), ([:paperclip:](https://arxiv.org/pdf/2306.05087.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.05087)), ([:house:](https://huggingface.co/papers/2306.05087)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pandalm-an-automatic-evaluation-benchmark-for)), ([:octocat:](https://github.com/weopenml/pandalm)![GitHub Repo stars](https://img.shields.io/github/stars/weopenml/pandalm?style=social)) |
| 6.8 | How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources ([:x:](https://arxiv.org/abs/2306.04751)), ([:paperclip:](https://arxiv.org/pdf/2306.04751.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.04751)), ([:house:](https://huggingface.co/papers/2306.04751)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/how-far-can-camels-go-exploring-the-state-of)) |
| 6.8 | StableDiffusion - Clipdrop Launches Uncrop: The Ultimate Aspect Ratio Editor ([blog](https://stability.ai/blog/clipdrop-launches-uncrop-the-ultimate-aspect-ratio-editor)) |
| 6.8 | Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models ([:x:](https://arxiv.org/abs/2306.05424)), ([:paperclip:](https://arxiv.org/pdf/2306.05424.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.05424)), ([:house:](https://huggingface.co/papers/2306.05424)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/video-chatgpt-towards-detailed-video)), ([:octocat:](https://github.com/mbzuai-oryx/video-chatgpt)![GitHub Repo stars](https://img.shields.io/github/stars/mbzuai-oryx/video-chatgpt?style=social)) |
| 6.8 | Simple and Controllable Music Generation ([:x:](https://arxiv.org/abs/2306.05284)), ([:paperclip:](https://arxiv.org/pdf/2306.05284.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.05284)), ([:house:](https://huggingface.co/papers/2306.05284)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/simple-and-controllable-music-generation)), ([:octocat:](https://github.com/facebookresearch/audiocraft)![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/audiocraft?style=social)) |
| 6.8 | Tracking Everything Everywhere All at Once ([project page](https://omnimotion.github.io/)), ([:x:](https://arxiv.org/abs/2306.05422)), ([:paperclip:](https://arxiv.org/pdf/2306.05422.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.05422)), ([:house:](https://huggingface.co/papers/2306.05422)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tracking-everything-everywhere-all-at-once)), ([:octocat:](https://github.com/qianqianwang68/omnimotion)![GitHub Repo stars](https://img.shields.io/github/stars/qianqianwang68/omnimotion?style=social) |
| 6.8 | Understanding GPT tokenizers ([blog](https://simonwillison.net/2023/Jun/8/gpt-tokenizers/)) |
| 6.7 | The Two Word Test: A Semantic Benchmark for Large Language Models ([:x:](https://arxiv.org/abs/2306.04610)), ([:paperclip:](https://arxiv.org/pdf/2306.04610.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.04610)), ([:house:](https://huggingface.co/papers/2306.04610)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-two-word-test-a-semantic-benchmark-for)) |
| 6.7 | Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation ([:x:](https://arxiv.org/abs/2306.04811)), ([:paperclip:](https://arxiv.org/pdf/2306.04811.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.04811)), ([:house:](https://huggingface.co/papers/2306.04811)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/generative-text-guided-3d-vision-language)) |
| 6.7 | Health system-scale language models are all-purpose prediction engines (Nature [https://doi.org/10.1038/s41586-023-06160-y](https://www.nature.com/articles/s41586-023-06160-y#citeas)), ([:paperclip:](https://www.nature.com/articles/s41586-023-06160-y.pdf?pdf=button%20sticky)), ([:octocat:](https://github.com/nyuolab/NYUTron)![GitHub Repo stars](https://img.shields.io/github/stars/nyuolab/NYUTron?style=social) |
| 6.7 | Learning to Ground Instructional Articles in Videos through Narrations ([:x:](https://arxiv.org/abs/2306.03802)), ([:paperclip:](https://arxiv.org/pdf/2306.03802.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.03802)), ([:house:](https://huggingface.co/papers/2306.03802)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/learning-to-ground-instructional-articles-in)) |
| 6.7 | Emergent Correspondence from Image Diffusion ([:x:](https://arxiv.org/abs/2306.03881)), ([:paperclip:](https://arxiv.org/pdf/2306.03881.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.03881)), ([:house:](https://huggingface.co/papers/2306.03881)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/emergent-correspondence-from-image-diffusion)) |
| 6.7 | Certified Reasoning with Language Models ([:x:](https://arxiv.org/abs/2306.04031)), ([:paperclip:](https://arxiv.org/pdf/2306.04031.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.04031)), ([:house:](https://huggingface.co/papers/2306.04031)), ([:eight_spoked_asterisk:]()) |
| 6.7 | Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions  ([:x:](https://arxiv.org/abs/2306.04140)), ([:paperclip:](https://arxiv.org/pdf/2306.04140.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.04140)), ([:house:](https://huggingface.co/papers/2306.04140)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/increasing-diversity-while-maintaining)) |
| 6.7 | Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks ([:x:](https://arxiv.org/abs/2306.04362)), ([:paperclip:](https://arxiv.org/pdf/2306.04362.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.04362)), ([:house:](https://huggingface.co/papers/2306.04362)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/youku-mplug-a-10-million-large-scale-chinese)), ([:octocat:](https://github.com/x-plug/youku-mplug)![GitHub Repo stars](https://img.shields.io/github/stars/x-plug/youku-mplug?style=social) |
| 6.7 | M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning ([:x:](https://arxiv.org/abs/2306.04387)), ([:paperclip:](https://arxiv.org/pdf/2306.04387.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.04387)), ([:house:](https://huggingface.co/papers/2306.04387)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/m-3-it-a-large-scale-dataset-towards-multi)) |
| 6.7 | PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts ([:x:](https://arxiv.org/abs/2306.04528)), ([:paperclip:](https://arxiv.org/pdf/2306.04528.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.04528)), ([:house:](https://huggingface.co/papers/2306.04528)), ([:eight_spoked_asterisk:]()), ([:octocat:](https://github.com/microsoft/promptbench)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/promptbench?style=social) |
| 6.7 | INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models ([:x:](https://arxiv.org/abs/2306.04757)), ([:paperclip:](https://arxiv.org/pdf/2306.04757.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.04757)), ([:house:](https://huggingface.co/papers/2306.04757)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/promptbench-towards-evaluating-the-robustness)), ([SS](https://www.semanticscholar.org/paper/INSTRUCTEVAL%3A-Towards-Holistic-Evaluation-of-Large-Chia-Hong/17f247649498f5fe5a35a61f5b1cb238cbed70e0)), ([:octocat:](https://github.com/declare-lab/instruct-eval)![GitHub Repo stars](https://img.shields.io/github/stars/declare-lab/instruct-eval?style=social)) |
| 6.7 | ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models ([:x:](https://arxiv.org/abs/2306.04563)), ([:paperclip:](https://arxiv.org/pdf/2306.04563.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.04563)), ([:house:](https://huggingface.co/papers/2306.04563)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chatgpt-is-fun-but-it-is-not-funny-humor-is)) |
| 6.7 | Deductive Verification of Chain-of-Thought Reasoning ([:x:](https://arxiv.org/abs/2306.03872)), ([:paperclip:](https://arxiv.org/pdf/2306.03872.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.03872)), ([:house:](https://huggingface.co/papers/2306.03872)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/deductive-verification-of-chain-of-thought)), ([:octocat:](https://github.com/lz1oceani/verify_cot)![GitHub Repo stars](https://img.shields.io/github/stars/lz1oceani/verify_cot?style=social))  |
| 6.6 | ChatGPT might replace your doctor — and it will actually do a better job of caring for you ([news](https://www.businessinsider.com/ai-chatbots-tech-doctors-medicine-healthcare-system-empathy-quality-email-2023-6)) |
| 6.6 | ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory ([:x:](https://arxiv.org/abs/2306.03901)), ([:paperclip:](https://arxiv.org/pdf/2306.03901.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.03901)), ([:house:](https://huggingface.co/papers/2306.03901)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chatdb-augmenting-llms-with-databases-as)) |
| 6.6 | InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models ([:x:](https://arxiv.org/abs/2306.03082)), ([:paperclip:](https://arxiv.org/pdf/2306.03082.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.03082)), ([:house:](https://huggingface.co/papers/2306.03082)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/instructzero-efficient-instruction)), ([:octocat:](https://github.comlichang-chen/instructzero)![GitHub Repo stars](https://img.shields.io/github/stars/lichang-chen/instructzero?style=social)) |
| 6.6 | HeadSculpt: Crafting 3D Head Avatars with Text ([:x:](https://arxiv.org/abs/2306.03038)), ([:paperclip:](https://arxiv.org/pdf/2306.03038.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.03038)), ([:house:](https://huggingface.co/papers/2306.03038)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/headsculpt-crafting-3d-head-avatars-with-text)) |
| 6.6 | MotionDiffuser: Controllable Multi-Agent Motion Prediction using Diffusion ([:x:](https://arxiv.org/abs/2306.03083)), ([:paperclip:](https://arxiv.org/pdf/2306.03083.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.03083)), ([:house:](https://huggingface.co/papers/2306.03083)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/motiondiffuser-controllable-multi-agent-1)) |
| 6.6 | Neuralangelo: High-Fidelity Neural Surface Reconstruction ([:x:](https://arxiv.org/abs/2306.03092)), ([:paperclip:](https://arxiv.org/pdf/2306.03092.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.03092)), ([:house:](https://huggingface.co/papers/2306.03092)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/neuralangelo-high-fidelity-neural-surface-1)) |
| 6.6 | PokemonChat: Auditing ChatGPT for Pokémon Universe Knowledge ([:x:](https://arxiv.org/abs/2306.03024)), ([:paperclip:](https://arxiv.org/pdf/2306.03024.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.03024)), ([:house:](https://huggingface.co/papers/2306.03024)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pokemonchat-auditing-chatgpt-for-pokemon)) |
| 6.6 | A Static Evaluation of Code Completion by Large Language Models ([:x:](https://arxiv.org/abs/2306.03203)), ([:paperclip:](https://arxiv.org/pdf/2306.03203.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.03203)), ([:house:](https://huggingface.co/papers/2306.03203)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-static-evaluation-of-code-completion-by)) |
| 6.6 | Large Language Models of Code Fail at Completing Code with Potential Bugs  ([:x:](https://arxiv.org/abs/2306.03438)), ([:paperclip:](https://arxiv.org/pdf/2306.03438.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.03438)), ([:house:](https://huggingface.co/papers/2306.03438)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-of-code-fail-at)) |
| 6.6 | Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias ([:x:](https://arxiv.org/abs/2306.03509)), ([:paperclip:](https://arxiv.org/pdf/2306.03509.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.03509)), ([:house:](https://huggingface.co/papers/2306.03509)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mega-tts-zero-shot-text-to-speech-at-scale)) |
| 6.6 | Ada-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis ([:x:](https://arxiv.org/abs/2306.03504)), ([:paperclip:](https://arxiv.org/pdf/2306.03504.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.03504)), ([:house:](https://huggingface.co/papers/2306.03504)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ada-tta-towards-adaptive-high-quality-text-to)) |
| 6.6 | Recognize Anything: A Strong Image Tagging Model ([:x:](https://arxiv.org/abs/2306.03514)), ([:paperclip:](https://arxiv.org/pdf/2306.03514.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.03514)), ([:house:](https://huggingface.co/papers/2306.03514)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/recognize-anything-a-strong-image-tagging)), ([:octocat:](https://github.com/xinyu1205/recognize-anything)![GitHub Repo stars](https://img.shields.io/github/stars/xinyu1205/recognize-anything?style=social)) |
| 6.6 | ATT3D: Amortized Text-to-3D Object Synthesis ([:x:](https://arxiv.org/abs/2306.07349)), ([:paperclip:](https://arxiv.org/pdf/2306.07349.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.07349)), ([:house:](https://huggingface.co/papers/2306.07349)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/att3d-amortized-text-to-3d-object-synthesis)) |
| 6.6 | Falcon-40B-Instruct is a 40B parameters causal decoder-only model built by TII based on Falcon-40B and finetuned on a mixture of Baize ([HF](https://huggingface.co/tiiuae/falcon-40b-instruct)) |
| 6.5 | PULSAR: Pre-training with Extracted Healthcare Terms for Summarising Patients' Problems and Data Augmentation with Black-box Large Language Models ([:x:](https://arxiv.org/abs/2306.02754)), ([:paperclip:](https://arxiv.org/pdf/2306.02754.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.02754)), ([:house:](https://huggingface.co/papers/2306.02754)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pulsar-pre-training-with-extracted-healthcare)) |
| 6.5 | Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese Medical Exam Dataset ([:x:](https://arxiv.org/abs/2306.03030)), ([:paperclip:](https://arxiv.org/pdf/2306.03030.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.03030)), ([:house:](https://huggingface.co/papers/2306.03030)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/benchmarking-large-language-models-on-cmexam)) |
| 6.5 | shs-nlp at RadSum23: Domain-Adaptive Pre-training of Instruction-tuned LLMs for Radiology Report Impression Generation ([:x:](https://arxiv.org/abs/2306.03264)), ([:paperclip:](https://arxiv.org/pdf/2306.03264.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.03264)), ([:house:](https://huggingface.co/papers/2306.03264)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/shs-nlp-at-radsum23-domain-adaptive-pre)) |
| 6.5 | A survey of Generative AI Applications ([:x:](https://arxiv.org/abs/2306.02781)), ([:paperclip:](https://arxiv.org/pdf/2306.02781.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.02781)), ([:house:](https://huggingface.co/papers/2306.02781)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-survey-of-generative-ai-applications)) |
| 6.5 | New Artificial Intelligence ChatGPT Performs Poorly on the 2022 Self-assessment Study Program for Urology (AUA Urology practice [https://doi.org/10.1097/UPJ.0000000000000406](https://www.auajournals.org/doi/10.1097/UPJ.0000000000000406)) |
| 6.5 | LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion ([:x:](https://arxiv.org/abs/2306.02561)), ([:paperclip:](https://arxiv.org/pdf/2306.02561.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.02561)), ([:house:](https://huggingface.co/papers/2306.02561)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llm-blender-ensembling-large-language-models)) |
| 6.5 | Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding ([:x:](https://arxiv.org/abs/2306.02858)), ([:paperclip:](https://arxiv.org/pdf/2306.02858.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.02858)), ([:house:](https://huggingface.co/papers/2306.02858)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/video-llama-an-instruction-tuned-audio-visual)), ([:octocat:](https://github.com/damo-nlp-sg/video-llama)![GitHub Repo stars](https://img.shields.io/github/stars/damo-nlp-sg/video-llama?style=social)) |
| 6.5 | PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Mode ([:x:](https://arxiv.org/abs/2306.02531)), ([:paperclip:](https://arxiv.org/pdf/2306.02531.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.02531)), ([:house:](https://huggingface.co/papers/2306.02531)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/planner-generating-diversified-paragraph-via)) |
| 6.5 | Orca: Progressive Learning from Complex Explanation Traces of GPT-4 ([:x:](https://arxiv.org/abs/2306.02707)), ([:paperclip:](https://arxiv.org/pdf/2306.02707.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.02707)), ([:house:](https://huggingface.co/papers/2306.02707)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/orca-progressive-learning-from-complex)) |
| 6.4 | Fine-Tuning Language Models with Advantage-Induced Policy Alignment ([:x:](https://arxiv.org/abs/2306.02231)), ([:paperclip:](https://arxiv.org/pdf/2306.02231.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.02231)), ([:house:](https://huggingface.co/papers/2306.02231)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fine-tuning-language-models-with-advantage)), ([:octocat:](https://github.com/microsoft/rlhf-apa)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/rlhf-apa?style=social)) |
| 6.4 | SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model  ([:x:](https://arxiv.org/abs/2306.02245)), ([:paperclip:](https://arxiv.org/pdf/2306.02245.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.02245)), ([:house:](https://huggingface.co/papers/2306.02245)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sam3d-zero-shot-3d-object-detection-via)), ([:octocat:](https://github.com/dyzhang09/sam3d)![GitHub Repo stars](https://img.shields.io/github/stars/dyzhang09/sam3d?style=social))  |
| 6.4 | A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models ([:x:](https://arxiv.org/abs/2306.02254)), ([:paperclip:](https://arxiv.org/pdf/2306.02254.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.02254)), ([:house:](https://huggingface.co/papers/2306.02254)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-technical-report-for-polyglot-ko-open)) |
| 6.3 | The Role of ChatGPT, Generative Language Models, and Artificial Intelligence in Medical Education: A Conversation With ChatGPT and a Call for Papers (JMIR, [doi: 10.2196/46885](https://mededu.jmir.org/2023/1/e46885)), ([PDF](https://mededu.jmir.org/2023/1/e46885/PDF)) |
| 6.3 | VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores ([:x:](https://arxiv.org/abs/2306.01879)), ([:paperclip:](https://arxiv.org/pdf/2306.01879.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.01879)), ([:house:](https://huggingface.co/papers/2306.01879)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/visualgptscore-visio-linguistic-reasoning)) |
| 6.2 | Segment Anything in High Quality ([:x:](https://arxiv.org/abs/2306.01567)), ([:paperclip:](https://arxiv.org/pdf/2306.01567.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.01567)), ([:house:](https://huggingface.co/papers/2306.01567)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/segment-anything-in-high-quality)), ([:octocat:](https://github.com/syscv/sam-hq)![GitHub Repo stars](https://img.shields.io/github/stars/syscv/sam-hq?style=social)) |
| 6.2 | The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only ([:x:](https://arxiv.org/abs/2306.01116)), ([:paperclip:](https://arxiv.org/pdf/2306.01116.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.01116)), ([:house:](https://huggingface.co/papers/2306.01116)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-refinedweb-dataset-for-falcon-llm)) |
| 6.2 | StyleDrop: Text-To-Image Generation in Any Style ([project page](https://styledrop.github.io/)), ([:x:](https://arxiv.org/abs/2306.00983)), ([:paperclip:](https://arxiv.org/pdf/2306.00983.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.00983)), ([:house:](https://huggingface.co/papers/2306.00983)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/styledrop-text-to-image-generation-in-any)) |
| 6.1 | How Chatbots and Large Language Model Artificial Intelligence Systems Will Reshape Modern Medicine: Fountain of Creativity or Pandora’s Box? (Jama [doi: 10.1001/jamainternmed.2023.1835](https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2804310)) |
| 6.1 | StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners ([:x:](https://arxiv.org/abs/2306.00984)), ([:paperclip:](https://arxiv.org/pdf/2306.00984.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.00984)), ([:house:](https://huggingface.co/papers/2306.00984)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/stablerep-synthetic-images-from-text-to-image)) |
| 6.1 | The TIME - "The End of Humanity" cover ([tweet](https://twitter.com/TIME/status/1663939590908985348)), (["AI Is Not an Arms Race"](https://time.com/6283609/artificial-intelligence-race-existential-threat/)) | 
| 6.1 | AutoGPTQ - An easy-to-use LLMs quantization package with user-friendly apis, based on GPTQ algorithm ([:octocat:](https://github.com/PanQiWei/AutoGPTQ)![GitHub Repo stars](https://img.shields.io/github/stars/PanQiWei/AutoGPTQ?style=social)) |
| 6.1 | Wuerstchen: Efficient Pretraining of Text-to-Image Models ([:x:](https://arxiv.org/abs/2306.00637)), ([:paperclip:](https://arxiv.org/pdf/2306.00637.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.00637)), ([:house:](https://huggingface.co/papers/2306.00637)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/wuerstchen-efficient-pretraining-of-text-to)) |
| 6.1 | StyleGAN knows Normal, Depth, Albedo, and More ([:x:](https://arxiv.org/abs/2306.00987)), ([:paperclip:](https://arxiv.org/pdf/2306.00987.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.00987)), ([:house:](https://huggingface.co/papers/2306.00987)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/stylegan-knows-normal-depth-albedo-and-more)) |
| 6.1 | Diffusion Self-Guidance for Controllable Image Generation ([:x:](https://arxiv.org/abs/2306.00986)), ([:paperclip:](https://arxiv.org/pdf/2306.00986.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.00986)), ([:house:](https://huggingface.co/papers/2306.00986)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/diffusion-self-guidance-for-controllable)) |
| 6.1 | Thought Cloning: Learning to Think while Acting by Imitating Human Thinking ([:x:](https://arxiv.org/abs/2306.00323)), ([:paperclip:](https://arxiv.org/pdf/2306.00323.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.00323)), ([:house:](https://huggingface.co/papers/2306.00323)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/thought-cloning-learning-to-think-while)), ([:octocat:](https://github.com/ShengranHu/Thought-Cloning)![GitHub Repo stars](https://img.shields.io/github/stars/ShengranHu/Thought-Cloning?style=social)) |
| 6.1 | Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles ([:x:](https://arxiv.org/abs/2306.00989)), ([:paperclip:](https://arxiv.org/pdf/2306.00989.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.00989)), ([:house:](https://huggingface.co/papers/2306.00989)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hiera-a-hierarchical-vision-transformer)) |
| 6.1 | The Hidden Language of Diffusion Models ([:x:](https://arxiv.org/abs/2306.00966)), ([:paperclip:](https://arxiv.org/pdf/2306.00966.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.00966)), ([:house:](https://huggingface.co/papers/2306.00966)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-hidden-language-of-diffusion-models)) |
| 6.1 | Inserting Anybody in Diffusion Models via Celeb Basis ([:x:](https://arxiv.org/abs/2306.00926)), ([:paperclip:](https://arxiv.org/pdf/2306.00926.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.00926)), ([:house:](https://huggingface.co/papers/2306.00926)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/inserting-anybody-in-diffusion-models-via)), ([project page](https://celeb-basis.github.io/)), ([:octocat:](https://github.com/ygtxr1997/celebbasis)![GitHub Repo stars](https://img.shields.io/github/stars/ygtxr1997/celebbasis?style=social)) |
| 6.1 | LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day ([:x:](https://arxiv.org/abs/2306.00890)), ([:paperclip:](https://arxiv.org/pdf/2306.00890.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.00890)), ([:house:](https://huggingface.co/papers/2306.00890)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llava-med-training-a-large-language-and)), ([:octocat:](https://github.com/microsoft/LLaVA-Med)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/LLaVA-Med?style=social)) |
| 6.1 | Birth of a Transformer: A Memory Viewpoint  ([:x:](https://arxiv.org/abs/2306.00802)), ([:paperclip:](https://arxiv.org/pdf/2306.00802.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.00802)), ([:house:](https://huggingface.co/papers/2306.00802)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/birth-of-a-transformer-a-memory-viewpoint)) |
| 6.1 | SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds ([:x:](https://arxiv.org/abs/2306.00980)), ([:paperclip:](https://arxiv.org/pdf/2306.00980.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.00980)), ([:house:](https://huggingface.co/papers/2306.00980)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/snapfusion-text-to-image-diffusion-model-on)) |
| 6.1 | Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance ([:x:](https://arxiv.org/abs/2306.00943)), ([:paperclip:](https://arxiv.org/pdf/2306.00943.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.00943)), ([:house:](https://huggingface.co/papers/2306.00943)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/make-your-video-customized-video-generation)) |
| 6.1 | ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing ([:x:](https://arxiv.org/abs/2306.00622)), ([:paperclip:](https://arxiv.org/pdf/2306.00622.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.00622)), ([:house:](https://huggingface.co/papers/2306.00622)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/reviewergpt-an-exploratory-study-on-using)) |
| 5.31 | The Impact of Positional Encoding on Length Generalization in Transformers ([:x:](https://arxiv.org/abs/2305.19466)), ([:paperclip:](https://arxiv.org/pdf/2305.19466.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.19466)), ([:house:](https://huggingface.co/papers/2305.19466)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-impact-of-positional-encoding-on-length)), ([:octocat:](https://github.com/mcgill-nlp/length-generalization)![GitHub Repo stars](https://img.shields.io/github/stars/mcgill-nlp/length-generalization?style=social)) |
| 5.31 | Artificial Intelligence Can Generate Fraudulent but Authentic-Looking Scientific Medical Articles: Pandora’s Box Has Been Opened (J Med Internet Res 2023;25:e46924 [doi: 10.2196/46924](https://www.jmir.org/2023/1/e46924)) |
| 5.31 | Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust ([:x:](https://arxiv.org/abs/2305.20030)), ([:paperclip:](https://arxiv.org/pdf/2305.20030.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.20030)), ([:house:](https://huggingface.co/papers/2305.20030)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tree-ring-watermarks-fingerprints-for)), ([:octocat:](https://github.com/YuxinWenRick/tree-ring-watermark)![GitHub Repo stars](https://img.shields.io/github/stars/YuxinWenRick/tree-ring-watermark?style=social)) |
| 5.31 | Discovering New Interpretable Conservation Laws as Sparse Invariants ([:x:](https://arxiv.org/abs/2305.19525)), ([:paperclip:](https://arxiv.org/pdf/2305.19525.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.19525)), ([:house:](https://huggingface.co/papers/2305.19525)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/discovering-new-interpretable-conservation)) |
| 5.31 | Control4D: Dynamic Portrait Editing by Learning 4D GAN from 2D Diffusion-based Editor ([:x:](https://arxiv.org/abs/2305.20082)), ([:paperclip:](https://arxiv.org/pdf/2305.20082.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.20082)), ([:house:](https://huggingface.co/papers/2305.20082)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/control4d-dynamic-portrait-editing-by)) |
| 5.31 | Understanding and Mitigating Copying in Diffusion Models ([:x:](https://arxiv.org/abs/2305.20086)), ([:paperclip:](https://arxiv.org/pdf/2305.20086.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.20086)), ([:house:](https://huggingface.co/papers/2305.20086)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/understanding-and-mitigating-copying-in)) |
| 5.31 | PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning ([:x:](https://arxiv.org/abs/2305.19472)), ([:paperclip:](https://arxiv.org/pdf/2305.19472.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.19472)), ([:house:](https://huggingface.co/papers/2305.19472)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/plasma-making-small-language-models-better)) |
| 5.31 | Human or Not? A Gamified Approach to the Turing Test ([:x:](https://arxiv.org/abs/2305.20010)), ([:paperclip:](https://arxiv.org/pdf/2305.20010.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.20010)), ([:house:](https://huggingface.co/papers/2305.20010)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/human-or-not-a-gamified-approach-to-the)) |
| 5.31 | OpenAI - Let’s Verify Step by Step ([paper](https://cdn.openai.com/improving-mathematical-reasoning-with-process-supervision/Lets_Verify_Step_by_Step.pdf)), ([:x:](https://arxiv.org/abs/2305.20050)), ([:paperclip:](https://arxiv.org/pdf/2305.20050.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.20050)), ([:house:](https://huggingface.co/papers/2305.20050)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/let-s-verify-step-by-step-1)), ([blog](https://openai.com/research/improving-mathematical-reasoning-with-process-supervision)), ([GitHub dataset](https://github.com/openai/prm800k)![GitHub Repo stars](https://img.shields.io/github/stars/openai/prm800k?style=social)) | 
| 5.31 | Humans in 4D: Reconstructing and Tracking Humans with Transformers ([:x:](https://arxiv.org/abs/2306.20091)), ([:paperclip:](https://arxiv.org/pdf/2306.20091.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.20091)), ([:house:](https://huggingface.co/papers/2306.20091)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/humans-in-4d-reconstructing-and-tracking)), ([:octocat:](https://github.com/shubham-goel/4D-Humans)![GitHub Repo stars](https://img.shields.io/github/stars/shubham-goel/4D-Humans?style=social)) |
| 5.31 | Improving CLIP Training with Language Rewrites ([:x:](https://arxiv.org/abs/2306.20088)), ([:paperclip:](https://arxiv.org/pdf/2306.20088.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.20088)), ([:house:](https://huggingface.co/papers/2306.20088)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/improving-clip-training-with-language)) |
| 5.31 | MuseCoco: Generating Symbolic Music from Text ([:x:](https://arxiv.org/abs/2306.00110)), ([:paperclip:](https://arxiv.org/pdf/2306.00110.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.00110)), ([:house:](https://huggingface.co/papers/2306.00110)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/musecoco-generating-symbolic-music-from-text)) |
| 5.31 | MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training ([:x:](https://arxiv.org/abs/2306.00107)), ([:paperclip:](https://arxiv.org/pdf/2306.00107.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.00107)), ([:house:](https://huggingface.co/papers/2306.00107)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mert-acoustic-music-understanding-model-with)), ([:octocat:](https://github.com/yizhilll/mert)![GitHub Repo stars](https://img.shields.io/github/stars/yizhilll/mert?style=social)) |
| 5.31 | CodeTF: One-stop Transformer Library for State-of-the-art Code LLM ([:x:](https://arxiv.org/abs/2306.00029)), ([:paperclip:](https://arxiv.org/pdf/2306.00029.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.00029)), ([:house:](https://huggingface.co/papers/2306.00029)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/codetf-one-stop-transformer-library-for-state)), ([:octocat:](https://github.com/salesforce/codetf)![GitHub Repo stars](https://img.shields.io/github/stars/salesforce/codetf?style=social)) |
| 5.30 | HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face (NeurIPS2003 [:x:](https://arxiv.org/abs/2305.17580)), ([:book:](https://browse.arxiv.org/pdf/2305.17580.pdf)), ([:paperclip:](https://arxiv.org/pdf/2305.17580.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.17580)), ([:house:](https://huggingface.co/papers/2305.17580)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hugginggpt-solving-ai-tasks-with-chatgpt-and)), ([:octocat:](https://github.com/microsoft/JARVIS)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/JARVIS?style=social))  |
| 5.30 | Prompt Engineering for Effective Use of Large Language Models in Radiology ([RSNA](https://pubs.rsna.org/page/ai/blog/2023/05/ryai_editorsblog053023)) |
| 5.30 | Re-evaluating Word Mover's Distance ([:x:](https://arxiv.org/abs/2305.14403)), ([:paperclip:](https://arxiv.org/pdf/2305.14403.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.14403)), ([:house:](https://huggingface.co/papers/2305.14403)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/re-evaluating-word-mover-s-distance)), ([:octocat:](https://github.com/joisino/reeval-wmd)![GitHub Repo stars](https://img.shields.io/github/stars/joisino/reeval-wmd?style=social)) |
| 5.30 | Bigger, Better, Faster: Human-level Atari with human-level efficiency ([:x:](https://arxiv.org/abs/2305.19452)), ([:paperclip:](https://arxiv.org/pdf/2305.19452.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.19452)), ([:house:](https://huggingface.co/papers/2305.19452)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/bigger-better-faster-human-level-atari-with)), ([:octocat:](https://github.com/google-research/google-research)![GitHub Repo stars](https://img.shields.io/github/stars/google-research/google-research?style=social)) |
| 5.30 | Japan Goes All In: Copyright Doesn’t Apply To AI Training ([news](https://technomancers.ai/japan-goes-all-in-copyright-doesnt-apply-to-ai-training/)) |
| 5.30 | A.I. Poses ‘Risk of Extinction,’ Industry Leaders Warn - ([NYT news](https://www.nytimes.com/2023/05/30/technology/ai-threat-warning.html)) |
| 5.30 | Statement on AI Risk - AI experts and public figures express their concern about AI risk ([statement](https://www.safe.ai/statement-on-ai-risk)) | 
| 5.30 | GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction ([:x:](https://arxiv.org/abs/2305.18752)), ([:paperclip:](https://arxiv.org/pdf/2305.18752.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.18752)), ([:house:](https://huggingface.co/papers/2305.18752)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gpt4tools-teaching-large-language-model-to)), ([SS](https://www.semanticscholar.org/paper/GPT4Tools%3A-Teaching-Large-Language-Model-to-Use-via-Yang-Song/b458fc5261595f44b36325e5eaea1f874d65138f)), ([:octocat:](https://github.com/stevengrove/gpt4tools)![GitHub Repo stars](https://img.shields.io/github/stars/stevengrove/gpt4tools?style=social)) |
| 5.30 | Nested Diffusion Processes for Anytime Image Generation ([:x:](https://arxiv.org/abs/2305.19066)), ([:paperclip:](https://arxiv.org/pdf/2305.19066.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.19066)), ([:house:](https://huggingface.co/papers/2305.19066)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/nested-diffusion-processes-for-anytime-image)) |
| 5.30 | StyleAvatar3D: Leveraging Image-Text Diffusion Models for High-Fidelity 3D Avatar Generation ([:x:](https://arxiv.org/abs/2305.19012)), ([:paperclip:](https://arxiv.org/pdf/2305.19012.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.19012)), ([:house:](https://huggingface.co/papers/2305.19012)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/styleavatar3d-leveraging-image-text-diffusion)), ([GitHub dataset](https://github.com/icoz69/styleavatar3d)![GitHub Repo stars](https://img.shields.io/github/stars/icoz69/styleavatar3d?style=social))  |
| 5.30 | HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance ([:x:](https://arxiv.org/abs/2305.18766)), ([:paperclip:](https://arxiv.org/pdf/2305.18766.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.18766)), ([:house:](https://huggingface.co/papers/2305.18766)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hifa-high-fidelity-text-to-3d-with-advanced)) |
| 5.30 | Grammar Prompting for Domain-Specific Language Generation with Large Language Models ([:x:](https://arxiv.org/abs/2305.19234)), ([:paperclip:](https://arxiv.org/pdf/2305.19234.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.19234)), ([:house:](https://huggingface.co/papers/2305.19234)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/grammar-prompting-for-domain-specific)) |
| 5.30 | AlteredAvatar: Stylizing Dynamic 3D Avatars with Fast Style Adaptation  ([:x:](https://arxiv.org/abs/2305.19245)), ([:paperclip:](https://arxiv.org/pdf/2305.19245.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.19245)), ([:house:](https://huggingface.co/papers/2305.19245)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/alteredavatar-stylizing-dynamic-3d-avatars)) |
| 5.30 | Ambient Diffusion: Learning Clean Distributions from Corrupted Data  ([:x:](https://arxiv.org/abs/2305.19256)), ([:paperclip:](https://arxiv.org/pdf/2305.19256.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.19256)), ([:house:](https://huggingface.co/papers/2305.19256)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ambient-diffusion-learning-clean)), ([:octocat:](https://github.com/giannisdaras/ambient-diffusion)![GitHub Repo stars](https://img.shields.io/github/stars/giannisdaras/ambient-diffusion?style=social)) |
| 5.30 | ChatGPT and large language models in gastroenterology, ([Nature Reviews Gastroenterology & Hepatology](https://www.nature.com/articles/s41575-023-00799-8)) |
| 5.30 | Blockwise Parallel Transformer for Long Context Large Models ([:x:](https://arxiv.org/abs/2305.19370)), ([:paperclip:](https://arxiv.org/pdf/2305.19370.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.19370)), ([:house:](https://huggingface.co/papers/2305.19370)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/blockwise-parallel-transformer-for-long)) |
| 5.29 | A lawyer used ChatGPT to prepare a court filing. It went horribly awry. ([CBS news](https://www.cbsnews.com/news/lawyer-chatgpt-court-filing-avianca/)) |
| 5.29 | Reconstructing the Mind's Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors ([:x:](https://arxiv.org/abs/2305.18274)), ([:paperclip:](https://arxiv.org/pdf/2305.18274.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.18274)), ([:house:](https://huggingface.co/papers/2305.18274)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/reconstructing-the-mind-s-eye-fmri-to-image)), ([:octocat:](https://github.com/medarc-ai/fmri-reconstruction-nsd)![GitHub Repo stars](https://img.shields.io/github/stars/medarc-ai/fmri-reconstruction-nsd?style=social)) |
| 5.29 | RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths ([:x:](https://arxiv.org/abs/2305.18295)), ([:paperclip:](https://arxiv.org/pdf/2305.18295.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.18295)), ([:house:](https://huggingface.co/papers/2305.18295)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/raphael-text-to-image-generation-via-large)) |
| 5.29 | Photoswap: Personalized Subject Swapping in Images ([:x:](https://arxiv.org/abs/2305.18286)), ([:paperclip:](https://arxiv.org/pdf/2305.18286.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.18286)), ([:house:](https://huggingface.co/papers/2305.18286)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/photoswap-personalized-subject-swapping-in)) |
| 5.29 | TaleCrafter: Interactive Story Visualization with Multiple Characters ([:x:](https://arxiv.org/abs/2305.18247)), ([:paperclip:](https://arxiv.org/pdf/2305.18247.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.18247)), ([:house:](https://huggingface.co/papers/2305.18247)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/talecrafter-interactive-story-visualization)), ([:octocat:](https://github.com/videocrafter/talecrafter)![GitHub Repo stars](https://img.shields.io/github/stars/videocrafter/talecrafter?style=social)) |
| 5.29 | GlyphControl: Glyph Conditional Control for Visual Text Generation ([:x:](https://arxiv.org/abs/2305.18259)), ([:paperclip:](https://arxiv.org/pdf/2305.18259.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.18259)), ([:house:](https://huggingface.co/papers/2305.18259)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/glyphcontrol-glyph-conditional-control-for)) |
| 5.29 | Make-An-Audio 2: Temporal-Enhanced Text-to-Audio Generation ([:x:](https://arxiv.org/abs/2305.18474)), ([:paperclip:](https://arxiv.org/pdf/2305.18474.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.18474)), ([:house:](https://huggingface.co/papers/2305.18474)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/make-an-audio-2-temporal-enhanced-text-to)) |
| 5.29 | Faith and Fate: Limits of Transformers on Compositionality  ([:x:](https://arxiv.org/abs/2305.18654)), ([:paperclip:](https://arxiv.org/pdf/2305.18654.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.18654)), ([:house:](https://huggingface.co/papers/2305.18654)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/faith-and-fate-limits-of-transformers-on)) |
| 5.29 | PaLI-X: On Scaling up a Multilingual Vision and Language Model ([:x:](https://arxiv.org/abs/2305.18565)), ([:paperclip:](https://arxiv.org/pdf/2305.18565.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.18565)), ([:house:](https://huggingface.co/papers/2305.18565)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pali-x-on-scaling-up-a-multilingual-vision)) |
| 5.29 | Controllable Text-to-Image Generation with GPT-4  ([:x:](https://arxiv.org/abs/2305.18583)), ([:paperclip:](https://arxiv.org/pdf/2305.18583.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.18583)), ([:house:](https://huggingface.co/papers/2305.18583)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/controllable-text-to-image-generation-with)) |
| 5.29 | Brainformers: Trading Simplicity for Efficiency ([:x:](https://arxiv.org/abs/2306.00008)), ([:paperclip:](https://arxiv.org/pdf/2306.00008.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.00008)), ([:house:](https://huggingface.co/papers/2306.00008)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/brainformers-trading-simplicity-for)) |
| 5.28 | Large Language Models, scientific knowledge and factuality: A systematic analysis in antibiotic discovery ([:x:](https://arxiv.org/abs/2305.17819)), ([:paperclip:](https://arxiv.org/pdf/2305.17819.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.17819)), ([:house:](https://huggingface.co/papers/2305.17819)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-scientific-knowledge)) |
| 5.28 | Geometric Algebra Transformers ([:x:](https://arxiv.org/abs/2305.18415)), ([:paperclip:](https://arxiv.org/pdf/2305.18415.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.18415)), ([:house:](https://huggingface.co/papers/2305.18415)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/geometric-algebra-transformers)) |
| 5.28 | Tab-CoT: Zero-shot Tabular Chain of Thought ([:x:](https://arxiv.org/abs/2305.17812)), ([:paperclip:](https://arxiv.org/pdf/2305.17812.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.17812)), ([:house:](https://huggingface.co/papers/2305.17812)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tab-cot-zero-shot-tabular-chain-of-thought)) |
| 5.28 | Tab-CoT: Zero-shot Tabular Chain of Thought ([:x:](https://arxiv.org/abs/2305.17812)), ([:paperclip:](https://arxiv.org/pdf/2305.17812.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.17812)), ([:house:](https://huggingface.co/papers/2305.17812)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tab-cot-zero-shot-tabular-chain-of-thought)) |
| 5.28 | FuseCap: Leveraging Large Language Models to Fuse Visual Data into Enriched Image Captions ([:x:](https://arxiv.org/abs/2305.17718)), ([:paperclip:](https://arxiv.org/pdf/2305.17718.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.17718)), ([:house:](https://huggingface.co/papers/2305.17718)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fusecap-leveraging-large-language-models-to)), ([demo](https://huggingface.co/spaces/Xalphinions/tab-cot)) |
| 5.28 | Introducing NVIDIA ACE For Games - Spark Life Into Virtual Characters With Generative AI ([blog](https://www.nvidia.com/en-us/geforce/news/nvidia-ace-for-games-generative-ai-npcs/)) |
| 5.27 | SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks ([:x:](https://arxiv.org/abs/2305.17390)), ([:paperclip:](https://arxiv.org/pdf/2305.17390.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.17390)), ([:house:](https://huggingface.co/papers/2305.17390)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/swiftsage-a-generative-agent-with-fast-and)) |
| 5.27 | The Curse of Recursion: Training on Generated Data Makes Models Forget ([:x:](https://arxiv.org/abs/2305.17493)), ([:paperclip:](https://arxiv.org/pdf/2305.17493.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.17493)), ([:house:](https://huggingface.co/papers/2305.17493)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/model-dementia-generated-data-makes-models)) |
| 5.27 | DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text ([:x:](https://arxiv.org/abs/2305.17359)), ([:paperclip:](https://arxiv.org/pdf/2305.17359.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.17359)), ([:house:](https://huggingface.co/papers/2305.17359)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dna-gpt-divergent-n-gram-analysis-for)) |
| 5.27 | What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks ([:x:](https://arxiv.org/abs/2305.18365)), ([:paperclip:](https://arxiv.org/pdf/2305.18365.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.18365)), ([:house:](https://huggingface.co/papers/2305.18365)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/what-indeed-can-gpt-models-do-in-chemistry-a)) |
| 5.27 | WingmanAI - real-time transcription of audio, integrated with ChatGPT for interactive use ([:octocat:](https://github.com/e-johnstonn/wingmanAI)![GitHub Repo stars](https://img.shields.io/github/stars/e-johnstonn/wingmanAI?style=social)) |
| 5.27 | ToolBench - Large-scale instruction tuning SFT data to equip LLMs with general tool-use capability ([:octocat:](https://github.com/OpenBMB/ToolBench)![GitHub Repo stars](https://img.shields.io/github/stars/OpenBMB/ToolBench?style=social)) |
| 5.27 | G7 officials to hold first meeting on AI regulation next week ([news](https://www.reuters.com/world/g7-officials-hold-first-meeting-ai-regulation-next-week-2023-05-26/)) |
| 5.26 | Large language models improve Alzheimer's disease diagnosis using multi-modality data ([:x:](https://arxiv.org/abs/2305.19280)), ([:paperclip:](https://arxiv.org/pdf/2305.19280.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.19280)), ([:house:](https://huggingface.co/papers/2305.19280)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-improve-alzheimer-s)) |
| 5.26 | ChatGPT Fails American College of Gastroenterology Assessment Tests ([news](https://healthitanalytics.com/news/chatgpt-fails-american-college-of-gastroenterology-assessment-tests)) |
| 5.26 | Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance ([:x:](https://arxiv.org/abs/2305.17306)), ([:paperclip:](https://arxiv.org/pdf/2305.17306.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.17306)), ([:house:](https://huggingface.co/papers/2305.17306)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chain-of-thought-hub-a-continuous-effort-to)), ([:octocat:](https://github.com/franxyao/chain-of-thought-hub)![GitHub Repo stars](https://img.shields.io/github/stars/franxyao/chain-of-thought-hub?style=social)) |
| 5.26 | A new antibiotic, discovered with artificial intelligence, may defeat a dangerous superbug ([CNN news](https://edition.cnn.com/2023/05/25/health/antibiotic-artificial-intelligence-superbug/index.html)) |
| 5.26 | Generating Images with Multimodal Language Models ([:x:](https://arxiv.org/abs/2305.17216)), ([:paperclip:](https://arxiv.org/pdf/2305.17216.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.17216)), ([:house:](https://huggingface.co/papers/2305.17216)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/generating-images-with-multimodal-language)), ([:octocat:](https://github.com/kohjingyu/gill)![GitHub Repo stars](https://img.shields.io/github/stars/kohjingyu/gill?style=social)) |
| 5.26 | Backpack Language Models ([:x:](https://arxiv.org/abs/2305.16765)), ([:paperclip:](https://arxiv.org/pdf/2305.16765.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.16765)), ([:house:](https://huggingface.co/papers/2305.16765)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/backpack-language-models)) |
| 5.26 | Impossible Distillation: from Low-Quality Model to High-Quality Dataset & Model for Summarization and Paraphrasing ([:x:](https://arxiv.org/abs/2305.16635)), ([:paperclip:](https://arxiv.org/pdf/2305.16635.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.16635)), ([:house:](https://huggingface.co/papers/2305.16635)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/impossible-distillation-from-low-quality)) |
| 5.26 | Playing repeated games with Large Language Models ([:x:](https://arxiv.org/abs/2305.16867)), ([:paperclip:](https://arxiv.org/pdf/2305.16867.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.16867)), ([:house:](https://huggingface.co/papers/2305.16867)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/playing-repeated-games-with-large-language)) |
| 5.26 | Training Socially Aligned Language Models in Simulated Human Society ([:x:](https://arxiv.org/abs/2305.16960)), ([:paperclip:](https://arxiv.org/pdf/2305.16960.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.16960)), ([:house:](https://huggingface.co/papers/2305.16960)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/training-socially-aligned-language-models-in)) |
| 5.26 | BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks ([:x:](https://arxiv.org/abs/2305.17100)), ([:paperclip:](https://arxiv.org/pdf/2305.17100.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.17100)), ([:house:](https://huggingface.co/papers/2305.17100)) |
| 5.26 | Large Language Models as Tool Makers ([:x:](https://arxiv.org/abs/2305.17126)), ([:paperclip:](https://arxiv.org/pdf/2305.17126.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.17126)), ([:house:](https://huggingface.co/papers/2305.17126)) |
| 5.26 | ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation  ([:x:](https://arxiv.org/abs/2305.16213)), ([:paperclip:](https://arxiv.org/pdf/2305.16213.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.16213)), ([:house:](https://huggingface.co/papers/2305.16213)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/prolificdreamer-high-fidelity-and-diverse)), ([project page](https://ml.cs.tsinghua.edu.cn/prolificdreamer/)) |
| 5.25 | ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs  ([:x:](https://arxiv.org/abs/2305.15964)), ([:paperclip:](https://arxiv.org/pdf/2305.15964.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.15964)), ([:house:](https://huggingface.co/papers/2305.15964)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chatcad-towards-a-universal-and-reliable)), ([:octocat:](https://github.com/zhaozh10/ChatCAD)![GitHub Repo stars](https://img.shields.io/github/stars/zhaozh10/ChatCAD?style=social)) |
| 5.25 | The Current and Future State of AI Interpretation of Medical Images (NEJM, [DOI: 10.1056/NEJMra2301725](https://www.nejm.org/doi/full/10.1056/NEJMra2301725)) |
| 5.25 | Deep learning-guided discovery of an antibiotic targeting Acinetobacter baumannii, (nature chemical biology [https://doi.org/10.1038/s41589-023-01349-8](https://www.nature.com/articles/s41589-023-01349-8)), ([:octocat:](https://github.com/chemprop/chemprop)![GitHub Repo stars](https://img.shields.io/github/stars/chemprop/chemprop?style=social)), ([Cloned snapshot](https://github.com/GaryLiu152/chemprop_abaucin)) |
| 5.25 | Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory ([:x:](https://arxiv.org/abs/2305.17144)), ([:paperclip:](https://arxiv.org/pdf/2305.17144.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.17144)), ([:house:](https://huggingface.co/papers/2305.17144)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ghost-in-the-minecraft-generally-capable)), ([:octocat:](https://github.com/opengvlab/gitm)![GitHub Repo stars](https://img.shields.io/github/stars/opengvlab/gitm?style=social)) |
| 5.25 | Role-Play with Large Language Models ([:x:](https://arxiv.org/abs/2305.16367)), ([:paperclip:](https://arxiv.org/pdf/2305.16367.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.16367)), ([:house:](https://huggingface.co/papers/2305.16367)) |
| 5.25 | Break-A-Scene: Extracting Multiple Concepts from a Single Image ([:x:](https://arxiv.org/abs/2305.16311)), ([:paperclip:](https://arxiv.org/pdf/2305.16311.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.16311)), ([:house:](https://huggingface.co/papers/2305.16311)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/break-a-scene-extracting-multiple-concepts)) |
| 5.25 | Voyager: An Open-Ended Embodied Agent with Large Language Models ([Project page](https://voyager.minedojo.org/)), ([:x:](https://arxiv.org/abs/2305.16291)), ([:paperclip:](https://arxiv.org/pdf/2305.16291.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.16291)), ([:house:](https://huggingface.co/papers/2305.16291)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/voyager-an-open-ended-embodied-agent-with)), ([:octocat:](https://github.com/MineDojo/Voyager)![GitHub Repo stars](https://img.shields.io/github/stars/MineDojo/Voyager?style=social)), ([MindDojo](https://minedojo.org/)) |
| 5.25 | Efficient Neural Music Generation ([:x:](https://arxiv.org/abs/2305.15719)), ([:paperclip:](https://arxiv.org/pdf/2305.15719.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.15719)), ([:house:](https://huggingface.co/papers/2305.15719)) |
| 5.25 | Custom-Edit: Text-Guided Image Editing with Customized Diffusion Models ([:x:](https://arxiv.org/abs/2305.15779)), ([:paperclip:](https://arxiv.org/pdf/2305.15779.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.15779)), ([:house:](https://huggingface.co/papers/2305.15779)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/custom-edit-text-guided-image-editing-with)) |
| 5.25 | On Architectural Compression of Text-to-Image Diffusion Models ([:x:](https://arxiv.org/abs/2305.15798)), ([:paperclip:](https://arxiv.org/pdf/2305.15798.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.15798)), ([:house:](https://huggingface.co/papers/2305.15798)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/on-architectural-compression-of-text-to-image)) |
| 5.25 | Prompt-Free Diffusion: Taking "Text" out of Text-to-Image Diffusion Models ([:x:](https://arxiv.org/abs/2305.16223)), ([:paperclip:](https://arxiv.org/pdf/2305.16223.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.16223)), ([:house:](https://huggingface.co/papers/2305.16223)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/prompt-free-diffusion-taking-text-out-of-text)), ([:octocat:](https://github.com/SHI-Labs/Prompt-Free-Diffusion)![GitHub Repo stars](https://img.shields.io/github/stars/SHI-Labs/Prompt-Free-Diffusion?style=social)) |
| 5.25 | The False Promise of Imitating Proprietary LLMs ([:x:](https://arxiv.org/abs/2305.15717)), ([:paperclip:](https://arxiv.org/pdf/2305.15717.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.15717)), ([:house:](https://huggingface.co/papers/2305.15717)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-false-promise-of-imitating-proprietary)) |
| 5.25 | the new Stable Diffusion “Reimagine XL” model on @ClipdropApp x @StabilityAI ([tweet](https://twitter.com/hardmaru/status/1661739577395286022)), ([Clipdrop](https://clipdrop.co/stable-diffusion-reimagine)) |
| 5.25 | Gorilla: Large Language Model Connected with Massive APIs ([tweet](https://twitter.com/shishirpatil_/status/1661780076277678082)), ([project page](https://gorilla.cs.berkeley.edu/)), ([:x:](https://arxiv.org/abs/2305.15334)), ([:paperclip:](https://arxiv.org/pdf/2305.15334.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.15334)), ([:house:](https://huggingface.co/papers/2305.15334)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gorilla-large-language-model-connected-with)), ([:octocat:](https://github.com/ShishirPatil/gorilla)![GitHub Repo stars](https://img.shields.io/github/stars/ShishirPatil/gorilla?style=social)), ([demo video](https://drive.google.com/file/d/1E0k5mG1mTiaz0kukyK1PdeohJipTFh6j/view)), ([discord](https://discord.com/invite/3apqwwME)) |
| 5.25 | OpenAI - Democratic Inputs to AI ([Tweet](https://twitter.com/OpenAI/status/1661811329957781504)), ([Blog](https://openai.com/blog/democratic-inputs-to-ai)) |
| 5.24 | Reasoning with Language Model is Planning with World Model ([:x:](https://arxiv.org/abs/2305.14992)), ([:book:](https://browse.arxiv.org/pdf/2305.14992.pdf)), ([:paperclip:](https://arxiv.org/pdf/2305.14992.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.14992)), ([:house:](https://huggingface.co/papers/2305.14992)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/reasoning-with-language-model-is-planning)), ([:octocat:](https://github.com/ber666/llm-reasoners)![GitHub Repo stars](https://img.shields.io/github/stars/ber666/llm-reasoners?style=social))  |
| 5.24 | Large Language Models are Few-Shot Health Learners ([:x:](https://arxiv.org/abs/2305.15525)), ([:paperclip:](https://arxiv.org/pdf/2305.15525.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.15525)), ([:house:](https://huggingface.co/papers/2305.15525)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-are-few-shot-health)) |
| 5.24 | HuatuoGPT, towards Taming Language Model to Be a Doctor ([:x:](https://arxiv.org/abs/2305.15075)), ([:paperclip:](https://arxiv.org/pdf/2305.15075.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.15075)), ([:house:](https://huggingface.co/papers/2305.15075)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/huatuogpt-towards-taming-language-model-to-be)), ([:octocat:](https://github.com/freedomintelligence/huatuogpt)![GitHub Repo stars](https://img.shields.io/github/stars/freedomintelligence/huatuogpt?style=social)) |
| 5.24 | Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models ([:x:](https://arxiv.org/abs/2305.14710)), ([:paperclip:](https://arxiv.org/pdf/2305.14710.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.14710)), ([:house:](https://huggingface.co/papers/2305.14710)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/instructions-as-backdoors-backdoor)) |
| 5.24 | Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models ([:x:](https://arxiv.org/abs/2305.15074)), ([:paperclip:](https://arxiv.org/pdf/2305.15074.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.15074)), ([:house:](https://huggingface.co/papers/2305.15074)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/have-llms-advanced-enough-a-challenging)) |
| 5.24 | Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models ([:x:](https://arxiv.org/abs/2305.14763)), ([:paperclip:](https://arxiv.org/pdf/2305.14763.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.14763)), ([:house:](https://huggingface.co/papers/2305.14763)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/clever-hans-or-neural-theory-of-mind-stress)) |
| 5.24 | A majority of Americans have heard of ChatGPT, but few have tried it themselves ([Pew Research Center news](https://www.pewresearch.org/short-reads/2023/05/24/a-majority-of-americans-have-heard-of-chatgpt-but-few-have-tried-it-themselves/)) |
| 5.24 | Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective ([:x:](https://arxiv.org/abs/2305.16338)), ([:paperclip:](https://arxiv.org/pdf/2305.16338.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.16338)), ([:house:](https://huggingface.co/papers/2305.16338)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-revealing-the-mystery-behind-chain-of)) |
| 5.24 | Think Before You Act: Decision Transformers with Internal Working Memory ([:x:](https://arxiv.org/abs/2305.15408)), ([:paperclip:](https://arxiv.org/pdf/2305.15408.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.15408)), ([:house:](https://huggingface.co/papers/2305.15408)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/think-before-you-act-decision-transformers)) |
| 5.24 | PandaGPT: One Model to Instruction-Follow Them All ([project page](https://panda-gpt.github.io/)), ([:paperclip:](https://github.com/yxuansu/PandaGPT/blob/main/PandaGPT.pdf)), ([demo](https://huggingface.co/spaces/GMFTBY/PandaGPT)), ([video](https://www.youtube.com/watch?v=96XgdQle7EY)), ([dataset](https://huggingface.co/datasets/openllmplayground/pandagpt_visual_instruction_dataset)), ([model](https://huggingface.co/openllmplayground/pandagpt_13b_max_len_400)), ([:octocat:](https://github.com/yxuansu/PandaGPT)![GitHub Repo stars](https://img.shields.io/github/stars/yxuansu/PandaGPT?style=social)), ([tweet](https://twitter.com/yixuan_su/status/1661064018868551691)) |
| 5.24 | SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning ([:x:](https://arxiv.org/abs/2305.15486)), ([:paperclip:](https://arxiv.org/pdf/2305.15486.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.15486)), ([:house:](https://huggingface.co/papers/2305.15486)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/spring-gpt-4-out-performs-rl-algorithms-by)) |
| 5.24 | Manifold Diffusion Fields ([:x:](https://arxiv.org/abs/2305.15586)), ([:paperclip:](https://arxiv.org/pdf/2305.15586.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.15586)), ([:house:](https://huggingface.co/papers/2305.15586)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/manifold-diffusion-fields)) |
| 5.24 | A Neural Space-Time Representation for Text-to-Image Personalization ([:x:](https://arxiv.org/abs/2305.15391)), ([:paperclip:](https://arxiv.org/pdf/2305.15391.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.15391)), ([:house:](https://huggingface.co/papers/2305.15391)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-neural-space-time-representation-for-text)), ([:octocat:](https://github.com/NeuralTextualInversion/NeTI)![GitHub Repo stars](https://img.shields.io/github/stars/NeuralTextualInversion/NeTI?style=social)) |
| 5.24 | Can Transformers Learn to Solve Problems Recursively? ([:x:](https://arxiv.org/abs/2305.14699)), ([:paperclip:](https://arxiv.org/pdf/2305.14699.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.14699)), ([:house:](https://huggingface.co/papers/2305.14699)) |
| 5.24 | This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language Models ([:x:](https://arxiv.org/abs/2305.14610)), ([:paperclip:](https://arxiv.org/pdf/2305.14610.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.14610)), ([:house:](https://huggingface.co/papers/2305.14610)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/this-land-is-your-my-land-evaluating)) |
| 5.24 | Model evaluation for extreme risks ([:x:](https://arxiv.org/abs/2305.15324)), ([:paperclip:](https://arxiv.org/pdf/2305.15324.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.15324)), ([:house:](https://huggingface.co/papers/2305.15324)) |
| 5.24 | State of GPT and RLHF LLMs - Andrej Karpathy, OpenAI ([session](https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2)), ([video](https://mediusdownload.event.microsoft.com/video-51378/ea11c3c20e/BRK216HFS_v1.mp4?sv=2018-03-28&sr=c&sig=tlIPwp2z6q8TNAEig%2BOQGh4lL8o8hAHcdw33msvikXY%3D&se=2028-05-24T06%3A23%3A01Z&sp=r)) |
| 5.24 | LMs with a Voice: Spoken Language Modeling beyond Speech Tokens ([:x:](https://arxiv.org/abs/2305.15255)), ([:paperclip:](https://arxiv.org/pdf/2305.15255.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.15255)), ([:house:](https://huggingface.co/papers/2305.15255)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/blip-diffusion-pre-trained-subject)), ([:octocat:](https://paperswithcode.com/paper/lms-with-a-voice-spoken-language-modeling)) |
| 5.24 | BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing ([:x:](https://arxiv.org/abs/2305.14720)), ([:paperclip:](https://arxiv.org/pdf/2305.14720.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.14720)), ([:house:](https://huggingface.co/papers/2305.14720)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/blip-diffusion-pre-trained-subject)), ([:octocat:](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion)), ([Project page](https://dxli94.github.io/BLIP-Diffusion-website/)) |
| 5.23 | Threats by artificial intelligence to human health and human existence (BMJ, [http://dx.doi.org/10.1136/bmjgh-2022-010435](https://gh.bmj.com/content/8/5/e010435)), ([PDF](https://gh.bmj.com/content/bmjgh/8/5/e010435.full.pdf)) |
| 5.23 | Transformer-based Vulnerability Detection in Code at EditTime: Zero-shot, Few-shot, or Fine-tuning? [:x:](https://arxiv.org/abs/2306.01754)), ([:paperclip:](https://arxiv.org/pdf/2306.01754.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.01754)), ([:house:](https://huggingface.co/papers/2306.01754)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/transformer-based-vulnerability-detection-in)) |
| 5.23 | Unity’s Project Barracuda Injects Generative AI Into Games To Kickstart Exponential Growth ([Forbes news](https://www.forbes.com/sites/johnkoetsier/2023/05/23/unitys-project-barracuda-injects-generative-ai-into-games-to-kickstart-exponential-growth/?sh=5b2154b3703a)) |
| 5.23 | VisorGPT: Learning Visual Prior via Generative Pre-Training ([:x:](https://arxiv.org/abs/2305.13777)), ([:paperclip:](https://arxiv.org/pdf/2305.13777.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.13777)), ([:house:](https://huggingface.co/papers/2305.13777)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/visorgpt-learning-visual-prior-via-generative)), ([:octocat:](https://github.com/sierkinhane/visorgpt)![GitHub Repo stars](https://img.shields.io/github/stars/sierkinhane/visorgpt?style=social)) |
| 5.23 | ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models ([:x:](https://arxiv.org/abs/2305.18323)), ([:paperclip:](https://arxiv.org/pdf/2305.18323.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.18323)), ([:house:](https://huggingface.co/papers/2305.18323)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rewoo-decoupling-reasoning-from-observations)) |
| 5.23 | OlaGPT: Empowering LLMs With Human-like Problem-Solving Abilities ([:x:](https://arxiv.org/abs/2305.16334)), ([:paperclip:](https://arxiv.org/pdf/2305.16334.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.16334)), ([:house:](https://huggingface.co/papers/2305.16334)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/olagpt-empowering-llms-with-human-like)) |
| 5.23 | Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks ([:x:](https://arxiv.org/abs/2305.14201)), ([:paperclip:](https://arxiv.org/pdf/2305.14201.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.14201)), ([:house:](https://huggingface.co/papers/2305.14201)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/goat-fine-tuned-llama-outperforms-gpt-4-on)) |
| 5.23 | Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach ([:x:](https://arxiv.org/abs/2305.13579)), ([:paperclip:](https://arxiv.org/pdf/2305.13579.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.13579)), ([:house:](https://huggingface.co/papers/2305.13579)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/enhancing-detail-preservation-for-customized)) |
| 5.23 | Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models ([:x:](https://arxiv.org/abs/2305.13840)), ([:paperclip:](https://arxiv.org/pdf/2305.13840.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.13840)), ([:house:](https://huggingface.co/papers/2305.13840)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/control-a-video-controllable-text-to-video)), ([:octocat:](https://github.com/Weifeng-Chen/control-a-video)![GitHub Repo stars](https://img.shields.io/github/stars/Weifeng-Chen/control-a-video?style=social))|
| 5.23 | Aligning Large Language Models through Synthetic Feedback ([:x:](https://arxiv.org/abs/2305.13735)), ([:paperclip:](https://arxiv.org/pdf/2305.13735.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.13735)), ([:house:](https://huggingface.co/papers/2305.13735)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/aligning-large-language-models-through)) |
| 5.23 | LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond ([:x:](https://arxiv.org/abs/2305.14540)), ([:paperclip:](https://arxiv.org/pdf/2305.14540.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.14540)), ([:house:](https://huggingface.co/papers/2305.14540)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llms-as-factual-reasoners-insights-from)), ([:octocat:](https://github.com/salesforce/factualnlg)![GitHub Repo stars](https://img.shields.io/github/stars/salesforce/factualnlg?style=social)) |
| 5.23 | Lost in Translation: Large Language Models in Non-English Content Analysis ([news](https://cdt.org/insights/lost-in-translation-large-language-models-in-non-english-content-analysis)) |
| 5.23 | Anchor Prediction: Automatic Refinement of Internet Links ([:x:](https://arxiv.org/abs/2305.14337)), ([:paperclip:](https://arxiv.org/pdf/2305.14337.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.14337)), ([:house:](https://huggingface.co/papers/2305.14337)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/anchor-prediction-automatic-refinement-of)) |
| 5.23 | Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality ([:x:](https://arxiv.org/abs/2305.13812)), ([:paperclip:](https://arxiv.org/pdf/2305.13812.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.13812)), ([:house:](https://huggingface.co/papers/2305.13812)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/coarse-to-fine-contrastive-learning-in-image)) |
| 5.23 | Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training ([:x:](https://arxiv.org/abs/2305.14342)), ([:paperclip:](https://arxiv.org/pdf/2305.14342.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.14342)), ([:house:](https://huggingface.co/papers/2305.14342)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sophia-a-scalable-stochastic-second-order)) |
| 5.23 | PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents ([:x:](https://arxiv.org/abs/2305.14564)), ([:paperclip:](https://arxiv.org/pdf/2305.14564.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.14564)), ([:house:](https://huggingface.co/papers/2305.14564)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pearl-prompting-large-language-models-to-plan)) |
| 5.23 | Bing at Microsoft Build 2023: Continuing the Transformation of Search ([blog](https://blogs.bing.com/search/may_2023/Bing-at-Microsoft-Build-2023)) |
| 5.23 | Bringing the power of AI to Windows 11 – unlocking a new era of productivity for customers and developers with Windows Copilot and Dev Home ([blog](https://blogs.windows.com/windowsdeveloper/2023/05/23/bringing-the-power-of-ai-to-windows-11-unlocking-a-new-era-of-productivity-for-customers-and-developers-with-windows-copilot-and-dev-home/)) |
| 5.23 | Adobe Unveils Future of Creative Cloud With Generative AI as a Creative Co-Pilot in Photoshop ([news](https://news.adobe.com/news/news-details/2023/Adobe-Unveils-Future-of-Creative-Cloud-with-Generative-AI-as-a-Creative-Co-Pilot-in-Photoshop-default.aspx/default.aspx)), ([blog](https://blog.adobe.com/en/publish/2023/05/23/photoshop-new-features-ai-contextual-presets)) |
| 5.23 | QLoRA: Efficient Finetuning of Quantized LLMs ([:x:](https://arxiv.org/abs/2305.14314)), ([:paperclip:](https://arxiv.org/pdf/2305.14314.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.14314)), ([:house:](https://huggingface.co/papers/2305.14314)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/qlora-efficient-finetuning-of-quantized-llms)), ([:octocat:](https://github.com/artidoro/qlora)![GitHub Repo stars](https://img.shields.io/github/stars/artidoro/qlora?style=social)) |
| 5.22 | A Study of Generative Large Language Model for Medical Research and Healthcare ([:x:](https://arxiv.org/abs/2305.13523)), ([:paperclip:](https://arxiv.org/pdf/2305.13523.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.13523)), , ([:house:](https://huggingface.co/papers/2305.13523)),([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-study-of-generative-large-language-model)) |
| 5.22 | Large-language-model-based 10-year risk prediction of cardiovascular disease: insight from the UK biobank data ([medRxiv](https://www.medrxiv.org/content/10.1101/2023.05.22.23289842v1)), ([SS](https://www.semanticscholar.org/paper/Large-language-model-based-10-year-risk-prediction-Han-Kim/8f7d91fb227a1578f520df23b9d2e105ab30395f)) |
| 5.22 | SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation ([:x:](https://arxiv.org/abs/2305.13194)), ([:paperclip:](https://arxiv.org/pdf/2305.13194.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.13194)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/seahorse-a-multilingual-multifaceted-dataset)) |
| 5.22 | Meta-in-context learning in large language models  ([:x:](https://arxiv.org/abs/2305.12907)), ([:paperclip:](https://arxiv.org/pdf/2305.12907.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.12907)), ([:house:](https://huggingface.co/papers/2305.12647)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/meta-in-context-learning-in-large-language)) |
| 5.22 | AudioToken: Adaptation of Text-Conditioned Diffusion Models for Audio-to-Image Generation ([:x:](https://arxiv.org/abs/2305.13050)), ([:paperclip:](https://arxiv.org/pdf/2305.13050.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.13050)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/audiotoken-adaptation-of-text-conditioned-1)), ([:octocat:](https://github.com/guyyariv/AudioToken)![GitHub Repo stars](https://img.shields.io/github/stars/guyyariv/AudioToken?style=social)) |
| 5.22 | Iterative Forward Tuning Boosts In-context Learning in Language Models  ([:x:](https://arxiv.org/abs/2305.13016)), ([:paperclip:](https://arxiv.org/pdf/2305.13016.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.13016)), ([:house:](https://huggingface.co/papers/2305.13016)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/iterative-forward-tuning-boosts-in-context)) |
| 5.22 | How Language Model Hallucinations Can Snowball  ([:x:](https://arxiv.org/abs/2305.13534)), ([:paperclip:](https://arxiv.org/pdf/2305.13534.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.13534)), ([:house:](https://huggingface.co/papers/2305.13534)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/how-language-model-hallucinations-can)), ([demo](https://huggingface.co/spaces/huybery/deep-thinking)) |
| 5.22 | Intel Announces Aurora genAI, Generative AI Model With 1 Trillion Parameters ([news](https://wccftech.com/intel-aurora-genai-chatgpt-competitor-generative-ai-model-with-1-trillion-parameters/)), ([Intel newsroom](https://www.intel.com/content/www/us/en/newsroom/news/intel-delivers-ai-accelerated-hpc-performance.html#gs.ymzdf9)) |
| 5.22 | Introducing Mind-Video ([Tweet](https://twitter.com/ZijiaoC/status/1660470518569639937)), ([demo](https://mind-video.com/)), ([data](https://drive.google.com/drive/folders/1swYQD-69phlJUz4_HmdM0RFk_7okLK4v)) |
| 5.22 | Reflective Linguistic Programming (RLP): A Stepping Stone in Socially-Aware AGI (SocialAGI) ([:x:](https://arxiv.org/abs/2305.12647)), ([:paperclip:](https://arxiv.org/pdf/2305.12647.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.12647)), ([:house:](https://huggingface.co/papers/2305.12647)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/reflective-linguistic-programming-rlp-a)) |
| 5.22 | GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints ([:x:](https://arxiv.org/abs/2305.13245)), ([:paperclip:](https://arxiv.org/pdf/2305.13245.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.13245)), ([:house:](https://huggingface.co/papers/2305.13245)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gqa-training-generalized-multi-query)) |
| 5.22 | LM vs LM: Detecting Factual Errors via Cross Examination ([:x:](https://arxiv.org/abs/2305.13281)), ([:paperclip:](https://arxiv.org/pdf/2305.13281.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.13281)), ([:house:](https://huggingface.co/papers/2305.13281)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lm-vs-lm-detecting-factual-errors-via-cross)) |
| 5.22 | XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages ([:paperclip:](https://storage.googleapis.com/xtreme-up/xtreme-up.pdf)), ([:octocat:](https://github.com/google-research/xtreme-up)![GitHub Repo stars](https://img.shields.io/github/stars/google-research/xtreme-up?style=social)) |
| 5.22 | VideoLLM: Modeling Video Sequence with Large Language Models ([:x:](https://arxiv.org/abs/2305.13292)), ([:paperclip:](https://arxiv.org/pdf/2305.13292.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.13292)), ([:house:](https://huggingface.co/papers/2305.13292)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/videollm-modeling-video-sequence-with-large)) |
| 5.22 | RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text ([:x:](https://arxiv.org/abs/2305.13304)), ([:paperclip:](https://arxiv.org/pdf/2305.13304.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.13304)), ([:house:](https://huggingface.co/papers/2305.13304)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/recurrentgpt-interactive-generation-of)) |
| 5.22 | RWKV: Reinventing RNNs for the Transformer Era  ([:x:](https://arxiv.org/abs/2305.13048)), ([:paperclip:](https://arxiv.org/pdf/2305.13048.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.13048)), ([:house:](https://huggingface.co/papers/2305.13048)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rwkv-reinventing-rnns-for-the-transformer-era)) |
| 5.22 | Introducing speech-to-text, text-to-speech, and more for 1,100+ languages ([Blog](https://ai.facebook.com/blog/multilingual-model-speech-recognition/)), ([:paperclip:](https://scontent-nrt1-1.xx.fbcdn.net/v/t39.8562-6/348836647_265923086001014_6878005808275791319_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=ae5e01&_nc_ohc=5exJiCqt0Y4AX-_7kQa&_nc_ht=scontent-nrt1-1.xx&oh=00_AfApkDyQVBqv7V82fFveVwLv3AC7KxmOR1FegeXQkrsPiQ&oe=6471ACCF)), ([:octocat:](https://github.com/facebookresearch/fairseq/tree/main/examples/mms)) | 
| 5.21 | Embracing Large Language Models for Medical Applications: Opportunities and Challenges ([abstract](https://www.cureus.com/articles/149797-embracing-large-language-models-for-medical-applications-opportunities-and-challenges#!/)), ([SS](https://www.semanticscholar.org/paper/Embracing-Large-Language-Models-for-Medical-and-Karabacak-Margetis/6486f0b6e443cb864639d4a85277d71cf69f78e0)) |
| 5.21 | Augmenting Autotelic Agents with Large Language Models ([:x:](https://arxiv.org/abs/2305.12487)), ([:paperclip:](https://arxiv.org/pdf/2305.12487.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.12487)), ([:house:](https://huggingface.co/papers/2305.12487)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/augmenting-autotelic-agents-with-large)) |
| 5.21 | XrayGPT: Chest Radiographs Summarization using Medical Vision-Language Models ([:octocat:](https://github.com/mbzuai-oryx/XrayGPT)![GitHub Repo stars](https://img.shields.io/github/stars/mbzuai-oryx/XrayGPT?style=social)), ([Video](https://www.youtube.com/watch?v=-zzq7bzbUuY&t=31s)) | 
| 5.20 | G7 Hiroshima Leaders’ Communiqué ([statement](https://www.mofa.go.jp/files/100506878.pdf)), ([html](https://www.whitehouse.gov/briefing-room/statements-releases/2023/05/20/g7-hiroshima-leaders-communique/)) |
| 5.20 | G7 calls for developing global technical standards for AI ([news](https://www.reuters.com/world/g7-calls-developing-global-technical-standards-ai-2023-05-20/)) |
| 5.20 | Labour should pledge £11bn to build ‘BritGPT’ AI, thinktank says ([news](https://www.theguardian.com/technology/2023/may/20/labour-should-pledge-11bn-to-build-britgpt-ai-thinktank-says)) |
| 5.20 | CodeCompose: A Large-Scale Industrial Deployment of AI-assisted Code Authoring ([:x:](https://arxiv.org/abs/2305.12050)), ([:paperclip:](https://arxiv.org/pdf/2305.12050.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.12050)), ([:house:](https://huggingface.co/papers/2305.12050)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/codecompose-a-large-scale-industrial)) |
| 5.19 | Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews ([:x:](https://arxiv.org/abs/2305.11828)), ([:paperclip:](https://arxiv.org/pdf/2305.11828.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11828)), ([:house:](https://huggingface.co/papers/2305.11828)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/appraising-the-potential-uses-and-harms-of)) |
| 5.19 | A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation ([:x:](https://arxiv.org/abs/2305.11391)), ([:paperclip:](https://arxiv.org/pdf/2305.11391.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11391)), ([:house:](https://huggingface.co/papers/2305.11391)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-survey-of-safety-and-trustworthiness-of)), ([SS](https://www.semanticscholar.org/paper/A-Survey-of-Safety-and-Trustworthiness-of-Large-the-Huang-Ruan/a723e78473c2f6d096b087e38eb31f6e7800b039)) |
| 5.19 | HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models ([:x:](https://arxiv.org/abs/2305.11747)), ([:paperclip:](https://arxiv.org/pdf/2305.11747.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11747)), ([:house:](https://huggingface.co/papers/2305.11747)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/helma-a-large-scale-hallucination-evaluation)), ([:octocat:](https://github.com/RUCAIBox/HaluEval)![GitHub Repo stars](https://img.shields.io/github/stars/RUCAIBox/HaluEval?style=social)) |
| 5.19 | OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models ([:x:](https://arxiv.org/abs/2305.12001)), ([:paperclip:](https://arxiv.org/pdf/2305.12001.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.12001)), ([:house:](https://huggingface.co/papers/2305.12001)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/opt-r-exploring-the-role-of-explanations-in)) |
| 5.19 | Neural Foundations of Mental Simulation: Future Prediction of Latent Representations on Dynamic Scenes ([:x:](https://arxiv.org/abs/2305.11772)), ([:paperclip:](https://arxiv.org/pdf/2305.11772.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11772)), ([:house:](https://huggingface.co/papers/2305.11772)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/neural-foundations-of-mental-simulation)) |
| 5.19 | Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding ([:x:](https://arxiv.org/abs/2305.12031)), ([:paperclip:](https://arxiv.org/pdf/2305.12031.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.12050)), ([:house:](https://huggingface.co/papers/2305.12050)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/codecompose-a-large-scale-industrial)), ([huggingface](https://huggingface.co/wanglab/clinical-camel)), ([:octocat:](https://github.com/bowang-lab/clinical-camel)![GitHub Repo stars](https://img.shields.io/github/stars/bowang-lab/clinical-camel?style=social)) |
| 5.19 | New York City public schools remove ChatGPT ban ([news](https://www.nbcnews.com/tech/chatgpt-ban-dropped-new-york-city-public-schools-rcna85089)) |
| 5.19 | Graphologue: Exploring Large Language Model Responses with Interactive Diagrams ([:x:](https://arxiv.org/abs/2305.11473)), ([:paperclip:](https://arxiv.org/pdf/2305.11473.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11473)), ([:house:](https://huggingface.co/papers/2305.11473)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/graphologue-exploring-large-language-model)) |
| 5.19 | The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics ([:x:](https://arxiv.org/abs/2305.11806)), ([:paperclip:](https://arxiv.org/pdf/2305.11806.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11806)), ([:house:](https://huggingface.co/papers/2305.11806)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-inside-story-towards-better-understanding)), ([:octocat:](https://github.com/Unbabel/COMET)![GitHub Repo stars](https://img.shields.io/github/stars/Unbabel/COMET?style=social)) |
| 5.19 | HalOmi: A Manually Annotated Benchmark for Multilingual Hallucination and Omission Detection in Machine Translation ([:x:](https://arxiv.org/abs/2305.11746)), ([:paperclip:](https://arxiv.org/pdf/2305.11746.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11746)), ([:house:](https://huggingface.co/papers/2305.11746)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/halomi-a-manually-annotated-benchmark-for)) |
| 5.19 | Characterizing tradeoffs between teaching via language and demonstrations in multi-agent systems ([:x:](https://arxiv.org/abs/2305.11374)), ([:paperclip:](https://arxiv.org/pdf/2305.11374.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11374)), ([:house:](https://huggingface.co/papers/2305.11374)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/characterizing-tradeoffs-between-teaching-via)) |
| 5.19 | TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks ([:x:](https://arxiv.org/abs/2305.11430)), ([:paperclip:](https://arxiv.org/pdf/2305.11430.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11430)), ([:house:](https://huggingface.co/papers/2305.11430)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/teler-a-general-taxonomy-of-llm-prompts-for)) |
| 5.19 | Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields ([:x:](https://arxiv.org/abs/2305.11588)), ([:paperclip:](https://arxiv.org/pdf/2305.11588.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11588)), ([:house:](https://huggingface.co/papers/2305.11588)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/text2nerf-text-driven-3d-scene-generation)) |
| 5.19 | Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models ([:x:](https://arxiv.org/abs/2305.11870)), ([:paperclip:](https://arxiv.org/pdf/2305.11870.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11870)), ([:house:](https://huggingface.co/papers/2305.11870)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chupa-carving-3d-clothed-humans-from-skinned)) |
| 5.19 | Comparing Software Developers with ChatGPT: An Empirical Investigation ([:x:](https://arxiv.org/abs/2305.11837)), ([:paperclip:](https://arxiv.org/pdf/2305.11837.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11837)), ([:house:](https://huggingface.co/papers/2305.11837)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/comparing-software-developers-with-chatgpt-an)) |
| 5.19 | CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing ([:x:](https://arxiv.org/abs/2305.11738)), ([:paperclip:](https://arxiv.org/pdf/2305.11738.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11738)), ([:house:](https://huggingface.co/papers/2305.11738)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/critic-large-language-models-can-self-correct)), ([:octocat:](https://github.com/microsoft/ProphetNet)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/ProphetNet?style=social)) |
| 5.19 | Multimodal Web Navigation with Instruction-Finetuned Foundation Models ([:x:](https://arxiv.org/abs/2305.11854)), ([:paperclip:](https://arxiv.org/pdf/2305.11854.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11854)), ([:house:](https://huggingface.co/papers/2305.11854)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/multimodal-web-navigation-with-instruction)) |
| 5.19 | Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity ([:x:](https://arxiv.org/abs/2305.11675)), ([:paperclip:](https://arxiv.org/pdf/2305.11675.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11675)), ([:house:](https://huggingface.co/papers/2305.11675)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/cinematic-mindscapes-high-quality-video)) | 
| 5.19 | Scaling laws for language encoding models in fMRI ([:x:](https://arxiv.org/abs/2305.11863)), ([:paperclip:](https://arxiv.org/pdf/2305.11863.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11863)), ([:house:](https://huggingface.co/papers/2305.11863)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/scaling-laws-for-language-encoding-models-in)) | 
| 5.19 | Any-to-Any Generation via Composable Diffusion ([:x:](https://arxiv.org/abs/2305.11846)), ([:paperclip:](https://arxiv.org/pdf/2305.11846.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11846)), ([:house:](https://huggingface.co/papers/2305.11846)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/any-to-any-generation-via-composable)), ([:octocat:](https://github.com/microsoft/i-Code/tree/main/i-Code-V3)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/i-Code?style=social)) |
| 5.19 | ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings ([:x:](https://arxiv.org/abs/2305.11554)), ([:paperclip:](https://arxiv.org/pdf/2305.11554.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11554)), ([:house:](https://huggingface.co/papers/2305.11554)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/toolkengpt-augmenting-frozen-language-models)) |
| 5.19 | Apple Bans Employees From Using ChatGPT Amid Its Own AI Efforts ([news](https://www.macrumors.com/2023/05/19/apple-bans-employees-from-using-chatgpt/)) |
| 5.18 | A Framework for Critically Assessing ChatGPT and Other Large Language Artificial Intelligence Model Applications in Health Care ([https://doi.org/10.1016/j.mcpdig.2023.03.006](https://www.mcpdigitalhealth.org/article/S2949-7612(23)00022-6/fulltext)) |
| 5.18 | Brain-inspired learning in artificial neural networks: a review ([:x:](https://arxiv.org/abs/2305.11252)), ([:paperclip:](https://arxiv.org/pdf/2305.11252.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11252)), ([:house:](https://huggingface.co/papers/2305.11252)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/brain-inspired-learning-in-artificial-neural)) |
| 5.18 | ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities ([:x:](https://arxiv.org/abs/2305.11172)), ([:paperclip:](https://arxiv.org/pdf/2305.11172.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11172)), ([:house:](https://huggingface.co/papers/2305.11337)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/one-peace-exploring-one-general)) |
| 5.18 | RoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent Geometry and Texture  ([:x:](https://arxiv.org/abs/2305.11337)), ([:paperclip:](https://arxiv.org/pdf/2305.11337.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11337)), ([:house:](https://huggingface.co/papers/2305.11337)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/roomdreamer-text-driven-3d-indoor-scene)) |
| 5.18 | LIMA: Less Is More for Alignment ([:x:](https://arxiv.org/abs/2305.11206)), ([:paperclip:](https://arxiv.org/pdf/2305.11206.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11206)), ([:house:](https://huggingface.co/papers/2305.11206)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lima-less-is-more-for-alignment)) |
| 5.18 | GETMusic: Generating Any Music Tracks with a Unified Representation and Diffusion Framework ([:x:](https://arxiv.org/abs/2305.10841)), ([:paperclip:](https://arxiv.org/pdf/2305.10841.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.10841)), ([:house:](https://huggingface.co/papers/2305.10841)), ([project page](https://ai-muzic.github.io/getmusic/)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/getmusic-generating-any-music-tracks-with-a)), ([:octocat:](https://github.com/microsoft/muzic)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/muzic?style=social)), ([Star history](https://star-history.com/#microsoft/muzic&Date)) |
| 5.18 | SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities ([:x:](https://arxiv.org/abs/2305.11000)), ([:paperclip:](https://arxiv.org/pdf/2305.11000.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11000)), ([:house:](https://huggingface.co/papers/2305.11000)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/speechgpt-empowering-large-language-models)) |
| 5.18 | mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences ([:x:](https://arxiv.org/abs/2305.11129)), ([:paperclip:](https://arxiv.org/pdf/2305.11129.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11129)), ([:house:](https://huggingface.co/papers/2305.11129)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mlongt5-a-multilingual-and-efficient-text-to)) |
| 5.18 | Language Models Meet World Models: Embodied Experiences Enhance Language Models  ([:x:](https://arxiv.org/abs/2305.10626)), ([:paperclip:](https://arxiv.org/pdf/2305.10626.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.10626)), ([:house:](https://huggingface.co/papers/2305.10626)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/language-models-meet-world-models-embodied)) |
| 5.18 | Roundhill Investments Launches Generative AI & Technology ETF (NYSE Arca: CHAT) ([news](https://www.prnewswire.com/news-releases/roundhill-investments-launches-generative-ai--technology-etf-nyse-arca-chat-301828049.html)), ([CHAT ETF](https://www.marketwatch.com/investing/fund/chat)) |
| 5.18 | VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks ([:x:](https://arxiv.org/abs/2305.11175)), ([:paperclip:](https://arxiv.org/pdf/2305.11175.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11175)), ([:house:](https://huggingface.co/papers/2305.11175)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/visionllm-large-language-model-is-also-an)) |
| 5.18 | Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold ([:x:](https://arxiv.org/abs/2305.10973)), ([:paperclip:](https://arxiv.org/pdf/2305.10973.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.10973)), ([:house:](https://huggingface.co/papers/2305.10973)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/drag-your-gan-interactive-point-based)), ([Huggingfae](https://huggingface.co/spaces/radames/UserControllableLT-Latent-Transformer)), ([Unofficial](https://github.com/Zeqiang-Lai/DragGAN)![GitHub Repo stars](https://img.shields.io/github/stars/Zeqiang-Lai/DragGAN?style=social)), ([colab](https://colab.research.google.com/github/Zeqiang-Lai/DragGAN/blob/master/colab.ipynb)), ([Official](https://github.com/XingangPan/DragGAN)![GitHub Repo stars](https://img.shields.io/github/stars/XingangPan/DragGAN?style=social)) |
| 5.18 | PyLLMs - a minimal Python library to connect to LLMs (OpenAI, Anthropic, Google, AI21, Cohere, Aleph Alpha, HuggingfaceHub) ([:octocat:](https://github.com/kagisearch/pyllms)![GitHub Repo stars](https://img.shields.io/github/stars/kagisearch/pyllms?style=social))
| 5.18 | Evidence of Meaning in Language Models Trained on Programs ([:x:](https://arxiv.org/abs/2305.11169)), ([:paperclip:](https://arxiv.org/pdf/2305.11169.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.11169)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evidence-of-meaning-in-language-models)) |
| 5.18 | Introducing the ChatGPT app for iOS ([blog](https://openai.com/blog/introducing-the-chatgpt-app-for-ios)), ([Download on the App Stor](https://apps.apple.com/app/openai-chatgpt/id6448311069)) |
| 5.18 | MTIA v1: Meta’s first-generation AI inference accelerator ([blog](https://ai.facebook.com/blog/meta-training-inference-accelerator-AI-MTIA/)) |
| 5.18 | Pursuing groundbreaking scale and accelerating research using Meta’s Research SuperCluster ([blog](https://ai.facebook.com/blog/supercomputer-meta-research-supercluster-2023/)) |
| 5.18 | Reimagining Meta’s infrastructure for the AI age ([blog](https://ai.facebook.com/blog/meta-ai-infrastructure-overview/)) |
| 5.17 | Evaluating Object Hallucination in Large Vision-Language Models ([:x:](https://arxiv.org/abs/2305.10355)), ([:paperclip:](https://arxiv.org/pdf/2305.10355.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.10355)), ([:house:](https://huggingface.co/papers/2305.10355)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evaluating-object-hallucination-in-large)) |
| 5.17 | Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models ([:x:](https://arxiv.org/abs/2305.10276)), ([:paperclip:](https://arxiv.org/pdf/2305.10276.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.10276)), ([:house:](https://huggingface.co/papers/2305.10276)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chain-of-symbol-prompting-elicits-planning-in)) |
| 5.17 | DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining ([:x:](https://arxiv.org/abs/2305.10429)), ([:paperclip:](https://arxiv.org/pdf/2305.10429.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.10429)), ([:house:](https://huggingface.co/papers/2305.10429)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/doremi-optimizing-data-mixtures-speeds-up)) |
| 5.17 | Explaining black box text modules in natural language with language models ([:x:](https://arxiv.org/abs/2305.09863)), ([:paperclip:](https://arxiv.org/pdf/2305.09863.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.09863)), ([:house:](https://huggingface.co/papers/2305.09863)), ([project page](https://ai-muzic.github.io/getmusic/)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/explaining-black-box-text-modules-in-natural) |
| 5.17 | Tree of Thoughts: Deliberate Problem Solving with Large Language Models ([:x:](https://arxiv.org/abs/2305.10601)), ([:paperclip:](https://arxiv.org/pdf/2305.10601.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.10601)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tree-of-thoughts-deliberate-problem-solving)) |
| 5.17 | Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback  ([:x:](https://arxiv.org/abs/2305.10142)), ([:paperclip:](https://arxiv.org/pdf/2305.10142.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.10142)), ([:eight_spoked_asterisk:](https://paperswithcode.com/search?q_meta=&q_type=&q=%09Improving%20Language%20Model%20Negotiation%20with%20Self-Play%20and%20In-Context%20Learning%20from%20AI%20Feedback)) |
| 5.17 | PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering ([:x:](https://arxiv.org/abs/2305.10415)), ([:paperclip:](https://arxiv.org/pdf/2305.10415.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.10415)), ([:trophy:papers with code](https://paperswithcode.com/paper/pmc-vqa-visual-instruction-tuning-for-medical)) |
| 5.17 | What You See is What You Read? Improving Text-Image Alignment Evaluation ([:x:](https://arxiv.org/abs/2305.10400)), ([:paperclip:](https://arxiv.org/pdf/2305.10400.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.10400)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/what-you-see-is-what-you-read-improving-text)) |
| 5.17 | PaLM 2 Technical Report  ([:x:](https://arxiv.org/abs/2305.10403)), ([:paperclip:](https://arxiv.org/pdf/2305.10403.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.10403)), ([:trophy:papers with code](https://paperswithcode.com/paper/palm-2-technical-report)) |
| 5.17 | Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback ([:x:](https://arxiv.org/abs/2305.10142)), ([:paperclip:](https://arxiv.org/pdf/2305.10142.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.10142)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/improving-language-model-negotiation-with)) |
| 5.17 | SoundStorm: Efficient Parallel Audio Generation ([:x:](https://arxiv.org/abs/2305.09636)), ([:paperclip:](https://arxiv.org/pdf/2305.09636.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.09636)), ([Project page](https://google-research.github.io/seanet/soundstorm/examples/)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/soundstorm-efficient-parallel-audio)) | 
| 5.16 | GPT-4 in Radiology: Improvements in Advanced Reasoning (RSNA Radiology, [https://doi.org/10.1148/radiol.230987](https://pubs.rsna.org/doi/10.1148/radiol.230987)) |
| 5.16 | Performance of ChatGPT on a Radiology Board-style Examination: Insights into Current Strengths and Limitations (RSNA Radiology, [https://doi.org/10.1148/radiol.230582](https://pubs.rsna.org/doi/10.1148/radiol.230582)) |
| 5.16 | AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation ([:x:](https://arxiv.org/abs/2305.09515)), ([:paperclip:](https://arxiv.org/pdf/2305.09515.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.09515)) |
| 5.16 | Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation ([:x:](https://arxiv.org/abs/2305.09662)), ([:paperclip:](https://arxiv.org/pdf/2305.09662.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.09662)) |
| 5.16 | ChatGPT versus human in generating medical graduate exam questions – An international prospective study ([medRxiv](https://www.medrxiv.org/content/10.1101/2023.05.13.23289943v1)), ([:paperclip:](https://www.medrxiv.org/content/10.1101/2023.05.13.23289943v1.full.pdf)) |
| 5.16 | Understanding 3D Object Interaction from a Single Image ([:x:](https://arxiv.org/abs/2305.09664)), ([:paperclip:](https://arxiv.org/pdf/2305.09664.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.09664)), ([project page](https://jasonqsy.github.io/3DOI/)), ([demo](https://huggingface.co/spaces/shengyi-qian/3DOI)), ([video](https://www.youtube.com/watch?v=YDIL93XxHyk)), ([:octocat:](https://github.com/JasonQSY/3DOI)) |
| 5.16 | StructGPT: A General Framework for Large Language Model to Reason over Structured Data ([:x:](https://arxiv.org/abs/2305.09645)), ([:paperclip:](https://arxiv.org/pdf/2305.09645.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.09645)), ([:eight_spoked_asterisk:](https://paperswithcode.com/search?q_meta=&q_type=&q=%09StructGPT%3A%20A%20General%20Framework%20for%20Large%20Language%20Model%20to%20Reason%20over%20Structured%20Data)) |
| 5.16 | FitMe: Deep Photorealistic 3D Morphable Model Avatars ([:x:](https://arxiv.org/abs/2305.09641)), ([:paperclip:](https://arxiv.org/pdf/2305.09641.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.09641)), ([project page](https://alexlattas.com/fitme)) |
| 5.16 | Pre-Training to Learn in Context ([:x:](https://arxiv.org/abs/2305.09137)), ([:paperclip:](https://arxiv.org/pdf/2305.09137.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.09137)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pre-training-to-learn-in-context)) |
| 5.16 | Towards Expert-Level Medical Question Answering with Large Language Models ([:x:](https://arxiv.org/abs/2305.09617)), ([:paperclip:](https://arxiv.org/pdf/2305.09617.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.09617)), ([:trophy:papers with code](https://paperswithcode.com/paper/towards-expert-level-medical-question)) |
| 5.16 | GPTeam: Collaborative AI Agents  ([:octocat:](https://github.com/101dotxyz/GPTeam)![GitHub Repo stars](https://img.shields.io/github/stars/101dotxyz/GPTeam?style=social)) |
| 5.16 | WATCH LIVE: OpenAI CEO Sam Altman testifies on artificial intelligence before Senate committee ([Youtube](https://www.youtube.com/watch?v=P_ACcQxJIsg)) |
| 5.16 | NYT - [Microsoft Says New A.I. Shows Signs of Human Reasoning](https://www.nytimes.com/2023/05/16/technology/microsoft-ai-human-reasoning.html) | 
| 5.15 | Common Diffusion Noise Schedules and Sample Steps are Flawed ([:x:](https://arxiv.org/abs/2305.08891)), ([:paperclip:](https://arxiv.org/pdf/2305.08891.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.08891)) |
| 5.15 | Symbol tuning improves in-context learning in language models ([:x:](https://arxiv.org/abs/2305.08298)), ([:paperclip:](https://arxiv.org/pdf/2305.08298.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.08298)) |
| 5.15 | Interpretability at Scale: Identifying Causal Mechanisms in Alpaca ([:x:](https://arxiv.org/abs/2305.08809)), ([:paperclip:](https://arxiv.org/pdf/2305.08809.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.08809)) |
| 5.15 | DarkBERT: A Language Model for the Dark Side of the Internet ([:x:](https://arxiv.org/abs/2305.08596)), ([:paperclip:](https://arxiv.org/pdf/2305.08596.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.08596)) | 
| 5.15 | AutoRecon: Automated 3D Object Discovery and Reconstruction ([:x:](https://arxiv.org/abs/2305.08810)), ([:paperclip:](https://arxiv.org/pdf/2305.08810.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.08810)), ([Project page](https://zju3dv.github.io/autorecon/)) | 
| 5.15 | RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs ([:x:](https://arxiv.org/abs/2305.08844)), ([:paperclip:](https://arxiv.org/pdf/2305.08844.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.08844)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rl4f-generating-natural-language-feedback)) | 
| 5.15 | Small Models are Valuable Plug-ins for Large Language Models ([:x:](https://arxiv.org/abs/2305.08848)), ([:paperclip:](https://arxiv.org/pdf/2305.08848.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.08848)) | 
| 5.15 | "ChatGPT can pick stocks better then top fund managers" - The ChatGPT Fund - ([tweet](https://twitter.com/chatgpttrader/status/1658193474708213760)), ([website](https://www.thechatgptfund.com/)) |
| 5.15 | officially launching the Poe API - ([Tweet](https://twitter.com/adamdangelo/status/1658121701077516291), ([:octocat:](https://github.com/poe-platform)): ([poe-protocol](https://github.com/poe-platform/poe-protocol)![GitHub Repo stars](https://img.shields.io/github/stars/poe-platform/poe-protocol?style=social)), ([api-bot-tutorial](https://github.com/poe-platform/api-bot-tutorial)![GitHub Repo stars](https://img.shields.io/github/stars/poe-platform/api-bot-tutorial?style=social)) | |
| 5.15 | Guidance - A guidance language for controlling large language models ([:octocat:](https://github.com/microsoft/guidance)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/guidance?style=social)) |
| 5.15 | BriefGPT - Locally hosted tool that connects documents to LLMs for summarization and querying, with a simple GUI ([:octocat:](https://github.com/e-johnstonn/BriefGPT)![GitHub Repo stars](https://img.shields.io/github/stars/e-johnstonn/BriefGPT?style=social)) |
| 5.15 | I’m an ER doctor. Here’s how I’m already using ChatGPT to help treat patients ([blog](https://www.fastcompany.com/90895618/how-a-doctor-uses-chat-gpt-to-treat-patients)) |
| 5.14 | A Comprehensive Survey on Segment Anything Model for Vision and Beyond ([:x:](https://arxiv.org/abs/2305.08196)), ([:paperclip:](https://arxiv.org/pdf/2305.08196.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.08196)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-comprehensive-survey-on-segment-anything)), ([:octocat:](https://github.com/liliu-avril/Awesome-Segment-Anything)![GitHub Repo stars](https://img.shields.io/github/stars/liliu-avril/Awesome-Segment-Anything?style=social)) |
| 5.14 | How to run Llama 13B with a 6GB graphics card ([Gist](https://gist.github.com/rain-1/8cc12b4b334052a21af8029aa9c4fafc)) |
| 5.13 | Leaked Copilot Chat's confidential rules ([tweet](https://twitter.com/marvinvonhagen/status/1657060506371346432)) |
| 5.13 | GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content (arXiv](https://arxiv.org/abs/2305.07969)), ([:paperclip:](https://arxiv.org/pdf/2305.07969.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.07969)) |
| 5.13 | Everything-LLMs-And-Robotics - The world's largest GitHub Repository for LLMs + Robotics ([:octocat:](https://github.com/jrin771/Everything-LLMs-And-Robotics)![GitHub Repo stars](https://img.shields.io/github/stars/jrin771/Everything-LLMs-And-Robotics?style=social)) | 
| 5.13 | CodeT5+: Open Code Large Language Models for Code Understanding and Generation ([:x:](https://arxiv.org/abs/2305.07922)), ([:paperclip:](https://arxiv.org/pdf/2305.07922.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.07922)), ([:octocat:](https://github.com/salesforce/CodeT5/tree/main/CodeT5%2B)![GitHub Repo stars](https://img.shields.io/github/stars/salesforce/CodeT5?style=social)), ([:trophy:papers with code](https://paperswithcode.com/paper/codet5-open-code-large-language-models-for)) |
| 5.13 | EU AI Act To Target US Open Source Software ([Blog](https://technomancers.ai/eu-ai-act-to-target-us-open-source-software/#more-561)) |
| 5.13 | PCAST Working Group on Generative AI Invites Public Input ([Blog](https://terrytao.wordpress.com/2023/05/13/pcast-working-group-on-generative-ai-invites-public-input/)) | 
| 5.12 | Text2Cohort: Democratizing the NCI Imaging Data Commons with Natural Language Cohort Discovery  ([:x:](https://arxiv.org/abs/2305.07637)), ([:paperclip:](https://arxiv.org/pdf/2305.07637.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.07637)), ([:house:](https://huggingface.co/papers/2305.07637)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/text2cohort-democratizing-the-nci-imaging)) |
| 5.12 | MedGPTEval: A Dataset and Benchmark to Evaluate Responses of Large Language Models in Medicine ([:x:](https://arxiv.org/abs/2305.07340)), ([:paperclip:](https://arxiv.org/pdf/2305.07340.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.07340)), ([:house:](https://huggingface.co/papers/2305.07340)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/medgpteval-a-dataset-and-benchmark-to)) |
| 5.12 | A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering ([:x:](https://arxiv.org/abs/2305.06211)), ([:paperclip:](https://arxiv.org/pdf/2305.06211.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.06211)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-survey-on-segment-anything-model-sam-vision)) |
| 5.12 | spacy-llm, an extension for integrating LLMs into structured NLP pipelines! ([:octocat:](https://github.com/explosion/spacy-llm)![GitHub Repo stars](https://img.shields.io/github/stars/explosion/spacy-llm?style=social)), ([tweet](https://twitter.com/spacy_io/status/1656734286425255937)) |
| 5.12 | TinyStories: How Small Can Language Models Be and Still Speak Coherent English? ([:x:](https://arxiv.org/abs/2305.07759)), ([:paperclip:](https://arxiv.org/pdf/2305.07759.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.07759)) | 
| 5.12 | Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation  ([:x:](https://arxiv.org/abs/2305.07804)), ([:paperclip:](https://arxiv.org/pdf/2305.07804.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.07804)), ([model](https://huggingface.co/Tyrannosaurus/ArtGPT-4)), ([:octocat:](https://github.com/zguo0525/Dr.llama)![GitHub Repo stars](https://img.shields.io/github/stars/zguo0525/Dr.llama?style=social))  |
| 5.12 | ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4 ([:x:](https://arxiv.org/abs/2305.07490)), ([:paperclip:](https://arxiv.org/pdf/2305.07490.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.07490)), ([model](https://huggingface.co/Tyrannosaurus/ArtGPT-4)) |
| 5.12 | MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers ([:x:](https://arxiv.org/abs/2305.07185)), ([:paperclip:](https://arxiv.org/pdf/2305.07185.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.07185)) |
| 5.12 | AI FILM -The Carnival of the Ages - Runway gen2 ([Youtube](https://www.youtube.com/watch?v=q0EDV1HGbrc)), ([Reddit](https://www.reddit.com/r/aivideo/comments/13eh1rq/carnival_of_ages_text_to_video_runway_gen2/)) | 
| 5.11 | Towards best practices in AGI safety and governance: A survey of expert opinion ([:x:](https://arxiv.org/abs/2305.07153)), ([:book:](https://browse.arxiv.org/pdf/2305.07153.pdf)), ([:paperclip:](https://arxiv.org/pdf/2305.07153.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.07153)), ([:house:](https://huggingface.co/papers/2305.07153)), ([:eight_spoked_asterisk:]()), ([SS](https://www.semanticscholar.org/paper/Towards-best-practices-in-AGI-safety-and-A-survey-Schuett-Dreksler/c387a3999113f3f8bcf26681d95cf0f4313f64f4)) |
| 5.11 | The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain ([:x:](https://arxiv.org/abs/2305.07141)), ([:paperclip:](https://arxiv.org/pdf/2305.07141.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.07141)), ([:house:](https://huggingface.co/papers/2305.07141)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-conceptarc-benchmark-evaluating)), ([:octocat:](https://github.com/victorvikram/conceptarc)![GitHub Repo stars](https://img.shields.io/github/stars/victorvikram/conceptarc?style=social)) |
| 5.11 | Large Language Models Can Be Used To Effectively Scale Spear Phishing Campaigns ([:x:](https://arxiv.org/abs/2305.06972)), ([:paperclip:](https://arxiv.org/pdf/2305.06972.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.06972)) |
| 5.11 | Towards best practices in AGI safety and governance: A survey of expert opinion ([:x:](https://arxiv.org/abs/2305.07153)), ([:paperclip:](https://arxiv.org/pdf/2305.07153.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.07153)) |
| 5.11 | Optimizing Memory Mapping Using Deep Reinforcement Learning ([:x:](https://arxiv.org/abs/2305.07440)), ([:paperclip:](https://arxiv.org/pdf/2305.07440.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.07440)) |
| 5.11 | Universal Source Separation with Weakly Labelled Data ([:x:](https://arxiv.org/abs/2305.07447)), ([:paperclip:](https://arxiv.org/pdf/2305.07447.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.07447)), ([:octocat:](https://github.com/bytedance/uss)![GitHub Repo stars](https://img.shields.io/github/stars/bytedance/uss?style=social)) |
| 5.11 | Active Retrieval Augmented Generation  ([:x:](https://arxiv.org/abs/2305.06983)), ([:paperclip:](https://arxiv.org/pdf/2305.06983.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.06983)), ([:octocat:](https://github.com/jzbjyb/FLARE)![GitHub Repo stars](https://img.shields.io/github/stars/jzbjyb/FLARE?style=social)) |
| 5.11 | Anthropic - Introducing 100K Context Windows ([Blog](https://www.anthropic.com/index/100k-context-windows)) |
| 5.11 | CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model ([:x:](https://arxiv.org/abs/2305.06908)), ([:paperclip:](https://arxiv.org/pdf/2305.06908.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.06908)) |
| 5.11 | Exploiting Diffusion Prior for Real-World Image Super-Resolution ([:x:](https://arxiv.org/abs/2305.07015)), ([:paperclip:](https://arxiv.org/pdf/2305.07015.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.07015)), ([Project page](https://iceclear.github.io/projects/stablesr/)) |
| 5.11 | Domain Incremental Lifelong Learning in an Open World ([:x:](https://arxiv.org/abs/2305.06555)), ([:paperclip:](https://arxiv.org/pdf/2305.06555.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.06555)) |
| 5.11 | Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting ([:x:](https://arxiv.org/abs/2305.07004)), ([:paperclip:](https://arxiv.org/pdf/2305.07004.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.07004)) |
| 5.11 | Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers ([:x:](https://arxiv.org/abs/2305.07011)), ([:paperclip:](https://arxiv.org/pdf/2305.07011.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.07011)) |
| 5.11 | EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention ([:x:](https://arxiv.org/abs/2305.07027)), ([:paperclip:](https://arxiv.org/pdf/2305.07027.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.07027)), ([:octocat:](https://github.com/microsoft/Cream/tree/main/EfficientViT)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/Cream?style=social)) |
| 5.11 | InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning ([:x:](https://arxiv.org/abs/2305.06500)), ([:paperclip:](https://arxiv.org/pdf/2305.06500.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.06500)), ([:octocat:](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)![GitHub Repo stars](https://img.shields.io/github/stars/salesforce/LAVIS?style=social)) |
| 5.11 | Huggingface Transformers Agent ([API](https://huggingface.co/docs/transformers/transformers_agents)) |
| 5.11 | Google PaLM 2 Technical Report ([:paperclip:](https://ai.google/static/documents/palm2techreport.pdf)), ([Blog](https://ai.google/discover/palm2)) |
| 5.11 | Google MusicLM ([Demo](https://aitestkitchen.withgoogle.com/experiments/music-lm)), ([news](https://techcrunch.com/2023/05/10/google-makes-its-text-to-music-ai-public/)) |
| 5.10 | LMFlow Benchmark: An Automatic Evaluation Framework for Open-Source LLMs ([blog](https://blog.gopenai.com/lmflow-benchmark-an-automatic-evaluation-framework-for-open-source-llms-ef5c6f142418)) | 
| 5.10 | Bridging the Literacy Gap for Surgical Consents: An AI-Human Expert Collaborative Approach (medxRiv [paper](https://www.medrxiv.org/content/10.1101/2023.05.06.23289615v1)) | 
| 5.10 | Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks ([:x:](https://arxiv.org/abs/2305.05862)), ([:paperclip:](https://arxiv.org/pdf/2305.05862.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.05862)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/are-chatgpt-and-gpt-4-general-purpose-solvers)) |
| 5.10 | HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion ([:x:](https://arxiv.org/abs/2305.06356)), ([:paperclip:](https://arxiv.org/pdf/2305.06356.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.06356)) |
| 5.10 | VideoChat: Chat-Centric Video Understanding ([:x:](https://arxiv.org/abs/2305.06355)), ([:paperclip:](https://arxiv.org/pdf/2305.06355.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.06355)) |
| 5.10 | Bot or Human? Detecting ChatGPT Imposters with A Single Question ([:x:](https://arxiv.org/abs/2305.06424)), ([:paperclip:](https://arxiv.org/pdf/2305.06424.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.06424)) |
| 5.10 | Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction ([:x:](https://arxiv.org/abs/2305.06474)), ([:paperclip:](https://arxiv.org/pdf/2305.06474.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.06474)) |
| 5.10 | Relightify: Relightable 3D Faces from a Single Image via Diffusion Models ([:x:](https://arxiv.org/abs/2305.06077)), ([:paperclip:](https://arxiv.org/pdf/2305.06077.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.06077)) |
| 5.10 | Similarity of Neural Network Models: A Survey of Functional and Representational Measures ([:x:](https://arxiv.org/abs/2305.06329)), ([:paperclip:](https://arxiv.org/pdf/2305.06329.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.06329)) |
| 5.10 | Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era  ([:x:](https://arxiv.org/abs/2305.06131)), ([:paperclip:](https://arxiv.org/pdf/2305.06131.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.06131)) |
| 5.10 | MPT-7B StoryWriter- new open-source language model that can handle really long inputs ([Replicate](https://replicate.com/replicate/mpt-7b-storywriter)) |
| 5.10 | [Humata.ai](https://www.humata.ai/) - Ask AI anything about your files ([Tweet](https://twitter.com/hasantoxr/status/1655963736149045249)) |
| 5.10 | IMAGEBIND: One Embedding Space To Bind Them All ([:paperclip:](https://dl.fbaipublicfiles.com/imagebind/imagebind_final.pdf)), ([Blog](https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/)), ([:octocat:](https://github.com/facebookresearch/ImageBind)![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/ImageBind?style=social)), ([:trophy:papers with code](https://paperswithcode.com/paper/imagebind-one-embedding-space-to-bind-them)), ([star history](https://star-history.com/#facebookresearch/ImageBind&Date)) |
| 5.9 | Large Language Models Need Holistically Thought in Medical Conversational QA  ([:x:](https://arxiv.org/abs/2305.05410)), ([:paperclip:](https://arxiv.org/pdf/2305.05410.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.05410)), ([:house:](https://huggingface.co/papers/2305.05410)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-need-holistically)) |
| 5.9 | StarCoder: may the source be with you! ([:x:](https://arxiv.org/abs/2305.06161)), ([:paperclip:](https://arxiv.org/pdf/2305.06161.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.06161)), ([:house:](https://huggingface.co/papers/2305.06161)) |
| 5.9 | Towards Building the Federated GPT: Federated Instruction Tuning ([:x:](https://arxiv.org/abs/2305.05644)), ([:paperclip:](https://arxiv.org/pdf/2305.05644.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.05644)), ([:octocat:](https://github.com/JayZhang42/FederatedGPT-Shepherd)![GitHub Repo stars](https://img.shields.io/github/stars/JayZhang42/FederatedGPT-Shepherd?style=social)) |
| 5.9 | Large Language Model Programs ([:x:](https://arxiv.org/abs/2305.05364)), ([:paperclip:](https://arxiv.org/pdf/2305.05364.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.05364)) |
| 5.9 | FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance ([:x:](https://arxiv.org/abs/2305.05176)), ([:paperclip:](https://arxiv.org/pdf/2305.05176.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.05176)) |
| 5.9 | OpenAI - Language models can explain neurons in language models ([Blog](https://openai.com/research/language-models-can-explain-neurons-in-language-models)), ([Paper](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html)), ([:octocat:](https://github.com/openai/automated-interpretability)![GitHub Repo stars](https://img.shields.io/github/stars/openai/automated-interpretability?style=social)), ([Tweet](https://twitter.com/OpenAI/status/1655982364273831936)) |
| 5.9 | AvatarReX: Real-time Expressive Full-body Avatars ([:x:](https://arxiv.org/abs/2305.04789)), ([:paperclip:](https://arxiv.org/pdf/2305.04789.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.04789)) | 
| 5.8 | Augmented Large Language Models with Parametric Knowledge Guiding ([:x:](https://arxiv.org/abs/2305.04789)), ([:paperclip:](https://arxiv.org/pdf/2305.04789.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.04789)) | 
| 5.8 | We had ChatGPT take the CPA exam — and it failed ([news](https://www.accountingtoday.com/news/we-ran-the-cpa-exam-through-chatgpt-and-it-failed-miserably)) |
| 5.8 | Comparison of GPT-3.5, GPT-4, and human user performance on a practice ophthalmology written examination ([Nature](https://www.nature.com/articles/s41433-023-02564-2)) |
| 5.8 | MultiModal-GPT: A Vision and Language Model for Dialogue with Humans  ([:x:](https://arxiv.org/abs/2305.04790)), ([:paperclip:](https://arxiv.org/pdf/2305.04790.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.04790)), ([:octocat:](https://github.com/open-mmlab/Multimodal-GPT)![GitHub Repo stars](https://img.shields.io/github/stars/open-mmlab/Multimodal-GPT?style=social)), ([:house:](https://huggingface.co/papers/2305.04790)), ([Star history](https://star-history.com/#open-mmlab/Multimodal-GPT&Date)) |
| 5.7 | SuperAgent - Deploy LLM Agents to production ([:octocat:](https://github.com/homanp/superagent)![GitHub Repo stars](https://img.shields.io/github/stars/homanp/superagent?style=social)) |
| 5.7 | Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models ([:x:](https://arxiv.org/abs/2305.04091)), ([:paperclip:](https://arxiv.org/pdf/2305.04091.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.04091)), ([:octocat:](https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting)![GitHub Repo stars](https://img.shields.io/github/stars/AGI-Edgerunners/Plan-and-Solve-Prompting?style=social)) |
| 5.7 | X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages ([:x:](https://arxiv.org/abs/2305.04160)), ([:paperclip:](https://arxiv.org/pdf/2305.04160.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.04160)) |
| 5.7 | Multi-Space Neural Radiance Fields ([:x:](https://arxiv.org/abs/2305.04268)), ([:paperclip:](https://arxiv.org/pdf/2305.04268.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.04268)), ([Project page](https://zx-yin.github.io/msnerf/)), ([Dataset](https://drive.google.com/drive/folders/1gqmonTlR8LbJkljtT28S47N_o_YoExFz)) |
| 5.7 | Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting ([:x:](https://arxiv.org/abs/2305.04388)), ([:paperclip:](https://arxiv.org/pdf/2305.04388.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.04388)) |
| 5.7 | Yoshua Bengio - AI Scientists: Safe and Useful AI? ([Blog](https://yoshuabengio.org/2023/05/07/ai-scientists-safe-and-useful-ai/)) |
| 5.5 | privateGPT - Interact privately with your documents using the power of GPT, 100% privately, no data leaks ([:octocat:](https://github.com/imartinez/privateGPT)![GitHub Repo stars](https://img.shields.io/github/stars/imartinez/privateGPT?style=social)), ([star history](https://star-history.com/#imartinez/privateGPT&Date)) |
| 5.5 | Open LLMs : A list of open LLMs available for commercial use - ([:octocat:](https://github.com/eugeneyan/open-llms)![GitHub Repo stars](https://img.shields.io/github/stars/eugeneyan/open-llms?style=social)) |
| 5.5 | A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding ([:x:](https://arxiv.org/abs/2305.03668)), ([:paperclip:](https://arxiv.org/pdf/2305.03668.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.03668)), ([:house:](https://huggingface.co/papers/2305.03668)) |
| 5.5 | Otter: A Multi-Modal Model with In-Context Instruction Tuning  ([:x:](https://arxiv.org/abs/2305.03726)), ([:paperclip:](https://arxiv.org/pdf/2305.03726.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.03726)), ([:octocat:](https://github.com/Luodian/Otter)![GitHub Repo stars](https://img.shields.io/github/stars/Luodian/Otter?style=social)), ([:house:](https://huggingface.co/papers/2305.03726)) |
| 5.5 | Composite Motion Learning with Task Control ([:x:](https://arxiv.org/abs/2305.03286)), ([:paperclip:](https://arxiv.org/pdf/2305.03286.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.03286)), ([:octocat:](https://github.com/xupei0610/CompositeMotion)![GitHub Repo stars](https://img.shields.io/github/stars/xupei0610/CompositeMotion?style=social)), ([Papper page](https://huggingface.co/papers/2305.03286)) |
| 5.5 | StarCoderBase: trained on 1T tokens in 80+ programming languages ([Huggingface](https://huggingface.co/bigcode/starcoderbase)) |
| 5.5 | Dolphin: General video interaction platform based on LLMs ([Demo](https://da2c48f45ee053ef87.gradio.live/)), ([:octocat:](https://github.com/kaleido-lab/dolphin)![GitHub Repo stars](https://img.shields.io/github/stars/kaleido-lab/dolphin?style=social)), ([Tweet](https://twitter.com/_akhaliq)) | 
| 5.5 | MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs ([Blog](https://www.mosaicml.com/blog/mpt-7b)), Commercially usable: ([MPT-7B](https://huggingface.co/mosaicml/mpt-7b))  ([MPT-7B-Instruct](https://huggingface.co/mosaicml/mpt-7b-instruct)), ([MPT-7B-StoryWriter](https://huggingface.co/mosaicml/mpt-7b-storywriter)), For non-commerical use: ([MPT-7B-Chat](https://huggingface.co/mosaicml/mpt-7b-chat)) |
| 5.5 | StarCoder: A State-of-the-Art LLM for Code ([Blog](https://huggingface.co/blog/starcoder)), ([:octocat:](https://github.com/bigcode-project/starcoder/)![GitHub Repo stars](https://img.shields.io/github/stars/bigcode-project/starcoder?style=social)), ([HuggingFace](https://huggingface.co/bigcode/starcoder)), ([Tweet](https://twitter.com/BigCodeProject/status/1654174941976068119)) |
| 5.5 | OpenAlpaca, an instruction-following model based on OpenLLaMA ([:octocat:](https://github.com/openlm-research/open_llama)![GitHub Repo stars](https://img.shields.io/github/stars/openlm-research/open_llama?style=social)), ([Huggingface](https://huggingface.co/openlm-research/open_llama_7b_preview_200bt)), ([Tweet](https://twitter.com/yixuan_su/status/1654234602003636226)) |
| 5.4 | Caption Anything: Interactive Image Description with Diverse Multimodal Controls ([:x:](https://arxiv.org/abs/2305.02677)), ([:book:](https://browse.arxiv.org/pdf/2305.02677.pdf)), ([:paperclip:](https://arxiv.org/pdf/2305.02677.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.02677)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/caption-anything-interactive-image)), ([:octocat:](https://github.com/ttengwang/caption-anything)![GitHub Repo stars](https://img.shields.io/github/stars/ttengwang/caption-anything?style=social))  |
| 5.4 | Seeing is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability  ([:x:](https://arxiv.org/abs/2305.08746)), ([:paperclip:](https://arxiv.org/pdf/2305.08746.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.08746)), ([:octocat:](https://github.com/KindXiaoming/BIMT)(https://img.shields.io/github/stars/KindXiaoming/BIMT?style=social)), ([demo](https://colab.research.google.com/drive/1hggc5Tae97BORVNdesLcwp9og3SmPtM7?usp=sharing)), ([Papper page](https://huggingface.co/papers/2305.08746)) |
| 5.4 | Evaluating the Performance of ChatGPT in Ophthalmology: An Analysis of its Successes and Shortcomings ([Ophthalmology Science](https://www.ophthalmologyscience.org/article/S2666-9145(23)00056-8/fulltext)) | 
| 5.4 | Cognitive Reframing of Negative Thoughts through Human-Language Model Interaction ([:x:](https://arxiv.org/abs/2305.02466)), ([:paperclip:](https://arxiv.org/pdf/2305.02466.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.02466)) |
| 5.4 | Governance of the AI, by the AI, and for the AI ([:x:](https://arxiv.org/abs/2305.03719)), ([:paperclip:](https://arxiv.org/pdf/2305.03719.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.03719)), ([Papper page](https://huggingface.co/papers/2305.03719)) |
| 5.4 | Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs  ([:x:](https://arxiv.org/abs/2305.03111)), ([:paperclip:](https://arxiv.org/pdf/2305.03111.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.03111)) |
| 5.4 | Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion ([:x:](https://arxiv.org/abs/2305.03509)), ([:paperclip:](https://arxiv.org/pdf/2305.03509.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.03509)), ([Papper page](https://huggingface.co/papers/2305.03509)) |
| 5.4 | AttentionViz: A Global View of Transformer Attention ([:x:](https://arxiv.org/abs/2305.03210)), ([:paperclip:](https://arxiv.org/pdf/2305.03210.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.03210)), ([Papper page](https://huggingface.co/papers/2305.03210)) |
| 5.4 | Reddit - [OpenAI lost $540M in 2022, will need $100B more to develop AGI, says Altman. My breakdown on why this matters and what it means for other AI startups](https://www.reddit.com/r/ChatGPT/comments/1383obf/openai_lost_540m_in_2022_will_need_100b_more_to/) |
| 5.4 | FACT SHEET: Biden-⁠Harris Administration Announces New Actions to Promote Responsible AI Innovation that Protects Americans’ Rights and Safety - ([White house](https://www.whitehouse.gov/briefing-room/statements-releases/2023/05/04/fact-sheet-biden-harris-administration-announces-new-actions-to-promote-responsible-ai-innovation-that-protects-americans-rights-and-safety/)) |
| 5.4 | Google "We Have No Moat, And Neither Does OpenAI" - ([Blog](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)) |
| 5.4 | CNBC - [Britain launches probe into ChatGPT-style A.I. as regulators grow concerned by risks](https://www.cnbc.com/2023/05/04/chatgpt-britain-launches-competition-probe-into-ai-consumer-risks.html?utm_content=Main&utm_medium=Social&utm_source=Twitter) |
| 5.4 | Personalize Segment Anything Model with One Shot ([:x:](https://arxiv.org/abs/2305.03048)), ([:paperclip:](https://arxiv.org/pdf/2305.03048.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.03048)), ([:octocat:](https://github.com/ZrrSkywalker/Personalize-SAM)![GitHub Repo stars](https://img.shields.io/github/stars/ZrrSkywalker/Personalize-SAM?style=social)), ([:house:](https://huggingface.co/papers/2305.03048)) |
| 5.4 | AutoML-GPT: Automatic Machine Learning with GPT ([:x:](https://arxiv.org/abs/2305.02499)), ([:paperclip:](https://arxiv.org/pdf/2305.02499.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.02499)), ([:house:](https://huggingface.co/papers/2305.02499)) |
| 5.4 | NeRSemble: Multi-view Radiance Field Reconstruction of Human Heads ([:x:](https://arxiv.org/abs/2305.03027)), ([:paperclip:](https://arxiv.org/pdf/2305.03027.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.03027), ([Project page](https://tobias-kirschstein.github.io/nersemble/)), ([:house:](https://huggingface.co/papers/2305.03027)) |
| 5.4 | An automatically discovered chain-of-thought prompt generalizes to novel models and datasets ([:x:](https://arxiv.org/abs/2305.02897)), ([:paperclip:](https://arxiv.org/pdf/2305.02897.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.02897))
| 5.4 | NYT - [White House Pushes Tech C.E.O.s to Limit Risks of A.I.](https://www.nytimes.com/2023/05/04/technology/us-ai-research-regulation.html) |
| 5.4 | Microsoft Bing AI chatbot and Edge browser get massive AI upgrades. See the list. ([Blog](https://mashable.com/article/microsoft-bing-ai-chatbot-edge-browser-new-updates-features)) |
| 5.3 | Can Large Language Models Be an Alternative to Human Evaluations? ([:x:](https://arxiv.org/abs/2305.01937)), ([:paperclip:](https://arxiv.org/pdf/2305.01937.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.01937)), ([:house:](https://huggingface.co/papers/2305.01937)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/can-large-language-models-be-an-alternative)) |
| 5.4 | Introducing Slack GPT ([Blog](https://slack.com/intl/ko-kr/blog/news/introducing-slack-gpt)) |
| 5.3 | Distinguishing GPT-4-generated Radiology Abstracts from Original Abstracts: Performance of Blinded Human Observers and AI Content Detector ([medRxiv](https://www.medrxiv.org/content/10.1101/2023.04.28.23289283v1)), ([:paperclip:](https://www.medrxiv.org/content/10.1101/2023.04.28.23289283v1.full.pdf)) |
| 5.3 | Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings - ([Blog](https://lmsys.org/blog/2023-05-03-arena/)) |
| 5.3 | CodeGen2: Lessons for Training LLMs on Programming and Natural Languages ([:x:](https://arxiv.org/abs/2305.02309)), ([:paperclip:](https://arxiv.org/pdf/2305.02309.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.02309)), ([:octocat:](https://github.com/salesforce/CodeGen2)![GitHub Repo stars](https://img.shields.io/github/stars/salesforce/CodeGen2?style=social)) |
| 5.3 | Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes ([:x:](https://arxiv.org/abs/2305.02301)), ([:paperclip:](https://arxiv.org/pdf/2305.02301.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.02301)) |
| 5.3 | Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings ([:x:](https://arxiv.org/abs/2305.02317)), ([:paperclip:](https://arxiv.org/pdf/2305.02317.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.02317)) |
| 5.3 | AG3D: Learning to Generate 3D Avatars from 2D Image Collections ([:x:](https://arxiv.org/abs/2305.02312)), ([:paperclip:](https://arxiv.org/pdf/2305.02312.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.02312)), ([Project page](https://zj-dong.github.io/AG3D/)) |
| 5.3 | Shap-E: Generating Conditional 3D Implicit Functions ([:x:](https://arxiv.org/abs/2305.02463)), ([:paperclip:](https://arxiv.org/pdf/2305.02463.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.02463)), ([:octocat:](https://github.com/openai/shap-e)![GitHub Repo stars](https://img.shields.io/github/stars/openai/shap-e?style=social)), ([:house:](https://huggingface.co/papers/2305.02463)) |
| 5.3 | 100 Practical Applications and Use Cases of Generative AI - ([:paperclip:](https://ai.gov.ae/wp-content/uploads/2023/04/406.-Generative-AI-Guide_ver1-EN.pdf)), ([News](https://gulfnews.com/uae/uae-government-launches-guide-on-generative-ai-applications-such-as-chatgpt-1.95449467)) |  
| 5.3 | Comprehensive LLM model zoo - Ecosystem Graphs to track the foundation model ecosystem assets (datasets, models, and applications) and their relationship ([Table](https://crfm.stanford.edu/ecosystem-graphs/index.html?mode=table)), ([Graph](https://crfm.stanford.edu/ecosystem-graphs/index.html?mode=graph)), ([:octocat:](https://github.com/stanford-crfm/ecosystem-graphs)![GitHub Repo stars](https://img.shields.io/github/stars/stanford-crfm/ecosystem-graphs?style=social)) |
| 5.3 | GPTutor: a ChatGPT-powered programming tool for code explanation ([:x:](https://arxiv.org/abs/2305.01863)), ([:paperclip:](https://arxiv.org/pdf/2305.01863.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.01863)) |
| 5.3 | Midjourney 5.1 Arrives - And It’s Another Leap Forward For AI Art - ([Forbes](https://www.forbes.com/sites/barrycollins/2023/05/03/midjourney-51-arrivesand-its-another-leap-forward-for-ai-art/)) |
| 5.3 | Mojo 🔥 — a new programming language for all AI developers ([Web](https://www.modular.com/mojo)), ([tweet](https://twitter.com/Modular_AI/status/1653436642248781825)), ([:octocat:](https://github.com/modularml/mojo)![GitHub Repo stars](https://img.shields.io/github/stars/modularml/mojo?style=social)) |
| 5.3 | #NeurIPS2023 Creative AI Track ([Blog](https://blog.neurips.cc/2023/05/02/call-for-neurips-creative-ai-track/)), ([Call for proposal](https://neurips.cc/Conferences/2023/CallForCreativeAI)) |
| 5.3 | [HeyPi](https://heypi.com/) - Personal AI |
| 5.2 | RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models ([:x:](https://arxiv.org/abs/2305.01146)), ([:paperclip:](https://arxiv.org/pdf/2305.01146.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.01146)), ([:house:](https://huggingface.co/papers/2305.01146)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/radadapt-radiology-report-summarization-via)) |
| 5.2 | Interpretable Machine Learning for Science with PySR and SymbolicRegression.jl ([:x:](https://arxiv.org/abs/2305.01582)), ([:paperclip:](https://arxiv.org/pdf/2305.01582.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.01582)) |
| 5.2 | Andrew Ng - ChatGPT Prompt Engineering for Developers - ([online course](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)), ([Tweet](https://twitter.com/AndrewYNg/status/1653141408386260992)) |
| 5.2 | DreamPaint: Few-Shot Inpainting of E-Commerce Items for Virtual Try-On without 3D Modeling ([:x:](https://arxiv.org/abs/2305.01649)), ([:paperclip:](https://arxiv.org/pdf/2305.01649.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.01649)) |
| 5.2 | Generalizing Dataset Distillation via Deep Generative Prior ([:x:](https://arxiv.org/abs/2305.01257)), ([:paperclip:](https://arxiv.org/pdf/2305.01257.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.01257)) |
| 5.2 | Multimodal Procedural Planning via Dual Text-Image Prompting ([:x:](https://arxiv.org/abs/2305.01795)), ([:paperclip:](https://arxiv.org/pdf/2305.01795.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.01795)), ([:octocat:](https://github.com/YujieLu10/TIP)![GitHub Repo stars](https://img.shields.io/github/stars/YujieLu10/TIP?style=social)) |
| 5.2 | WSJ - [Google DeepMind CEO Says Some Form of AGI Possible in a Few Years](https://www.wsj.com/articles/google-deepmind-ceo-says-some-form-of-agi-possible-in-a-few-years-2705f452) |
| 5.2 | Latest NVIDIA Graphics Research Advances Generative AI’s Next Frontier ([Blog](https://blogs.nvidia.com/blog/2023/05/02/graphics-research-advances-generative-ai-next-frontier/)) |
| 5.2 | Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation ([:x:](https://arxiv.org/abs/2305.01569)), ([:paperclip:](https://arxiv.org/pdf/2305.01569.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.01569)), ([:octocat:](https://github.com/yuvalkirstain/pickscore)![GitHub Repo stars](https://img.shields.io/github/stars/yuvalkirstain/pickscore?style=social)) |
| 5.2 | TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis ([:x:](https://arxiv.org/abs/2305.00976)), ([:paperclip:](https://arxiv.org/pdf/2305.00976.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.00976)), ([Project page](https://t.co/KH4w0442YP)), ([Demo](https://huggingface.co/spaces/Mathux/TMR)) |
| 5.2 | Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation ([:x:](https://arxiv.org/abs/2305.01210)), ([:paperclip:](https://arxiv.org/pdf/2305.01210.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.01210)), ([:octocat:](https://github.com/evalplus/evalplus)![GitHub Repo stars](https://img.shields.io/github/stars/evalplus/evalplus?style=social)) |
| 5.2 | Unlimiformer: Long-Range Transformers with Unlimited Length Input ([:x:](https://arxiv.org/abs/2305.01625)), ([:paperclip:](https://arxiv.org/pdf/2305.01625.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.01625)) |
| 5.2 | Bark - Text-Prompted Generative Audio Model ([:octocat:](https://github.com/suno-ai/bark)![GitHub Repo stars](https://img.shields.io/github/stars/suno-ai/bark?style=social)) |
| 5.2 | Jsonformer: A Bulletproof Way to Generate Structured JSON from Language Models ([:octocat:](https://github.com/1rgs/jsonformer)![GitHub Repo stars](https://img.shields.io/github/stars/1rgs/jsonformer?style=social)) |
| 5.1 | scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics Using Generative AI ([bioXiv](https://www.biorxiv.org/content/10.1101/2023.04.30.538439v1)), ([:paperclip:](https://www.biorxiv.org/content/10.1101/2023.04.30.538439v1.full.pdf)) |
| 5.1 | The Guardian - [AI makes non-invasive mind-reading possible by turning thoughts into text](https://www.theguardian.com/technology/2023/may/01/ai-makes-non-invasive-mind-reading-possible-by-turning-thoughts-into-text) |
| 5.1 | Learning to Reason and Memorize with Self-Notes ([:x:](https://arxiv.org/abs/2305.00833)), ([:paperclip:](https://arxiv.org/pdf/2305.00833.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.00833)) |
| 5.1 | Poisoning Language Models During Instruction Tuning ([:x:](https://arxiv.org/abs/2305.00944)), ([:paperclip:](https://arxiv.org/pdf/2305.00944.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.00944)) |
| 5.1 | What Do Self-Supervised Vision Transformers Learn? ([:x:](https://arxiv.org/abs/2305.00729)), ([:paperclip:](https://arxiv.org/pdf/2305.00729.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.00729)) |
| 5.1 | [NYT](https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html) - ‘The Godfather of A.I.’ Leaves Google and Warns of Danger Ahead ([Archive](https://archive.is/TgPyC#selection-331.0-331.63)) |
| 4.30 | ChatGPT: Is this version good for healthcare and research? - ([ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S1871402123000401)) |
| 4.30 | Understanding Parameter-Efficient LLM Finetuning: Prompt Tuning And Prefix Tuning ([Blog](https://magazine.sebastianraschka.com/p/understanding-parameter-efficient)) |
| 4.30 | A brief history of LLaMA models ([Blog](https://agi-sphere.com/llama-models/)) |
| 4.30 | BabyBeeAGI: Task Management and Functionality Expansion on top of BabyAGI ([blog](https://yoheinakajima.com/babybeeagi-task-management-and-functionality-expansion-on-top-of-babyagi/)), ([Replit](https://replit.com/@YoheiNakajima/BabyBeeAGI?v=1)), ([:octocat:](https://github.com/yoheinakajima/babyagi)![GitHub Repo stars](https://img.shields.io/github/stars/yoheinakajima/babyagi?style=social)), ([OG BaybyAGI](https://replit.com/@YoheiNakajima/babyagi)) |
| 4.30 | Results of G7 Digital and Tech Ministers’ Meeting in Takasaki, Gunma - ([Summary](https://g7digital-tech-2023.go.jp/en/topics/topics_20230430.html)), ([Declaration](https://g7digital-tech-2023.go.jp/topics/pdf/pdf_20230430/ministerial_declaration_dtmm.pdf)), ([Annex1](https://g7digital-tech-2023.go.jp/topics/pdf/pdf_20230430/annex1.pdf)), ([Annex2](https://g7digital-tech-2023.go.jp/topics/pdf/pdf_20230430/annex2.pdf)), ([Annex3](https://g7digital-tech-2023.go.jp/topics/pdf/pdf_20230430/annex3.pdf)), ([Annex4](https://g7digital-tech-2023.go.jp/topics/pdf/pdf_20230430/annex4.pdf)), ([Annex5](https://g7digital-tech-2023.go.jp/topics/pdf/pdf_20230430/annex5.pdf)) |
| 4.30 | PandaLM: Reproducible and Automated Language Model Assessment ([:octocat:](https://github.com/WeOpenML/PandaLM)![GitHub Repo stars](https://img.shields.io/github/stars/WeOpenML/PandaLM?style=social)) |
| 4.29 | Can ChatGPT Pass An Introductory Level Functional Language Programming Course? ([:x:](https://arxiv.org/abs/2305.02230)), ([:paperclip:](https://arxiv.org/pdf/2305.02230.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.02230)) |
| 4.29 | A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions ([:x:](https://arxiv.org/abs/2305.00237)), ([:paperclip:](https://arxiv.org/pdf/2305.00237.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.00237)) |
| 4.29 | ChatGPT-2D, which can generate mind maps with AI - ([Tweet](https://twitter.com/eyishazyer/status/1652215468005146626)), ([ChatGPT-2D](https://superusapp.com/chatgpt2d/)) |
| 4.29 | MLC LLM - an open framework that brings language models (LLMs) directly into a broad class of platforms (CUDA, Vulkan, Metal) with GPU acceleration ([Tweet](https://twitter.com/bohanhou1998/status/1652151502012837890)), ([Demo](https://mlc.ai/mlc-llm/)), ([:octocat:](https://github.com/mlc-ai/mlc-llm)![GitHub Repo stars](https://img.shields.io/github/stars/mlc-ai/mlc-llm?style=social)) |
| 4.29 | GenOs Index - The April (aka the Frenetic Pace) Edition - ([blog](https://www.decibel.vc/articles/genos-index-the-april-aka-the-frenetic-pace-edition)) |
| 4.29 | StableVicuna, the AI World’s First Open Source RLHF LLM Chatbot! - ([Blog](https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot)), ([Tweet](https://twitter.com/StabilityAI/status/1652026192193785856)) |
| 4.29 | DeepFloyd - a state-of-the-art text-to-image model ([Web](https://deepfloyd.ai/deepfloyd-if)), ([:octocat:](https://github.com/deep-floyd/IF)![GitHub Repo stars](https://img.shields.io/github/stars/deep-floyd/IF?style=social)), ([HuggingFace demo](https://huggingface.co/spaces/DeepFloyd/IF)), ([Tweet](https://twitter.com/deepfloydai/status/1651983493717532673)) |
| 4.29 | When Patient Questions Are Answered With Higher Quality and Empathy by ChatGPT than Physicians - ([Blog](https://erictopol.substack.com/p/when-patient-questions-are-answered)) |
| 4.29 | BMTools - Tool Learning for Big Models, Open-Source Solutions of ChatGPT-Plugins ([:octocat:](https://github.com/openbmb/bmtools)) |
| 4.29 | FastChat-T5 ([:octocat:](https://github.com/lm-sys/FastChat#FastChat-T5)![GitHub Repo stars](https://img.shields.io/github/stars/lm-sys/FastChat#FastChat-T5?style=social)), ([Tweet](https://twitter.com/lmsysorg/status/1652037026705985537)) |
| 4.29 | Lamini, the LLM Engine for Rapidly Customizing Models - ([Blog](https://lamini.ai/blog/introducing-lamini)) |
| 4.28 | SAM on Medical Images: A Comprehensive Study on Three Prompt Modes ([:x:](https://arxiv.org/abs/2305.00035)), ([:paperclip:](https://arxiv.org/pdf/2305.00035.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.00035)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sam-on-medical-images-a-comprehensive-study)) |
| 4.28 | EU proposes new copyright rules for generative AI - ([Reuter](https://www.reuters.com/technology/eu-lawmakers-committee-reaches-deal-artificial-intelligence-act-2023-04-27/)), ([Economic times](https://www.reuters.com/technology/eu-lawmakers-committee-reaches-deal-artificial-intelligence-act-2023-04-27/)) | 
| 4.28 | PROMPTENGINEERING FORCHATGPTA QUICKGUIDE TOTECHNIQUES, TIPS,ANDBESTPRACTICES - ([:paperclip:](https://www.techrxiv.org/articles/preprint/Prompt_Engineering_For_ChatGPT_A_Quick_Guide_To_Techniques_Tips_And_Best_Practices/22683919)) |
| 4.28 | ResiDual: Transformer with Dual Residual Connections ([:x:](https://arxiv.org/abs/2304.14802)), ([:paperclip:](https://arxiv.org/pdf/2304.14802.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.14802)), ([:octocat:](https://github.com/microsoft/ResiDual)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/ResiDual?style=social)) |
| 4.28 | Causal Reasoning and Large Language Models: Opening a New Frontier for Causality ([:x:](https://arxiv.org/abs/2305.00050)), ([:paperclip:](https://arxiv.org/pdf/2305.00050.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.00050)) |
| 4.28 | We Interviewed the Engineer Google Fired for Saying Its AI Had Come to Life ([Futurism](https://futurism.com/blake-lemoine-google-interview)) |
| 4.28 | LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model ([:x:](https://arxiv.org/abs/2304.15010)), ([:paperclip:](https://arxiv.org/pdf/2304.15010.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.15010)), ([:octocat:](https://github.com/ZrrSkywalker/LLaMA-Adapter)![GitHub Repo stars](https://img.shields.io/github/stars/ZrrSkywalker/LLaMA-Adapter?style=social)) |
| 4.28 | MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks ([:x:](https://arxiv.org/abs/2304.14979)), ([:paperclip:](https://arxiv.org/pdf/2304.14979.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.14979)) |
| 4.28 | Are Emergent Abilities of Large Language Models a Mirage? ([:x:](https://arxiv.org/abs/2304.15004)), ([:paperclip:](https://arxiv.org/pdf/2304.15004.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.15004)) |
| 4.28 | The Ultimate Battle of Language Models: Lit-LLaMA vs GPT3.5 vs Bloom vs …. ([Blog](https://lightning.ai/pages/community/community-discussions/the-ultimate-battle-of-language-models-lit-llama-vs-gpt3.5-vs-bloom-vs/)) |
| 4.28 | Otter, a multi-modal in-context learning model with instruction tuning - ([:octocat:](https://github.com/Luodian/otter)![GitHub Repo stars](https://img.shields.io/github/stars/Luodian/otter?style=social)), ([Demo](https://otter.cliangyu.com/)), ([Youtube](https://www.youtube.com/watch?v=r-YM4DGGAdE)) |
| 4.28 | [Economist](https://www.economist.com/by-invitation/2023/04/28/yuval-noah-harari-argues-that-ai-has-hacked-the-operating-system-of-human-civilisation) - Yuval Noah Harari argues that AI has hacked the operating system of human civilisation ([Archive](https://archive.is/HGRsq#selection-1039.0-1039.86)) |
| 4.28 | Assessing the Potential of USMLE-Like Exam Questions Generated by GPT-4 ([medRxiv](https://www.medrxiv.org/content/10.1101/2023.04.25.23288588v1)), ([:paperclip:](https://www.medrxiv.org/content/10.1101/2023.04.25.23288588v1.full.pdf)) |
| 4.28 | JAMA - Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum - ([paper](https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2804309?guestAccessKey=6d6e7fbf-54c1-49fc-8f5e-ae7ad3e02231&utm_source=For_The_Media&utm_medium=referral&utm_campaign=ftm_links&utm_content=tfl&utm_term=042823)) |
| 4.27 | Ethics of large language models in medicine and medical research (The Lancet, [https://doi.org/10.1016/S2589-7500(23)00083-3](https://www.thelancet.com/journals/landig/article/PIIS2589-7500(23)00083-3/fulltext)), ([PDF](https://www.thelancet.com/action/showPdf?pii=S2589-7500%2823%2900083-3)) |
| 4.27 | ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger ([:x:](https://arxiv.org/abs/2304.14475)), ([:paperclip:](https://arxiv.org/pdf/2304.14475.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.14475)) |
| 4.27 | PMC-LLaMA: Further Finetuning LLaMA on Medical Papers ([:x:](https://arxiv.org/abs/2304.14454)), ([:paperclip:](https://arxiv.org/pdf/2304.14454.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.14454)), ([:octocat:](https://github.com/chaoyi-wu/PMC-LLaMA)![GitHub Repo stars](https://img.shields.io/github/stars/chaoyi-wu/PMC-LLaMA?style=social)) |
| 4.27 | "Can ChatGPT Diagnose Me?" How Large Language Models will Transform Clinical Care - ([Youtube](https://www.youtube.com/playlist?list=PLe6zdIMe5B7JWokb0Vvket4M3h-0KvBNn)) |
| 4.27 | Large Language Models Are State-of-the-Art Evaluators of Code Generation ([:x:](https://arxiv.org/abs/2304.14317)), ([:paperclip:](https://arxiv.org/pdf/2304.14317.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.14317)) |
| 4.27 | Controlled Text Generation with Natural Language Instructions  ([:x:](https://arxiv.org/abs/2303.14293)), ([:paperclip:](https://arxiv.org/pdf/2303.14293.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.14293)) |
| 4.27 | ⭐ A Survey of Large Language Models - version 8 ([:x:](https://arxiv.org/abs/2303.18223)), ([:paperclip:](https://arxiv.org/pdf/2303.18223.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.18223)) |
| 4.27 | LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions ([:x:](https://arxiv.org/abs/2304.14402)), ([:paperclip:](https://arxiv.org/pdf/2304.14402.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.14402)), ([:octocat:]([https://github.com/m](https://github.com/mbzuai-nlp/LaMini-LM)![GitHub Repo stars](https://img.shields.io/github/stars/mbzuai-nlp/LaMini-LM?style=social)) |
| 4.27 | DataComp: In search of the next generation of multimodal datasets ([:x:](https://arxiv.org/abs/2304.14108)), ([:paperclip:](https://arxiv.org/pdf/2304.14108.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.14108)), ([:octocat:](https://github.com/mlfoundations/datacomp)![GitHub Repo stars](https://img.shields.io/github/stars/mlfoundations/datacomp?style=social)), ([Project page](https://www.datacomp.ai/)) |
| 4.27 | We're Afraid Language Models Aren't Modeling Ambiguity ([:x:](https://arxiv.org/abs/2304.14399)), ([:paperclip:](https://arxiv.org/pdf/2304.14399.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.14399)) |
| 4.27 | Boston Dynamics robot dog can answer your questions now, thanks to ChatGPT - ([ZDNet](https://www.zdnet.com/article/boston-dynamics-robot-dog-can-answer-your-questions-now-thanks-to-chatgpt/)), ([YouTube](https://www.youtube.com/watch?v=Y1-s37zrm1M)) |
| 4.27 | LlamaIndex & Deep Lake for Financial Statement Analysis ([Blog](https://medium.com/@jerryjliu98/llamaindex-deep-lake-for-financial-statement-analysis-954f2b789c8e)) |
| 4.26 | Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining ([:x:](https://arxiv.org/abs/2304.14204)), ([:paperclip:](https://arxiv.org/pdf/2304.14204.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.14204)),  ([:house:](https://huggingface.co/papers/2304.14204)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-medical-artificial-general)) |
| 4.26 | Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning ([:x:](https://arxiv.org/abs/2304.13653)), ([:paperclip:](https://arxiv.org/pdf/2304.13653.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.13653)) |
| 4.26 | Multidimensional Evaluation for Text Style Transfer Using ChatGPT ([:x:](https://arxiv.org/abs/2304.13462)), ([:paperclip:](https://arxiv.org/pdf/2304.13462.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.13462)) |
| 4.26 | NPJ - Comparing scientific abstracts generated by ChatGPT to real abstracts with detectors and blinded human reviewers ([Paper](https://www.nature.com/articles/s41746-023-00819-6)), ([:paperclip:](https://www.nature.com/articles/s41746-023-00819-6.pdf)) |
| 4.26 | [TopGPT](https://www.topgpt.io/) — the world’s first Andrew Tate large language model |
| 4.26 | Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models ([:x:](https://arxiv.org/abs/2304.13835)), ([:paperclip:](https://arxiv.org/pdf/2304.13835.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.13835)) |
| 4.26 | MOSS, a 16B tool-augmented conversational language model ([Tweet](https://twitter.com/tianxiangsun/status/1650895260493705216)), ([:octocat:](https://github.com/OpenLMLab/MOSS)![GitHub Repo stars](https://img.shields.io/github/stars/OpenLMLab/MOSS?style=social)) |
| 4.26 | Exploring the Curious Case of Code Prompts ([:x:](https://arxiv.org/abs/2304.13250)), ([:paperclip:](https://arxiv.org/pdf/2304.13250.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.13250)) |
| 4.26 | Controllable Image Generation via Collage Representations ([:x:](https://arxiv.org/abs/2304.13722)), ([:paperclip:](https://arxiv.org/pdf/2304.13722.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.13722)) |
| 4.26 | Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System ([:x:](https://arxiv.org/abs/2304.13343)), ([:paperclip:](https://arxiv.org/pdf/2304.13343.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.13343)) |
| 4.26 | TextDeformer: Geometry Manipulation using Text Guidance ([:x:](https://arxiv.org/abs/2304.13348)), ([:paperclip:](https://arxiv.org/pdf/2304.13348.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.13348)) |
| 4.26 | Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery ([:x:](https://arxiv.org/abs/2304.13714)), ([:paperclip:](https://arxiv.org/pdf/2304.13714.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.13714)) |
| 4.26 | Ray Conditioning: Trading Photo-consistency for Photo-realism in Multi-view Image Generation ([:x:](https://arxiv.org/abs/2304.13681)), ([:paperclip:](https://arxiv.org/pdf/2304.13681.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.13681)), ([Project page](https://ray-cond.github.io/)) |
| 4.26 | Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond ([:x:](https://arxiv.org/abs/2304.13712)), ([:paperclip:](https://arxiv.org/pdf/2304.13712.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.13712)), ([:octocat:](https://github.com/Mooler0410/LLMsPracticalGuide)![GitHub Repo stars](https://img.shields.io/github/stars/Mooler0410/LLMsPracticalGuide?style=social)), ([SS](https://www.semanticscholar.org/paper/Harnessing-the-Power-of-LLMs-in-Practice%3A-A-Survey-Yang-Jin/131c6f328c11706de2c43cd16e0b7c5d5e610b6a)) |
| 4.26 | [HuggingChat](https://huggingface.co/chat/) - the first open source alternative to ChatGPT |
| 4.25 | [Time](https://time.com/6273743/thinking-that-could-doom-us-with-ai/) - The 'Don't Look Up' Thinking That Could Doom Us With AI ([Archive](https://archive.is/gMi8q)) |
| 4.25 | AI-assisted coding: Experiments with GPT-4 ([:x:](https://arxiv.org/abs/2304.13187)), ([:paperclip:](https://arxiv.org/pdf/2304.13187.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.13187)) |
| 4.25 | NVIDIA NeMo Guardrails helps enterprises keep applications built on large language models aligned with their safety and security requirements ([Blog](https://blogs.nvidia.com/blog/2023/04/25/ai-chatbot-guardrails-nemo/)), ([:octocat:](https://github.com/NVIDIA/NeMo-Guardrails)![GitHub Repo stars](https://img.shields.io/github/stars/NVIDIA/NeMo-Guardrails?style=social)) |
| 4.25 | Stable and low-precision training for large-scale vision-language models ([:x:](https://arxiv.org/abs/2304.13013)), ([:paperclip:](https://arxiv.org/pdf/2304.13013.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.13013)) |
| 4.25 | AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head ([:x:](https://arxiv.org/abs/2304.12995)), ([:paperclip:](https://arxiv.org/pdf/2304.12995.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.12995)) |
| 4.25 | Answering Questions by Meta-Reasoning over Multiple Chains of Thought ([:x:](https://arxiv.org/abs/2304.13007)), ([:paperclip:](https://arxiv.org/pdf/2304.13007.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.13007)) |
| 4.25 | Patch-based 3D Natural Scene Generation from a Single Example ([:x:](https://arxiv.org/abs/2304.12670)), ([:paperclip:](https://arxiv.org/pdf/2304.12670.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.12670)), ([Project page](http://weiyuli.xyz/Sin3DGen/))  |
| 4.25 | Generative AI at Work - ([NBER](https://www.nber.org/papers/w31161)), ([:paperclip:](https://www.nber.org/system/files/working_papers/w31161/w31161.pdf)) | 
| 4.25 | [Chatbot Arena](https://chat.lmsys.org/?arena) |
| 4.24 | Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model ([:x:](https://arxiv.org/abs/2304.13731)), ([:paperclip:](https://arxiv.org/pdf/2304.13731.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.13731)),([Project page](https://tango-web.github.io/)), ([:octocat:](https://github.com/declare-lab/tango)![GitHub Repo stars](https://img.shields.io/github/stars/declare-lab/tango?style=social)) |
| 4.24 | AI, write an essay for me: A large-scale comparison of human-written versus ChatGPT-generated essays ([:x:](https://arxiv.org/abs/2304.14276)), ([:paperclip:](https://arxiv.org/pdf/2304.14276.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.14276)) |
| 4.24 | Pointersect: Neural Rendering with Cloud-Ray Intersection ([:x:](https://arxiv.org/abs/2304.12390)), ([:paperclip:](https://arxiv.org/pdf/2304.12390.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.12390)), ([web](https://machinelearning.apple.com/research/pointersect)) |
| 4.24 | A Cookbook of Self-Supervised Learning  ([:x:](https://arxiv.org/abs/2304.12210)), ([:paperclip:](https://arxiv.org/pdf/2304.12210.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.12210)) |
| 4.24 | On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research ([:x:](https://arxiv.org/abs/2304.12397)), ([:paperclip:](https://arxiv.org/pdf/2304.12397.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.12397)), ([:octocat:](https://github.com/for-ai/black-box-api-challenges)![GitHub Repo stars](https://img.shields.io/github/stars/for-ai/black-box-api-challenges?style=social)) |
| 4.24 | Towards Realistic Generative 3D Face Models ([:x:](https://arxiv.org/abs/2304.12483)), ([:paperclip:](https://arxiv.org/pdf/2304.12483.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.12483)) |
| 4.24 | TextMesh: Generation of Realistic 3D Meshes From Text Prompts ([:x:](https://arxiv.org/abs/2304.12439)), ([:paperclip:](https://arxiv.org/pdf/2304.12439.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.12439)) |
| 4.24 | Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training Exam (TXIT): Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology ([:x:](https://arxiv.org/abs/2304.11957)), ([:paperclip:](https://arxiv.org/pdf/2304.11957.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.11957)), ([:octocat:](https://github.com/yixinghuang/chatgpt-benchmark-on-radiation-oncology)![GitHub Repo stars](https://img.shields.io/github/stars/yixinghuang/chatgpt-benchmark-on-radiation-oncology?style=social)) |
| 4.24 | Social AGI - SAMANTHA (Self-Reflective Artificial Mind Attuned to Naturalistic Thought and Human Adaptability) ([:octocat:](https://github.com/Methexis-Inc/SocialAGI)![GitHub Repo stars](https://img.shields.io/github/stars/Methexis-Inc/SocialAGI?style=social)) |
| 4.24 | Segment Anything in Medical Images ([:x:](https://arxiv.org/abs/2304.12306)), ([:paperclip:](https://arxiv.org/pdf/2304.12306.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.12306)), ([:octocat:](https://github.com/bowang-lab/MedSAM)![GitHub Repo stars](https://img.shields.io/github/stars/bowang-lab/MedSAM?style=social)) |
| 4.24 | Segment Anything in 3D with NeRFs  ([:x:](https://arxiv.org/abs/2304.12308)), ([:paperclip:](https://arxiv.org/pdf/2304.12308.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.12308)), ([project page](https://jumpat.github.io/SA3D/)) |
| 4.24 | WizardLM: Empowering Large Language Models to Follow Complex Instructions ([:x:](https://arxiv.org/abs/2304.12244)), ([:paperclip:](https://arxiv.org/pdf/2304.12244.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.12244)) |
| 4.24 | Track Anything: Segment Anything Meets Videos ([:x:](https://arxiv.org/abs/2304.11968)), ([:paperclip:](https://arxiv.org/pdf/2304.11968.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.11968)) |
| 4.24 | OpenAI Brand guidelines - ([blog](https://openai.com/brand)) |
| 4.24 | GPT4Tools: Teaching LLM to Use Tools via Self-instruction - ([Project page](https://gpt4tools.github.io/)), ([:octocat:](https://github.com/StevenGrove/GPT4Tools)), ([Video](https://www.youtube.com/watch?v=Qrj94ibQIT8)),  |
| 4.24 | RAM: Relate-Anything-Model ([:octocat:](https://github.com/Luodian/RelateAnything)![GitHub Repo stars](https://img.shields.io/github/stars/Luodian/RelateAnything?style=social)), ([Demo](https://huggingface.co/spaces/mmlab-ntu/relate-anything-model)) |
| 4.24 | [Chart-GPT 1.0](https://www.chartgpt.dev/) |
| 4.23 | Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models ([:x:](https://arxiv.org/abs/2304.11657)), ([:paperclip:](https://arxiv.org/pdf/2304.11657.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.11657)), ([:octocat:](https://github.com/GasolSun36/Iter-CoT)![GitHub Repo stars](https://img.shields.io/github/stars/GasolSun36/Iter-CoT?style=social)) |
| 4.23 | Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness ([:x:](https://arxiv.org/abs/2304.11633)), ([:paperclip:](https://arxiv.org/pdf/2304.11633.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.11633)) |
| 4.22 | Boosting Theory-of-Mind Performance in Large Language Models via Prompting  ([:x:](https://arxiv.org/abs/2304.11490)), ([:paperclip:](https://arxiv.org/pdf/2304.11490.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.11490)) |
| 4.22 | LaMP: When Large Language Models Meet Personalization ([:x:](https://arxiv.org/abs/2304.11406)), ([:paperclip:](https://arxiv.org/pdf/2304.11406.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.11406)), ([Project page](https://lamp-benchmark.github.io/index.html)), ([Download](https://lamp-benchmark.github.io/download)), ([Leaderboard](https://lamp-benchmark.github.io/leaderboard)), ([:octocat:](https://github.com/LaMP-Benchmark/LaMP)![GitHub Repo stars](https://img.shields.io/github/stars/LaMP-Benchmark/LaMP?style=social)) |
| 4.22 | Finetuning Large Language Models ([Blog](https://magazine.sebastianraschka.com/p/finetuning-large-language-models)) |
| 4.21 | Can GPT-4 Perform Neural Architecture Search? ([:x:](https://arxiv.org/abs/2304.10970)), ([:paperclip:](https://arxiv.org/pdf/2304.10970.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.10970)) |
| 4.21 | Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition ([:x:](https://arxiv.org/abs/2304.10977)), ([:paperclip:](https://arxiv.org/pdf/2304.10977.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.10977)) |
| 4.21 | Emergent and Predictable Memorization in Large Language Models  ([:x:](https://arxiv.org/abs/2304.11158)), ([:paperclip:](https://arxiv.org/pdf/2304.11158.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.11158)) |
| 4.21 | CLaMP: Contrastive Language-Music Pre-training for Cross-Modal Symbolic Music Information Retrieval  ([:x:](https://arxiv.org/abs/2304.11029)), ([:paperclip:](https://arxiv.org/pdf/2304.11029.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.11029)) |
| 4.21 | Bard now helps you code with support for 20+ langs (Python, C++, JS, Go, etc.). ([Blog](https://blog.google/technology/ai/code-with-bard/)) |
| 4.21 | Inducing anxiety in large language models increases exploration and bias ([:x:](https://arxiv.org/abs/2304.11111)), ([:paperclip:](https://arxiv.org/pdf/2304.11111.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.11111)) |
| 4.20 | Is ChatGPT a Good Recommender? A Preliminary Study ([:x:](https://arxiv.org/abs/2304.10149)), ([:paperclip:](https://arxiv.org/pdf/2304.10149.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.10149)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/is-chatgpt-a-good-recommender-a-preliminary)) |
| 4.20 | Why Does ChatGPT Fall Short in Answering Questions Faithfully? ([:x:](https://arxiv.org/abs/2304.10513)), ([:paperclip:](https://arxiv.org/pdf/2304.10513.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.10513)) |
| 4.20 | [FinChat.io](https://finchat.io/chats/) - The Chat GPT for Finance |
| 4.20 | LlamaAcademy: Teaching Llamas How to Code ([:octocat:](https://github.com/danielgross/LlamaAcademy)![GitHub Repo stars](https://img.shields.io/github/stars/danielgross/LlamaAcademy?style=social)) |
| 4.20 | Announcing Google DeepMind: DeepMind + Brain = Google DeepMind ([Blog](https://www.deepmind.com/blog/announcing-google-deepmind)) |
| 4.20 | "Can ChatGPT Diagnose Me?" How Large Language Models will Transform Clinical Care. Thursday, April 27th, 2023 ([RSVP](https://aimi.stanford.edu/events/can-chatgpt-diagnose-me-how-large-language-models-will-transform-clinical-care)) |
| 4.20 | StableLM: Stability AI Language Models ([:octocat:](https://github.com/stability-AI/stableLM/)![GitHub Repo stars](https://img.shields.io/github/stars/stability-AI/stableLM?style=social)), ([Blog](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models)) |
| 4.19 | GeneGPT: Teaching Large Language Models to Use NCBI Web APIs ([:x:](https://arxiv.org/abs/2304.09667)), ([:paperclip:](https://arxiv.org/pdf/2304.09667.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.09667)), ([:house:](https://huggingface.co/papers/2304.09667)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/genegpt-teaching-large-language-models-to-use)) |
| 4.19 | Large Language Models in Medical Education: Opportunities, Challenges, and Future Directions (JMIR [doi:10.2196/48291](https://mededu.jmir.org/2023/1/e48291)), ([PDF](https://mededu.jmir.org/2023/1/e48291/PDF)) |
| 4.19 | Fundamental Limitations of Alignment in Large Language Models ([:x:](https://arxiv.org/abs/2304.11082)), ([:paperclip:](https://arxiv.org/pdf/2304.11082.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.11082)) |
| 4.19 | Scaling Transformer to 1M tokens and beyond with RMT ([:x:](https://arxiv.org/abs/2304.11062)), ([:paperclip:](https://arxiv.org/pdf/2304.11062.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.11062)), ([:octocat:](https://github.com/booydar/t5-experiments/tree/scaling-report)) |
| 4.19 | Occupational Heterogeneity in Exposure to Generative AI - ([paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4414065)), ([:paperclip:](https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID4414065_code2763040.pdf?abstractid=4414065&mirid=1)) |
| 4.19 | The Unintended Consequences of Censoring Digital Technology -- Evidence from Italy's ChatGPT Ban ([:x:](https://arxiv.org/abs/2304.09339)), ([:paperclip:](https://arxiv.org/pdf/2304.09339.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.09339)) |
| 4.19 | CompressGPT: Decrease Token Usage by ~70% ([blog](https://musings.yasyf.com/compressgpt-decrease-token-usage-by-70/)) |
| 4.19 | Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes ([:x:](https://arxiv.org/abs/2304.09433)), ([:paperclip:](https://arxiv.org/pdf/2304.09433.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.09433)), ([:octocat:](https://github.com/HazyResearch/evaporate)(https://img.shields.io/github/stars/HazyResearch/evaporate?style=social)) |
| 4.19 | LLM as A Robotic Brain: Unifying Egocentric Memory and Control ([:x:](https://arxiv.org/abs/2304.09349)), ([:paperclip:](https://arxiv.org/pdf/2304.09349.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.09349)) |
| 4.19 | Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent ([:x:](https://arxiv.org/abs/2304.09542)), ([:paperclip:](https://arxiv.org/pdf/2304.09542.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.09542)) |
| 4.19 | Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models ([:x:](https://arxiv.org/abs/2304.09842)), ([:paperclip:](https://arxiv.org/pdf/2304.09842.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.09842)), ([project page](https://chameleon-llm.github.io/)), ([:octocat:](https://github.com/lupantech/chameleon-llm)![GitHub Repo stars](https://img.shields.io/github/stars/lupantech/chameleon-llm?style=social)) |
| 4.19 | h2oai's LLM repositories - ([h2ogpt](https://github.com/h2oai/h2ogpt)), ([h2o-llmstudio](https://github.com/h2oai/h2o-llmstudio)), ([Huggingface](https://huggingface.co/h2oai)) | 
| 4.19 | Evaluating Verifiability in Generative Search Engines ([:x:](https://arxiv.org/abs/2304.09848)), ([:paperclip:](https://arxiv.org/pdf/2304.09848.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.09848)) |
| 4.19 | How to train your own Large Language Models ([Blog](https://blog.replit.com/llm-training)) |
| 4.19 | [AI Playground](https://play.vercel.ai/r/mWjP5Dt) from Vercel Labs ([tweet](https://twitter.com/vercel/status/1648451494440742917)) |
| 4.19 | StanfordBDHG HealthGPT ([tweet](https://twitter.com/varunshenoy_/status/1648374949537775616)), ([:octocat:](https://github.com/StanfordBDHG/HealthGPT)![GitHub Repo stars](https://img.shields.io/github/stars/StanfordBDHG/HealthGPT?style=social)) |
| 4.19 | GPT4All-J : the first Apache-2 Licensed Chatbot that runs locally on your machine ([:octocat:](https://github.com/nomic-ai/gpt4all)![GitHub Repo stars](https://img.shields.io/github/stars/nomic-ai/gpt4all?style=social)), ([:paperclip:](https://static.nomic.ai/gpt4all/2023_GPT4All-J_Technical_Report_2.pdf)) | 
| 4.19 | PersonalPrivate.AI - system to advise on new patent ideas ([tweet](https://twitter.com/BrianRoemmele/status/1648378237633073152)) |
| 4.18 | Computer-Vision Benchmark Segment-Anything Model (SAM) in Medical Images: Accuracy in 12 Datasets ([:x:](https://arxiv.org/abs/2304.09324)), ([:paperclip:](https://arxiv.org/pdf/2304.09324.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.09324)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/accuracy-of-segment-anything-model-sam-in)) |
| 4.18 | Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task ([:x:](https://arxiv.org/abs/2304.09138)), ([:paperclip:](https://arxiv.org/pdf/2304.09138.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.09138)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/exploring-the-trade-offs-unified-large)) |
| 4.18 | [Economist](https://www.economist.com/by-invitation/2023/04/18/the-world-needs-an-international-agency-for-artificial-intelligence-say-two-ai-experts) - The world needs an international agency for artificial intelligence, say two AI experts ([Archive](https://archive.is/jWEJ8#selection-1039.0-1039.87)) |
| 4.18 | CancerGPT: Few-shot Drug Pair Synergy Prediction using Large Pre-trained Language Models ([:x:](https://arxiv.org/abs/2304.10946)), ([:paperclip:](https://arxiv.org/pdf/2304.10946.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.10946)) |
| 4.18 | Think Before You Act: Unified Policy for Interleaving Language Reasoning with Actions ([:x:](https://arxiv.org/abs/2304.11063)), ([:paperclip:](https://arxiv.org/pdf/2304.11063.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.11063)) |
| 4.18 | Nature - [Why open-source generative AI models are an ethical way forward for science](https://www.nature.com/articles/d41586-023-01295-4) |
| 4.18 | Autonomous Agents(BabyAGI, AutoGPT) & Agent Simulations(CAMEL, Generative Agents) ([Blog](https://blog.langchain.dev/agents-round/)) |
| 4.18 | AutoTaskFormer: Searching Vision Transformers for Multi-task Learning ([:x:](https://arxiv.org/abs/2304.08756)), ([:paperclip:](https://arxiv.org/pdf/2304.08756.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.08756)) |
| 4.18 | SAM Fails to Segment Anything? -- SAM-Adapter: Adapting SAM in Underperformed Scenes: Camouflage, Shadow, and More ([:x:](https://arxiv.org/abs/2304.09148)), ([:paperclip:](https://arxiv.org/pdf/2304.09148.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.09148)), ([Project page](https://tianrun-chen.github.io/SAM-Adaptor/)) |
| 4.18 | Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models ([:x:](https://arxiv.org/abs/2304.08818)), ([:paperclip:](https://arxiv.org/pdf/2304.08818.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.08818)), ([Project page](https://research.nvidia.com/labs/toronto-ai/VideoLDM/)) | 
| 4.18 | Google - Differentially private heatmaps ([Blog](https://ai.googleblog.com/2023/04/differentially-private-heatmaps.html)) |
| 4.18 | [The Complete Beginners Guide To Autonomous Agents](https://www.mattprd.com/p/the-complete-beginners-guide-to-autonomous-agents) |
| 4.18 | Llama Lab - A repo dedicated to building cutting-edge AGI projects: llama_agi (inspired by babyagi) and auto_llama (inspired by autogpt) ([:octocat:](https://github.com/run-llama/llama-lab)![GitHub Repo stars](https://img.shields.io/github/stars/run-llama/llama-lab?style=social)), ([Llama Hub](https://llamahub.ai/)) |
| 4.18 | Elon Musk to start ChatGPT rival called “TruthGPT” ([tweet](https://twitter.com/theLionary/status/1648088563874156545)) |
| 4.17 | InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions (CVPR2023 [:x:](https://arxiv.org/abs/2311.05778)), ([:book:](https://browse.arxiv.org/pdf/2311.05778.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.05778.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.05778)), ([:house:](https://huggingface.co/papers/2311.05778)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/internimage-exploring-large-scale-vision)), ([:octocat:](https://github.com/opengvlab/internimage)![GitHub Repo stars](https://img.shields.io/github/stars/opengvlab/internimage?style=social))  |
| 4.17 | MasaCtrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing ([:x:](https://arxiv.org/abs/2304.08465)), ([:paperclip:](https://arxiv.org/pdf/2304.08465.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.08465)), ([:house:](https://huggingface.co/papers/2304.08465)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/masactrl-tuning-free-mutual-self-attention)), ([:octocat:](https://github.com/tencentarc/masactrl)![GitHub Repo stars](https://img.shields.io/github/stars/tencentarc/masactrl?style=social)) |
| 4.17 | Notice of the Cyberspace Administration of China on Public Comments on the "Administrative Measures for Generative Artificial Intelligence Services (Draft for Comment)" ([Announcement](http://www.cac.gov.cn/2023-04/11/c_1682854275475410.htm)) |
| 4.17 | Pretrained Language Models as Visual Planners for Human Assistance ([:x:](https://arxiv.org/abs/2304.09179)), ([:paperclip:](https://arxiv.org/pdf/2304.09179.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.09179)) |
| 4.17 | An Evaluation on Large Language Model Outputs: Discourse and Memorization ([:x:](https://arxiv.org/abs/2304.08637)), ([:paperclip:](https://arxiv.org/pdf/2304.08637.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.08637)) |
| 4.17 | [Epic, Microsoft bring generative AI to EHRs](https://digitalhealth.modernhealthcare.com/digital-health/himss23-epic-microsoft-bring-openais-gpt-4-ehrs) - ([Microsoft announcement](Microsoft and Epic expand strategic collaboration with integration of Azure OpenAI Service)) |
| 4.17 | BenchMD: A Benchmark for Modality-Agnostic Learning on Medical Images and Sensors ([:x:](https://arxiv.org/abs/2304.08486)), ([:paperclip:](https://arxiv.org/pdf/2304.08486.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.08486)) | 
| 4.17 | Towards Robust Prompts on Vision-Language Models ([:x:](https://arxiv.org/abs/2304.08479)), ([:paperclip:](https://arxiv.org/pdf/2304.08479.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.08479)) | 
| 4.17 | Tool Learning with Foundation Models ([:x:](https://arxiv.org/abs/2304.08354)), ([:paperclip:](https://arxiv.org/pdf/2304.08354.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.08354)), ([:octocat:](https://github.com/OpenBMB/BMTools)![GitHub Repo stars](https://img.shields.io/github/stars/OpenBMB/BMTools?style=social)) | 
| 4.17 | Low-code LLM: Visual Programming over LLMs ([:x:](https://arxiv.org/abs/2304.08103)), ([:paperclip:](https://arxiv.org/pdf/2304.08103.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.08103)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/low-code-llm-visual-programming-over-llms)), ([:octocat:](https://github.com/microsoft/visual-chatgpt)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/visual-chatgpt?style=social)) |
| 4.17 | Wired - [OpenAI’s CEO Says the Age of Giant AI Models Is Already Over](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/) |
| 4.17 | Synthetic Data from Diffusion Models Improves ImageNet Classification ([:x:](https://arxiv.org/abs/2304.08466)), ([:paperclip:](https://arxiv.org/pdf/2304.08466.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.08466)) |
| 4.17 | RedPajama-Data: An Open Source Recipe to Reproduce LLaMA training dataset ([GitHib](https://github.com/togethercomputer/RedPajama-Data)) |
| 4.17 | Visual Instruction Tuning  ([:x:](https://arxiv.org/abs/2304.08485)), ([:paperclip:](https://arxiv.org/pdf/2304.08485.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.08485)), ([:octocat:](https://github.com/haotian-liu/LLaVA)![GitHub Repo stars](https://img.shields.io/github/stars/haotian-liu/LLaVA?style=social)), ([Dataset](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K)), ([Model](https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0)), ([Project page](https://llava-vl.github.io/)), ([Demo](https://llava.hliu.cc/)) |
| 4.17 | Learning to Compress Prompts with Gist Tokens ([:x:](https://arxiv.org/abs/2304.08467)), ([:paperclip:](https://arxiv.org/pdf/2304.08467.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.08467)) |
| 4.17 | ImpressionGPT: An Iterative Optimizing Framework for Radiology Report Summarization with ChatGPT ([:x:](https://arxiv.org/abs/2304.08448)), ([:paperclip:](https://arxiv.org/pdf/2304.08448.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.08448)) |
| 4.17 | Meta - DINOv2: State-of-the-art computer vision models with self-supervised learning ([blog](https://ai.facebook.com/blog/dino-v2-computer-vision-self-supervised-learning/)), ([:octocat:](https://github.com/facebookresearch/dinov2)![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/dinov2?style=social)), ([Demo](https://dinov2.metademolab.com/)), ([:x:](https://arxiv.org/abs/2304.07193)), ([:paperclip:](https://arxiv.org/pdf/2304.07193.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.07193)) |
| 4.17 | [TypingMind](https://www.typingmind.com/) - A better UI for ChatGPT ([tweet](https://twitter.com/tdinh_me/status/1647820035820523523)) |
| 4.16 | Understanding Large Language Models ([Blog](https://magazine.sebastianraschka.com/p/understanding-large-language-models)) |
| 4.16 | INSIGHT - an autonomous AI that can do medical research ([:octocat:](https://github.com/oneil512/INSIGHT)![GitHub Repo stars](https://img.shields.io/github/stars/oneil512/INSIGHT?style=social)) |
| 4.16 | GPT4free - use ChatGPT, for free!! - ([:octocat:](https://github.com/xtekky/gpt4free)![GitHub Repo stars](https://img.shields.io/github/stars/xtekky/gpt4free?style=social)) | 
| 4.16 | Solving Math Word Problems by Combining Language Models With Symbolic Solvers ([:x:](https://arxiv.org/abs/2304.09102)), ([:paperclip:](https://arxiv.org/pdf/2304.09102.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.09102)) |
| 4.16 | ChatPLUG: Open-Domain Generative Dialogue System with Internet-Augmented Instruction Tuning for Digital Human ([:x:](https://arxiv.org/abs/2304.07849)), ([:paperclip:](https://arxiv.org/pdf/2304.07849.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.07849)) |
| 4.16 | Driving and suppressing the human language network using large language models ([bioRxiv](https://www.biorxiv.org/content/10.1101/2023.04.16.537080v1)), ([:paperclip:](https://www.biorxiv.org/content/10.1101/2023.04.16.537080v1.full.pdf)) |
| 4.16 | MultiGPT ([:octocat:](https://github.com/rumpfmax/Multi-GPT)![GitHub Repo stars](https://img.shields.io/github/stars/rumpfmax/Multi-GPT?style=social)). ([tweet](https://twitter.com/md_rumpf/status/1647911393796956162)) |
| 4.16 | OpenAssistant Conversations - Democratizing Large Language Model Alignment ([:paperclip:](https://drive.google.com/file/d/10iR5hKwFqAKhL3umx8muOWSRm7hs5FqX/view)), ([YouTube](https://www.youtube.com/watch?v=Ft9_RsKxrG4)), ([hacker news](https://news.ycombinator.com/item?id=35582417)) |
| 4.16 | Auto-evaluator - lightweight evaluation tool for question-answering using Langchain ([:octocat:](https://github.com/PineappleExpress808/auto-evaluator)![GitHub Repo stars](https://img.shields.io/github/stars/PineappleExpress808/auto-evaluator?style=social)) | 
| 4.16 | NYT - [Google Devising Radical Search Changes to Beat Back A.I. Rivals](https://www.nytimes.com/2023/04/16/technology/google-search-engine-ai.html) ([Archive](https://archive.is/ti9Ns)) |
| 4.15 | Brex's Prompt Engineering Guide ([:octocat:](https://github.com/brexhq/prompt-engineering)![GitHub Repo stars](https://img.shields.io/github/stars/brexhq/prompt-engineering?style=social)) |
| 4.15 | [Graphologue](https://twitter.com/HaijunXia/status/1646917869115166720) and [Sensecape](https://twitter.com/HaijunXia/status/1646919380704559104) by [UCSD Creativity Lab](https://creativity.ucsd.edu/ai) |
| 4.15 | Tractable Control for Autoregressive Language Generation ([:x:](https://arxiv.org/abs/2304.07438)), ([:paperclip:](https://arxiv.org/pdf/2304.07438.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.07438)) |
| 4.15 | Web LLM - language model chats directly onto web browsers ([Site](https://mlc.ai/web-llm/)), ([:octocat:](https://github.com/mlc-ai/web-llm#how)![GitHub Repo stars](https://img.shields.io/github/stars/mlc-ai/web-llm#how?style=social)) |
| 4.15 | MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models ([Project page](https://minigpt-4.github.io/)). ([Paper]()), ([:octocat:](https://github.com/Vision-CAIR/MiniGPT-4)![GitHub Repo stars](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4?style=social)), ([YouTube](https://www.youtube.com/watch?v=__tftoxpBAw)) |
| 4.15 | OpenAssistant - The world's largest open-source replication of ChatGPT ([site](https://open-assistant.io/)), ([:octocat:](https://github.com/LAION-AI/Open-Assistant)![GitHub Repo stars](https://img.shields.io/github/stars/LAION-AI/Open-Assistant?style=social)), ([Dataset - OASST1](https://huggingface.co/datasets/OpenAssistant/oasst1)), ([Paper](https://ykilcher.com/oa-paper)), ([YouTube](https://www.youtube.com/watch?v=ddG2fM9i4Kk&feature=youtu.be)), ([Reddit](https://www.reddit.com/r/OpenAssistant/)) |
| 4.14 | MedAlpaca -- An Open-Source Collection of Medical Conversational AI Models and Training Data ([:x:](https://arxiv.org/abs/2304.08247)), ([:paperclip:](https://arxiv.org/pdf/2304.08247.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.08247)), ([:trophy:papers with code](https://paperswithcode.com/paper/medalpaca-an-open-source-collection-of)) |
| 4.14 | HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge ([:x:](https://arxiv.org/abs/2304.06975)), ([:paperclip:](https://arxiv.org/pdf/2304.06975.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.06975)), ([:trophy:papers with code](https://paperswithcode.com/paper/huatuo-tuning-llama-model-with-chinese)) |
| 4.14 | ChatGPT: Applications, Opportunities, and Threats ([:x:](https://arxiv.org/abs/2304.09103)), ([:paperclip:](https://arxiv.org/pdf/2304.09103.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.09103)) |
| 4.14 | Swin3D: A Pretrained Transformer Backbone for 3D Indoor Scene Understanding ([:x:](https://arxiv.org/abs/2304.06906)), ([:paperclip:](https://arxiv.org/pdf/2304.06906.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.06906)) |
| 4.14 | OpenBB Terminal V3.0.0rc2 - ([:octocat:](https://github.com/OpenBB-finance/OpenBBTerminal/releases/tag/v3.0.0rc2)![GitHub Repo stars](https://img.shields.io/github/stars/OpenBB-finance/OpenBBTerminal?style=social)) |
| 4.14 | Delta Denoising Score  ([:x:](https://arxiv.org/abs/2304.07090)), ([:paperclip:](https://arxiv.org/pdf/2304.07090.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.07090)), ([Project page](https://delta-denoising-score.github.io/)) |
| 4.14 | DINOv2: Learning Robust Visual Features without Supervision ([:x:](https://arxiv.org/abs/2304.07193)), ([:paperclip:](https://arxiv.org/pdf/2304.07193.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.07193)) |
| 4.14 | Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text  ([:x:](https://arxiv.org/abs/2304.06939)), ([:paperclip:](https://arxiv.org/pdf/2304.06939.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.06939)), ([:octocat:](https://github.com/allenai/mmc4)![GitHub Repo stars](https://img.shields.io/github/stars/allenai/mmc4?style=social)) |
| 4.14 | WSJ - [Elon Musk Creates New Artificial Intelligence Company X.AI](https://www.wsj.com/articles/elon-musks-new-artificial-intelligence-business-x-ai-incorporates-in-nevada-962c7c2f) ([archive](https://archive.is/qzbbb)), ([FT](https://www.ft.com/content/2a96995b-c799-4281-8b60-b235e84aefe4)) |
| 4.14 | Google Med-PaLM 2 - [A responsible path to generative AI in healthcare](https://cloud.google.com/blog/topics/healthcare-life-sciences/sharing-google-med-palm-2-medical-large-language-model?hl=en) |
| 4.14 | Meta's open source Animated Drawings - ([Blog](https://developers.facebook.com/blog/post/2023/04/13/meta-os-animated-drawings/)) |
| 4.14 | ControlNet v1.1 nightly - ([:octocat:](https://github.com/lllyasviel/ControlNet-v1-1-nightly)![GitHub Repo stars](https://img.shields.io/github/stars/lllyasviel/ControlNet-v1-1-nightly?style=social)) |
| 4.13 | Teenage-AGI ([:octocat:](https://github.com/seanpixel/Teenage-AGI)![GitHub Repo stars](https://img.shields.io/github/stars/seanpixel/Teenage-AGI?style=social)) |
| 4.13 | Boosted Prompt Ensembles for Large Language Models ([:x:](https://arxiv.org/abs/2304.05970)), ([:paperclip:](https://arxiv.org/pdf/2304.05970.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.05970)) |
| 4.13 | ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning ([:x:](https://arxiv.org/abs/2304.06588)), ([:paperclip:](https://arxiv.org/pdf/2304.06588.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.06588)) |
| 4.13 | Soundini: Sound-Guided Diffusion for Natural Video Editing ([:x:](https://arxiv.org/abs/2304.06818)), ([:paperclip:](https://arxiv.org/pdf/2304.06818.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.06818)), ([Project page](https://kuai-lab.github.io/soundini-gallery/)) |
| 4.13 | Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study ([:x:](https://arxiv.org/abs/2304.06762)), ([:paperclip:](https://arxiv.org/pdf/2304.06762.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.06762)), ([:octocat:](https://github.com/geekyutao/Inpaint-Anything)![GitHub Repo stars](https://img.shields.io/github/stars/geekyutao/Inpaint-Anything?style=social)) |
| 4.13 | Inpaint Anything: Segment Anything Meets Image Inpainting ([:x:](https://arxiv.org/abs/2304.06790)), ([:paperclip:](https://arxiv.org/pdf/2304.06790.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.06790)), ([:octocat:](https://github.com/NVIDIA/Megatron-LM#retro)![GitHub Repo stars](https://img.shields.io/github/stars/NVIDIA/Megatron-LM?style=social)) |
| 4.13 | [GoalGPT](https://beta.nando.ai/goalgpt.php) by Nando.ai |
| 4.13 | Power-seeking can be probable and predictive for trained agents ([:x:](https://arxiv.org/abs/2304.06528)), ([:paperclip:](https://arxiv.org/pdf/2304.06528.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.06528)) |
| 4.13 | [GoalGPT](https://beta.nando.ai/goalgpt.php) by Nando.ai |
| 4.13 | [Stable Diffusion XL Beta Available for API Customers and DreamStudio Users](https://stability.ai/blog/stable-diffusion-xl-beta-available-for-api-customers-and-dreamstudio-users) |
| 4.13 | [NAB 2023: Introducing Text-Based Editing in Premiere Pro, Properties panel in After Effects, and much more](https://blog.adobe.com/en/publish/2023/04/13/nab-2023-introducing-text-based-editing-premiere-pro-properties-panel-after-effects-more) |
| 4.13 | [Announcing New Tools for Building with Generative AI on AWS](https://aws.amazon.com/ko/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/) - Amazon LLM (Titan), AWS fine-tuning model (Bedrock), Amazon copilot competitor (Code whisperer) |
| 4.13 | FT - [We must slow down the race to God-like AI](https://www.ft.com/content/03895dc4-a3b7-481e-95cc-336a524f2ac2) ([archive](https://archive.is/jFfBQ#selection-1443.0-1443.41)) |
| 4.13 | Segment Everything Everywhere All at Once ([:x:](https://arxiv.org/abs/2304.06718)), ([:paperclip:](https://arxiv.org/pdf/2304.06718.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.06718)) |
| 4.13 | Expressive Text-to-Image Generation with Rich Text ([:x:](https://arxiv.org/abs/2304.06720)), ([:paperclip:](https://arxiv.org/pdf/2304.06720.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.06720)), ([Project page](https://rich-text-to-image.github.io/)) |
| 4.13 | AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models ([:x:](https://arxiv.org/abs/2304.06364)), ([:paperclip:](https://arxiv.org/pdf/2304.06364.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.06364)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/agieval-a-human-centric-benchmark-for)), ([SS](https://www.semanticscholar.org/paper/AGIEval%3A-A-Human-Centric-Benchmark-for-Evaluating-Zhong-Cui/68c834c19cd126bbd6d25a3572d7205cfed76271)), ([:octocat:](https://github.com/ruixiangcui/agieval)![GitHub Repo stars](https://img.shields.io/github/stars/ruixiangcui/agieval?style=social)) |
| 4.12 | Can Large Language Models Transform Computational Social Science? ([:x:](https://arxiv.org/abs/2305.03514)), ([:paperclip:](https://arxiv.org/pdf/2305.03514.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2305.03514)) | 
| 4.12 | Galactic ChitChat: Using Large Language Models to Converse with Astronomy Literature ([:x:](https://arxiv.org/abs/2304.05406)), ([:paperclip:](https://arxiv.org/pdf/2304.05406.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.05406)) | 
| 4.12 | Performance of ChatGPT, GPT-4, and Google Bard on a Neurosurgery Oral Boards Preparation Question Bank ([medRxiv](https://www.medrxiv.org/content/10.1101/2023.04.06.23288265v1)), ([:paperclip:](https://www.medrxiv.org/content/10.1101/2023.04.06.23288265v1.full.pdf)) |
| 4.12 | ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning ([:x:](https://arxiv.org/abs/2304.05613)), ([:paperclip:](https://arxiv.org/pdf/2304.05613.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.05613)) |
| 4.12 | Foundation models for generalist medical artificial intelligence (Nature [https://doi.org/10.1038/s41586-023-05881-4](https://www.nature.com/articles/s41586-023-05881-4)), ([:paperclip:](https://www.nature.com/articles/s41586-023-05881-4.pdf)), ([SS](https://www.semanticscholar.org/paper/Foundation-models-for-generalist-medical-artificial-Moor-Banerjee/9faa2b0e5cb93f20df0555c3c350fab0b2eccf3a)) |
| 4.12 | Dolly v2 - 12B parameter language model ([Model weight](https://huggingface.co/databricks/dolly-v2-12b)), ([:octocat:](https://github.com/databrickslabs/dolly/tree/master/data)![GitHub Repo stars](https://img.shields.io/github/stars/databrickslabs/dolly?style=social)), ([Blog](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)) | 
| 4.11 | Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond ([:x:](https://arxiv.org/abs/2304.04968)), ([:paperclip:](https://arxiv.org/pdf/2304.04968.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.04968)), ([Project page](https://perp-neg.github.io/)), ([:octocat:](https://github.com/Perp-Neg/Perp-Neg-stablediffusion)![GitHub Repo stars](https://img.shields.io/github/stars/Perp-Neg/Perp-Neg-stablediffusion?style=social)), ([Colab](https://github.com/Perp-Neg/Perp-Neg-stablediffusion/blob/main/notebooks/demo.ipynb)), ([Hugging face](https://huggingface.co/spaces/rezaarmand/Perp-Neg)) | 
| 4.11 | Toxicity in ChatGPT: Analyzing Persona-assigned Language Models ([:x:](https://arxiv.org/abs/2304.05335)), ([:paperclip:](https://arxiv.org/pdf/2304.05335.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.05335)) | 
| 4.11 | Multi-step Jailbreaking Privacy Attacks on ChatGPT ([:x:](https://arxiv.org/abs/2304.05197)), ([:paperclip:](https://arxiv.org/pdf/2304.05197.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.05197)) | 
| 4.11 | [Building LLM applications for production](https://huyenchip.com/2023/04/11/llm-engineering.html) |
| 4.11 | Emergent autonomous scientific research capabilities of large language models ([:x:](https://arxiv.org/abs/2304.05332)), ([:paperclip:](https://arxiv.org/pdf/2304.05332.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.05332)) |
| 4.11 | [OpenAI’s Bug Bounty Program](https://openai.com/blog/bug-bounty-program) |
| 4.11 | [NTIA’s “AI Accountability Policy Request for Comment”](https://ntia.gov/issues/artificial-intelligence/request-for-comments) |
| 4.11 | WSJ - [Biden Administration Weighs Possible Rules for AI Tools Like ChatGPT](https://www.wsj.com/amp/articles/biden-administration-weighs-possible-rules-for-ai-tools-like-chatgpt-46f8257b?fbclid=IwAR1GauvAq8cuHIQQlZ8dlxiKkYuszBMPHqr_K6iZiAeTz2yCjGu9vP_S3cc), ([archive](https://archive.is/6phfS)) |
| 4.11 | ChemCrow: Augmenting large-language models with chemistry tools ([:x:](https://arxiv.org/abs/2304.05376)), ([:paperclip:](https://arxiv.org/pdf/2304.05376.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.05376)) |
| 4.11 | [LangChainJS Support for Multiple JS Environments](https://blog.langchain.dev/js-envs/) ([tweet](https://twitter.com/LangChainAI/status/1645831073358815232)) |
| 4.11 | Teaching Large Language Models to Self-Debug ([:x:](https://arxiv.org/abs/2304.05128)), ([:paperclip:](https://arxiv.org/pdf/2304.05128.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.05128)) |
| 4.10 | Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models ([Paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4412788)), ([:paperclip:](https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID4429709_code2675263.pdf?abstractid=4412788&mirid=1)) |
| 4.10 | On the Possibilities of AI-Generated Text Detection ([:x:](https://arxiv.org/abs/2304.04736)), ([:paperclip:](https://arxiv.org/pdf/2304.04736.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.04736)) |
| 4.10 | OpenAGI: When LLM Meets Domain Experts (NeurIPS2003 [:x:](https://arxiv.org/abs/2304.04370)), ([:paperclip:](https://arxiv.org/pdf/2304.04370.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.04370)), ([:octocat:](https://github.com/agiresearch/OpenAGI)![GitHub Repo stars](https://img.shields.io/github/stars/agiresearch/OpenAGI?style=social)) |
| 4.9 | ChatAll - oncurrently chat with ChatGPT, Bing Chat, bard, Alpaca, Vincuna, Claude, ChatGLM, MOSS, iFlytek Spark, ERNIE and more, discover the best answers ([:octocat:](https://github.com/sunner/ChatALL)![GitHub Repo stars](https://img.shields.io/github/stars/sunner/ChatALL?style=social)) |
| 4.9 | BabyAGI JS - ([:octocat:](https://github.com/ericciarla/babyagijs)![GitHub Repo stars](https://img.shields.io/github/stars/ericciarla/babyagijs?style=social)) |
| 4.9 | AgentGPT - Auto-GPT directly in the browser ([tweet](https://twitter.com/asimdotshrestha/status/1644883727707959296)), ([:octocat:](https://github.com/reworkd/AgentGPT)![GitHub Repo stars](https://img.shields.io/github/stars/reworkd/AgentGPT?style=social)), ([demo](https://agentgpt.reworkd.ai/)) |
| 4.8 | [A Recipe for Training Large Models](https://wandb.ai/craiyon/report/reports/Recipe-Training-Large-Models--VmlldzozNjc4MzQz) |
| 4.7 | [SuperPrompt Engineer Encourages ChatGPT Hallucinations](https://metanews.com/superprompt-engineer-encourages-chatgpt-hallucinations/) |
| 4.7 | Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster ([:x:](https://arxiv.org/abs/2304.03208)), ([:paperclip:](https://arxiv.org/pdf/2304.03208.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.03208)) |
| 4.7 | Why think step-by-step? Reasoning emerges from the locality of experience ([:x:](https://arxiv.org/abs/2304.03843)), ([:paperclip:](https://arxiv.org/pdf/2304.03843.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.03843)) |
| 4.7 | Generative Agents: Interactive Simulacra of Human Behavior ([:x:](https://arxiv.org/abs/2304.03442)), ([:paperclip:](https://arxiv.org/pdf/2304.03442.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.03442)), ([Project](https://reverie.herokuapp.com/arXiv_Demo/)), ([:octocat:](https://github.com/joonspk-research/generative_agents)![GitHub Repo stars](https://img.shields.io/github/stars/joonspk-research/generative_agents?style=social)) | 
| 4.7 | Vicuna-7B: small, efficient, yet capable ([:octocat:](https://github.com/lm-sys/FastChat)![GitHub Repo stars](https://img.shields.io/github/stars/lm-sys/FastChat?style=social)), ([Weight](https://huggingface.co/lmsys/vicuna-7b-delta-v0)) |
| 4.7 | StackLlama ([Blog](https://huggingface.co/blog/stackllama)), ([Demo](https://huggingface.co/spaces/trl-lib/stack-llama)), ([:octocat:](https://github.com/lvwerra/trl/tree/main/examples/stack_llama/scripts)![GitHub Repo stars](https://img.shields.io/github/stars/lvwerra/trl?style=social)) |
| 4.7 | SegGPT: Segmenting Everything In Context ([:x:](https://arxiv.org/abs/2304.03284)), ([:paperclip:](https://arxiv.org/pdf/2304.03284.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.03284)), ([:octocat:](https://github.com/baaivision/Painter)![GitHub Repo stars](https://img.shields.io/github/stars/baaivision/Painter?style=social)), ([Demo](https://huggingface.co/spaces/BAAI/SegGPT)) |
| 4.6 | Synthetic Data in Healthcare ([:x:](https://arxiv.org/abs/2304.03243)), ([:paperclip:](https://arxiv.org/pdf/2304.03243.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.03243)), ([:house:](https://huggingface.co/papers/2304.03243)), ([:eight_spoked_asterisk:]()) |
| 4.6 | Chrome ships WebGPU ([Blog](https://developer.chrome.com/blog/webgpu-release/)) |
| 4.6 | GPT detectors are biased against non-native English writers ([:x:](https://arxiv.org/abs/2304.02819)), ([:paperclip:](https://arxiv.org/pdf/2304.02819.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/02819.03411)) |
| 4.6 | ChaosGPT: Empowering GPT with Internet and Memory to Destroy Humanity ([YouTube](https://www.youtube.com/watch?v=g7YJIpkk7KM&t=912s)) |
| 4.6 | InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning ([:x:](https://arxiv.org/abs/2304.03411)), ([:paperclip:](https://arxiv.org/pdf/2304.03411.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.03411)), ([Project](https://jshi31.github.io/InstantBooth/)) | 
| 4.6 | Wired - [AI Desperately Needs Global Oversight](https://www.wired.com/story/ai-desperately-needs-global-oversight/) |
| 4.6 | Instruction Tuning with GPT-4 ([:x:](https://arxiv.org/abs/2304.03277)), ([:paperclip:](https://arxiv.org/pdf/2304.03277.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.03277)), ([:octocat:](https://instruction-tuning-with-gpt-4.github.io/)) |
| 4.6 | GeNVS: Generative Novel View Synthesis with 3D-Aware Diffusion Models ([:x:](https://arxiv.org/abs/2304.02602)), ([:paperclip:](https://arxiv.org/pdf/2304.02602.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.02602)), ([:octocat:](https://nvlabs.github.io/genvs/)) |
| 4.6 | Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark ([:x:](https://arxiv.org/abs/2304.03279)), ([:paperclip:](https://arxiv.org/pdf/2304.03279.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.03279)) |
| 4.5 | Yoshua Bengio - [Slowing down development of AI systems passing the Turing test](https://yoshuabengio.org/2023/04/05/slowing-down-development-of-ai-systems-passing-the-turing-test/) | 
| 4.5 | Language models are on Replicate - FLAN-T5, GPT-J, and LLaMA ([Blog](https://replicate.com/blog/language-models)) |
| 4.5 | [Meta's  Segment Anything Model (SAM)](https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/?utm_source=twitter&utm_medium=organic_social&utm_campaign=segmentanything&utm_content=gif) ([Paper](https://ai.facebook.com/research/publications/segment-anything/)), ([:paperclip:](https://scontent-ssn1-1.xx.fbcdn.net/v/t39.2365-6/10000000_6331779526880473_6748528980292947838_n.pdf?_nc_cat=102&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=lnYqcLNTtLQAX80UVfV&_nc_ht=scontent-ssn1-1.xx&oh=00_AfAAtzQrBx242Tl4miOfzWrYrJAhLw3VCm1FeWuMs319zw&oe=6432ACEA)), ([:octocat:](https://github.com/facebookresearch/segment-anything)![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/segment-anything?style=social)), ([Demo](https://segment-anything.com/)), ([:x:](https://arxiv.org/abs/2304.02643)), ([:paperclip:](https://arxiv.org/pdf/2304.02643.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.02643)) | 
| 4.4 | Leveraging GPT-4 for Post Hoc Transformation of Free-text Radiology Reports into Structured Reporting: A Multilingual Feasibility Study (RSNA Radiology, [https://doi.org/10.1148/radiol.230725](https://pubs.rsna.org/doi/10.1148/radiol.230725)) |
| 4.4 | Calibrated Chaos: Variance Between Runs of Neural Network Training is Harmless and Inevitable ([:x:](https://arxiv.org/abs/2304.01910)), ([:paperclip:](https://arxiv.org/pdf/2304.01910.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.01910)) |
| 4.4 | One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era ([:x:](https://arxiv.org/abs/2304.06488)), ([:paperclip:](https://arxiv.org/pdf/2304.06488.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.06488)) |
| 4.4 | [LangCahin raised $10 million in seed funding](https://blog.langchain.dev/announcing-our-10m-seed-round-led-by-benchmark/) |
| 4.4 | Kandinsky 2.1 ([:octocat:](https://github.com/ai-forever/Kandinsky-2)![GitHub Repo stars](https://img.shields.io/github/stars/ai-forever/Kandinsky-2?style=social)), ([HuggingFace](https://huggingface.co/ai-forever/Kandinsky_2.1)) |
| 4.4 | The weights of Vicuna-13B released ([WebUI demo](https://chat.lmsys.org/)) ([:octocat:](https://github.com/lm-sys/FastChat/#vicuna-weights)![GitHub Repo stars](https://img.shields.io/github/stars/lm-sys/FastChat?style=social)) |
| 4.4 | LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models ([:x:](https://arxiv.org/abs/2304.01933)), ([:paperclip:](https://arxiv.org/pdf/2304.01933.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.01933)), ([:octocat:](https://github.com/AGI-Edgerunners/LLM-Adapters)![GitHub Repo stars](https://img.shields.io/github/stars/AGI-Edgerunners/LLM-Adapters?style=social)) |
| 4.4 | Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models ([:x:](https://arxiv.org/abs/2304.01852)), ([:paperclip:](https://arxiv.org/pdf/2304.01852.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.01852)) |
| 4.3 | Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling ([:x:](https://arxiv.org/abs/2304.01373)), ([:paperclip:](https://arxiv.org/pdf/2304.01373.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.01373)) |
| 4.3 | Vicuna-13B: An Open-Source ChatGPT Alternative That Impresses GPT-4 ([Blog](https://docs.kanaries.net/articles/vicuna-chatgpt-alternative)), ([:octocat:](https://github.com/lm-sys/FastChat)![GitHub Repo stars](https://img.shields.io/github/stars/lm-sys/FastChat?style=social)) |
| 4.3 | Baby AGI ([:octocat:](https://github.com/yoheinakajima/babyagi)![GitHub Repo stars](https://img.shields.io/github/stars/yoheinakajima/babyagi?style=social)) |
| 4.3 | [Berkley just released Koala-13B!](https://bair.berkeley.edu/blog/2023/04/03/koala/) ([Demo](https://chat.lmsys.org/?model=koala-13b)) |
| 4.3 | [2023 Artificial Intelligence (AI) Index Report](https://aiindex.stanford.edu/report/) Published by Stanford Institute for Human-Centered Artificial Intelligence (HAI) |
| 4.3 | The LLM playground - open source ([:octocat:](https://github.com/nat/openplayground)) |
| 4.3 | Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data ([:x:](https://arxiv.org/abs/2304.01196)), ([:paperclip:](https://arxiv.org/pdf/2304.01196.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.01196)), ([:octocat:](https://github.com/project-baize/baize)![GitHub Repo stars](https://img.shields.io/github/stars/project-baize/baize?style=social)) |
| 4.2 | GPTCache : A Library for Creating Semantic Cache for LLM Queries - ([:octocat:]()) |
| 4.2 | Better Language Models of Code through Self-Improvement ([:x:](https://arxiv.org/abs/2304.01228)), ([:paperclip:](https://arxiv.org/pdf/2304.01228.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.01228)) |
| 4.2 | Eight Things to Know about Large Language Models ([:x:](https://arxiv.org/abs/2304.00612)), ([:paperclip:](https://arxiv.org/pdf/2304.00612.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.00612)) |
| 4.2 | LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models ([:x:](https://arxiv.org/abs/2304.00457)), ([:paperclip:](https://arxiv.org/pdf/2304.00457.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.00457)) |
| 4.1 | Towards General Purpose Vision Systems ([:x:](https://arxiv.org/abs/2304.00743)), ([:book:](https://browse.arxiv.org/pdf/2304.00743.pdf)), ([:paperclip:](https://arxiv.org/pdf/2304.00743.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.00743)), ([:house:](https://huggingface.co/papers/2304.00743)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-general-purpose-vision-systems)), ([:octocat:](https://github.com/mmaaz60/mvits_for_class_agnostic_od)![GitHub Repo stars](https://img.shields.io/github/stars/mmaaz60/mvits_for_class_agnostic_od?style=social))  |
| 4.1 | Evaluating Large Language Models on a Highly-specialized Topic, Radiation Oncology Physics ([:x:](https://arxiv.org/abs/2304.01938)),  ([:paperclip:](https://arxiv.org/pdf/2304.01938.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.01938)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evaluating-large-language-models-on-a-highly))	|
| 4.1 | [Italy curbs ChatGPT, starts probe over privacy concerns](https://www.cnbc.com/2023/04/01/italy-curbs-chatgpt-starts-probe-over-privacy-concerns.html) |
| 3.31 | Evaluating GPT-4 and ChatGPT on Japanese Medical Licensing Examinations ([:x:](https://arxiv.org/abs/2303.18027)), ([:paperclip:](https://arxiv.org/pdf/2303.18027.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.18027)), ([:house:](https://huggingface.co/papers/2303.18027)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evaluating-gpt-4-and-chatgpt-on-japanese)), ([:octocat:](https://github.com/jungokasai/igakuqa)![GitHub Repo stars](https://img.shields.io/github/stars/jungokasai/igakuqa?style=social))  |
| 3.31 | Choose Your Weapon: Survival Strategies for Depressed AI Academics ([:x:](https://arxiv.org/abs/2304.06035)), ([:paperclip:](https://arxiv.org/pdf/2304.06035.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.06035)) |
| 3.31 | CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society ([:x:](https://arxiv.org/abs/2303.17760)), ([:paperclip:](https://arxiv.org/pdf/2303.17760.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.17760)), ([:octocat:](https://github.com/lightaime/camel))![GitHub Repo stars](https://img.shields.io/github/stars/lightaime/camel?style=social) |
| 3.31 | ⭐ A Survey of Large Language Models - Version 1 ([:x:](https://arxiv.org/abs/2303.18223v1)), ([:paperclip:](https://arxiv.org/pdf/2303.18223v1.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.18223v1)) |
| 3.31 | (SCIENTIFIC AMERICAN) [AI Chatbots Can Diagnose Medical Conditions at Home. How Good Are They?](https://www.scientificamerican.com/article/ai-chatbots-can-diagnose-medical-conditions-at-home-how-good-are-they/) |
| 3.30 | ChatGPT in Healthcare: A Taxonomy and Systematic Review ([medRxiv](https://www.medrxiv.org/content/10.1101/2023.03.30.23287899v1.full)), ([:paperclip:](https://www.medrxiv.org/content/10.1101/2023.03.30.23287899v1.full.pdf)) |
| 3.30 | Launching the Generative AI Open Source (GenOS) Index - ([Index](https://www.decibel.vc/articles/launching-the-generative-ai-open-source-genos-index)), ([Tweet](https://twitter.com/chakrabartis/status/1641447121042964482)) |
| 3.30 | Whose Opinions Do Language Models Reflect? ([:x:](https://arxiv.org/abs/2303.17548)), ([:paperclip:](https://arxiv.org/pdf/2303.17548.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.17548)), ([:octocat:](https://github.com/tatsu-lab/opinions_qa)![GitHub Repo stars](https://img.shields.io/github/stars/tatsu-lab/opinions_qa?style=social)) |
| 3.30 | Language Models can Solve Computer Tasks ([:x:](https://arxiv.org/abs/2303.17491)), ([:paperclip:](https://arxiv.org/pdf/2303.17491.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.17491)) |
| 3.30 | Self-Refine: Iterative Refinement with Self-Feedback ([:x:](https://arxiv.org/abs/2303.17651)), ([:paperclip:](https://arxiv.org/pdf/2303.17651.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.17651)) |
| 3.30 | Humans in Humans Out: On GPT Converging Toward Common Sense in both Success and Failure ([:x:](https://arxiv.org/abs/2303.17276)), ([:paperclip:](https://arxiv.org/pdf/2303.17276.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.17276)) |
| 3.30 | [List of Open Sourced Fine-Tuned Large Language Models (LLM)](https://medium.com/geekculture/list-of-open-sourced-fine-tuned-large-language-models-llm-8d95a2e0dc76) |
| 3.30 | [NEJM - Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine](https://www.nejm.org/doi/pdf/10.1056/NEJMsr2214184) |
| 3.30 | BloombergGPT: A Large Language Model for Finance ([:x:](https://arxiv.org/abs/2303.17564)), ([:paperclip:](https://arxiv.org/pdf/2303.17564.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.17564)) |
| 3.30 | [Got It AI’s ELMAR challenges GPT-4 and LLaMa, scores well on hallucination benchmarks](https://venturebeat.com/ai/got-it-ai-elmar-challenges-gpt-4-and-llama) |
| 3.30 | HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace ([:x:](https://arxiv.org/abs/2303.17580)), ([:paperclip:](https://arxiv.org/pdf/2303.17580.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.17580)) |
| 3.30 | [CAIDP claims "The FTC should investigate OpenAI and block GPT over ‘deceptive’ behavior"](https://edition.cnn.com/2023/03/30/tech/ftc-openai-gpt-ai-think-tank/index.html) |
| 3.30 | [Epic to use Microsoft's GPT-4 in EHRs](https://www.beckershospitalreview.com/ehrs/epic-to-use-microsofts-open-ai-in-ehrs.html) |
| 3.30 | Auto-GPT: An Autonomous GPT-4 Experiment ([:octocat:](https://github.com/Torantulino/Auto-GPT)![GitHub Repo stars](https://img.shields.io/github/stars/Torantulino/Auto-GPT?style=social)) |
| 3.29 | HyperDiffusion: Generating Implicit Neural Fields with Weight-Space Diffusion ([project](https://llm-attacks.org/)), ([:x:](https://arxiv.org/abs/2303.17015)), ([:paperclip:](https://arxiv.org/pdf/2303.17015.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.17015)), ([:house:](https://huggingface.co/papers/2303.17015)), ([:eight_spoked_asterisk:]([https://paperswithcode.com/paper/universal-and-transferable-adversarial](https://paperswithcode.com/paper/hyperdiffusion-generating-implicit-neural)) |
| 3.29 | AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators ([:x:](https://arxiv.org/abs/2303.16854)), ([:paperclip:](https://arxiv.org/pdf/2303.16854.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.16854)) |
| 3.29 | [nucleotide transformers - genomics LLM, ranging from 500M to 2.5B parameters](https://twitter.com/instadeepai/status/1641075963051012097) - ([:octocat:](https://github.com/instadeepai/nucleotide-transformer)![GitHub Repo stars](https://img.shields.io/github/stars/nucleotide-transformer?style=social)) |
| 3.29 | [GeoV-9b - 9 billion parameter causal language model](https://twitter.com/labmlai/status/1641357802009395201) ([code](https://github.com/geov-ai/geov), [weights](https://huggingface.co/GeoV/GeoV-9b), [colab](https://colab.research.google.com/github/geov-ai/geov/blob/master/notebooks/generate.ipynb)) |
|	3.29	|	[GPT4All - 7B param language model finetuned from a curated set of 400k GPT-Turbo-3.5](https://twitter.com/andriy_mulyar/status/1640836003194630144)  	|
|	3.29	|	[LLaMA-Adapter!: Efficient Fine-tuning of Language Models with Zero-init Attention](https://twitter.com/lupantech/status/1640899600281395200)	|
|	3.29	|	[MacGPT 3.2](https://www.macgpt.com/)	|
| 3.29 | G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment ([:x:](https://arxiv.org/abs/2303.16634)), ([:paperclip:](https://arxiv.org/pdf/2303.16634.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.16634)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gpteval-nlg-evaluation-using-gpt-4-with)), ([:octocat:](https://github.com/nlpyang/geval)![GitHub Repo stars](https://img.shields.io/github/stars/nlpyang/geval?style=social))  |
| 3.29 | TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs  ([:x:](https://arxiv.org/abs/2303.16434)), ([:paperclip:](https://arxiv.org/pdf/2303.16434.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.16434)) |
| 3.28 | Training Language Models with Language Feedback at Scale ([:x:](https://arxiv.org/abs/2303.16755)), ([:book:](https://browse.arxiv.org/pdf/2303.16755.pdf)), ([:paperclip:](https://arxiv.org/pdf/2303.16755.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.16755)), ([:house:](https://huggingface.co/papers/2303.16755)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/training-language-models-with-language)), ([:octocat:](https://github.com/jeremyalain/imitation_learning_from_language_feedback)![GitHub Repo stars](https://img.shields.io/github/stars/jeremyalain/imitation_learning_from_language_feedback?style=social))  |
| 3.28 | Natural Selection Favors AIs over Humans [:x:](https://arxiv.org/abs/2303.16200)), ([:paperclip:](https://arxiv.org/pdf/2303.16200.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.16200)) |
| 3.28 | ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks ([:x:](https://arxiv.org/abs/2303.15056)), ([:paperclip:](https://arxiv.org/pdf/2303.15056.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.15056)) |
|	3.28	|	[LLaMA voice chat + Siri TTS](https://twitter.com/ggerganov/status/1640416314773700608)	|
|	3.28	|	[Cerebras-GPT - 111M to 13B parameters trained using the Chinchilla formula](https://twitter.com/CerebrasSystems/status/1640725880711569408)	|
|	3.28	|	[Microsoft Security Copilot: Empowering defenders at the speed of AI](https://blogs.microsoft.com/blog/2023/03/28/introducing-microsoft-security-copilot-empowering-defenders-at-the-speed-of-ai/)	|
| 3.28 | [Google pix2struct launched today, a multimodal model specializing in screenshot data](https://twitter.com/danielgross/status/1640515851014004737) |
|	3.28	|	[OpenFlamingo - a framework that enables training and evaluation of large multimodal models (LMMs)](https://laion.ai/blog/open-flamingo/)	|
| 3.27 | Microsoft JARVIS ([:octocat:](https://github.com/microsoft/JARVIS)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/JARVIS?style=social)) |
| 3.27 | [ChatGPT Survey: Performance on NLP datasets](http://opensamizdat.com/posts/chatgpt_survey/) |
| 3.27 | GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models ([:x:](https://arxiv.org/abs/2303.10130)), ([:paperclip:](https://arxiv.org/pdf/2303.10130.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.10130)) |
| 3.26 | AI-Generated Content (AIGC): A Survey ([:x:](https://arxiv.org/abs/2304.06632)), ([:paperclip:](https://arxiv.org/pdf/2304.06632.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.06632)), ([:house:](https://huggingface.co/papers/2304.06632)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ai-generated-content-aigc-a-survey) |
| 3.26 | Nature Language Reasoning, A Survey ([:x:](https://arxiv.org/abs/2303.14725)), ([:paperclip:](https://arxiv.org/pdf/2303.14725.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.14725)) |
|	3.26	|	[Sam Altman: OpenAI CEO on GPT-4, ChatGPT, and the Future of AI - Lex Fridman Podcast #367](https://www.youtube.com/watch?v=L_Guz73e6fw)	|
|	3.26	|	[LLaMA voice chat](https://twitter.com/ggerganov/status/1640022482307502085)	|
|	3.26	|	[Japanese Alpaca LoRA](https://twitter.com/kun1em0n/status/1639965140429963264)	|
| 3.24 | LLM for Patient-Trial Matching: Privacy-Aware Data Augmentation Towards Better Performance and Generalizability ([:x:](https://arxiv.org/abs/2303.16756)),  ([:paperclip:](https://arxiv.org/pdf/2303.16756.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.16756)), ([:house:](https://huggingface.co/papers/2303.16756)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llm-for-patient-trial-matching-privacy-aware)) |
| 3.24 | Progressively Optimized Local Radiance Fields for Robust View Synthesis ([:x:](https://arxiv.org/abs/2303.13791)),  ([:paperclip:](https://arxiv.org/pdf/2303.13791.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.13791)), ([:house:](https://huggingface.co/papers/2303.13791)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/progressively-optimized-local-radiance-fields)), (CVPR 2023)	|
| 3.24 | Efficient Methods for Natural Language Processing: A Survey ([:x:](https://arxiv.org/abs/2209.00099)),  ([:paperclip:](https://arxiv.org/pdf/2209.00099.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2209.00099)) |
|	3.24	|	[NYT OPINION - You Can Have the Blue Pill or the Red Pill, and We’re Out of Blue Pills](https://www.nytimes.com/2023/03/24/opinion/yuval-harari-ai-chatgpt.html)	([archive](https://archive.is/AUKPm)) |
|	3.24	|	[Dolly - open source LLM](https://twitter.com/databricks/status/1639239800145465344) 	|
|	3.24	|	[Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators](https://twitter.com/_akhaliq/status/1639062868850266112)	|
|	3.24	|	ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge ([:x:](https://arxiv.org/abs/2303.14070v1)),  ([:paperclip:](https://arxiv.org/pdf/2303.14070v1.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.14070v1)), ([:octocat:](https://github.com/Kent0n-Li/ChatDoctor)![GitHub Repo stars](https://img.shields.io/github/stars/Kent0n-Li/ChatDoctor?style=social))	|
| 3.24 | Do large language models need sensory grounding for meaning and understanding? @YannLeCun |
|	3.23	|	[OpenAI: ChatGPT Plugins](https://openai.com/blog/chatgpt-plugins)	|
|	3.23	|	[Opera brings AI ChatGPT bot sidebar to browsers](https://www.deccanherald.com/business/technology/opera-brings-ai-chatgpt-bot-sidebar-to-browsers-1202781.html)	|
| 3.22 | The Shaky Foundations of Clinical Foundation Models: A Survey of Large Language Models and Foundation Models for EMRs ([:x:](https://arxiv.org/abs/2303.12961)), ([:paperclip:](https://arxiv.org/pdf/2303.12961.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.12961)), ([:house:](https://huggingface.co/papers/2303.12961)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-shaky-foundations-of-clinical-foundation)), ([:octocat:](https://github.com/som-shahlab/ehrshot-benchmark)![GitHub Repo stars](https://img.shields.io/github/stars/som-shahlab/ehrshot-benchmark?style=social)), ([SS](https://www.semanticscholar.org/paper/The-Shaky-Foundations-of-Clinical-Foundation-A-of-Wornow-Xu/f8e7e8f5d00ffea4535c4bb548f572a21122ff78) |
| 3.22 | Artificial muses: Generative Artificial Intelligence Chatbots Have Risen to Human-Level Creativity ([:x:](https://arxiv.org/abs/2303.12003)), ([:paperclip:](https://arxiv.org/pdf/2303.12003.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.12003)), ([:house:](https://huggingface.co/papers/2303.12003)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sparks-of-artificial-general-intelligence)) |	
|	3.22	|	[GitHub: Copilot X](https://github.blog/2023-03-22-github-copilot-x-the-ai-powered-developer-experience)	|
|	3.22	|	Sparks of Artificial General Intelligence: Early experiments with GPT-4 ([:x:](https://arxiv.org/abs/2303.12712)), ([:paperclip:](https://arxiv.org/pdf/2303.12712.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.12712)), ([YouTube](https://www.youtube.com/watch?v=qbIk7-JPB2c)) |
|	3.22	|	[Pause Giant AI Experiments: An Open Letter](https://futureoflife.org/open-letter/pause-giant-ai-experiments/)	|
| 3.21 | WSJ - [Generative AI Makes Headway in Healthcare](https://www.wsj.com/articles/generative-ai-makes-headway-in-healthcare-cb5d4ee2) |
|	3.21	|	[NVIDIA Brings Generative AI to World’s Enterprises](https://nvidianews.nvidia.com/news/nvidia-brings-generative-ai-to-worlds-enterprises-with-cloud-services-for-creating-large-language-and-visual-models)	|
|	3.21	|	[Adobe launches Firefly](https://www.cnbc.com/2023/03/21/adobe-firefly-generative-ai-lets-you-type-to-edit-images.html)	|
|	3.21	|	[Google launches Bard in the US and UK](https://blog.google/technology/ai/try-bard)	|
|	3.21	|	[Microsoft: Bing Image Creator](https://blogs.microsoft.com/blog/2023/03/21/create-images-with-your-words-bing-image-creator-comes-to-the-new-bing)	|
|	3.21	|	[Stability AI Launches Stable Diffusion Reimagine](https://stability.ai/blog/stable-diffusion-reimagine)	|
| 3.20 | Capabilities of GPT-4 on Medical Challenge Problems ([:x:](https://arxiv.org/abs/2303.13375)), ([:book:](https://browse.arxiv.org/pdf/2303.13375.pdf)), ([:paperclip:](https://arxiv.org/pdf/2303.13375.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.13375)), ([:house:](https://huggingface.co/papers/2303.13375)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/capabilities-of-gpt-4-on-medical-challenge)) |
| 3.20 | Reflexion: an autonomous agent with dynamic memory and self-reflection ([:x:](https://arxiv.org/abs/2303.11366)), ([:paperclip:](https://arxiv.org/pdf/2303.11366.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.11366)), ([:octocat:](https://github.com/noahshinn024/reflexion)![GitHub Repo stars](https://img.shields.io/github/stars/noahshinn024/reflexion?style=social)) |
|	3.20	|	[March 20 ChatGPT outage: Here’s what happened](https://openai.com/blog/march-20-chatgpt-outage)	|
|	3.20	|	[Runway Gen-2](https://research.runwayml.com/gen2)	|
|	3.20	|	Capabilities of GPT-4 on Medical Challenge Problems ([:x:](https://arxiv.org/abs/2303.13375)), ([:paperclip:](https://arxiv.org/pdf/2303.13375.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.13375)), ([:house:](https://huggingface.co/papers/2303.13375)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/capabilities-of-gpt-4-on-medical-challenge)) |
| 3.20 | [Making Music with GPT 4](https://www.youtube.com/watch?v=Cvl30rn03Hg) by [(Wavtool)](https://wavtool.com/) |
| 3.19 | Simple LLM Finetuner ([:octocat:](https://github.com/lxe/simple-llm-finetuner)![GitHub Repo stars](https://img.shields.io/github/stars/lxe/simple-llm-finetuner?style=social)) |
| 3.18 | Data-centric Artificial Intelligence: A Survey ([:x:](https://arxiv.org/abs/2303.10158)), ([:paperclip:](https://arxiv.org/pdf/2303.10158.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.10158)), ([:octocat:](https://github.com/daochenzha/data-centric-AI)![GitHub Repo stars](https://img.shields.io/github/stars/data-centric-AI?style=social)) |
| 3.17 | Can AI-Generated Text be Reliably Detected? ([:x:](https://arxiv.org/abs/2303.11156)), ([:paperclip:](https://arxiv.org/pdf/2303.11156.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.11156)) |
| 3.17 | GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models  ([:x:](https://arxiv.org/abs/2303.10130)), ([:paperclip:](https://arxiv.org/pdf/2303.10130.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.10130)),  ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gpts-are-gpts-an-early-look-at-the-labor)), ([SS](https://www.semanticscholar.org/paper/GPTs-are-GPTs%3A-An-Early-Look-at-the-Labor-Market-of-Eloundou-Manning/5501d00310b06e00351295529498cc684187148d)) |
| 3.16 | WebSHAP: Towards Explaining Any Machine Learning Models Anywhere ([:x:](https://arxiv.org/abs/2303.09545)), ([:paperclip:](https://arxiv.org/pdf/2303.09545.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.09545)), ([:octocat:](https://poloclub.github.io/webshap/)) |
| 3.16 | LERF: Language Embedded Radiance Fields ([:x:](https://arxiv.org/abs/2303.09553)), ([:paperclip:](https://arxiv.org/pdf/2303.09553.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.09553)), ([:octocat:](https://www.lerf.io/)) |
|	3.16	|	[Microsoft: Microsoft 365 Copilot](https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work)	|
|	3.16	|	[Alpaca LoRA: instruct tune LLAMA on consumer hardware](https://twitter.com/_akhaliq/status/1636416647518097408)	|
|	3.16	|	[OpenAI CEO Sam Altman says AI will reshape society, acknowledges risks: 'A little bit scared of this'](https://abcnews.go.com/Technology/openai-ceo-sam-altman-ai-reshape-society-acknowledges/story?id=97897122)	|
|	3.15	|	[A new era for AI and Google Workspace](https://workspace.google.com/blog/product-announcements/generative-ai?hl=en)	|
|	3.15	|	[PyTorch 2.0: Our next generation release](https://pytorch.org/blog/pytorch-2.0-release/)	|
|	3.15	|	[Baidu: ERNIE Bot](https://www.youtube.com/watch?v=ukvEUI3x0vI)	|
|	3.15	|	[Midjourney: Midjourney V5](https://twitter.com/midjourney/status/1636130389365497857)	|
|	3.15	|	[arXiv - GPT-4 Technical report](https://arxiv.org/abs/2303.08774)	|
| 3.14 | Text-to-image Diffusion Models in Generative AI: A Survey ([:x:](https://arxiv.org/abs/2303.07909)), ([:paperclip:](https://arxiv.org/pdf/2303.07909.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.07909)), ([:house:](https://huggingface.co/papers/2303.07909)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/text-to-image-diffusion-model-in-generative)) |
| 3.14 | The Lancet - [Attention is not all you need: the complicated case of ethically using large language models in healthcare and medicine](https://www.thelancet.com/journals/ebiom/article/PIIS2352-3964(23)00077-4/fulltext) |
|	3.14	|	THUDM releases ChatGLM-6B	|
| 3.14 | Langflow - a UI for LangChain ([:octocat:](https://github.com/logspace-ai/langflow)![GitHub Repo stars](https://img.shields.io/github/stars/logspace-ai/langflow?style=social)) |
|	3.14	|	[Anthropic: Claude](https://www.anthropic.com/index/introducing-claude)	|
|	3.14	|	[Google: PaLM API & Workspace](https://blog.google/technology/ai/ai-developers-google-cloud-workspace)	|
|	3.14	|	[OpenAI: GPT-4](https://openai.com/research/gpt-4)	|
|	3.13	|	[Stanford Alpaca 7B](https://crfm.stanford.edu/2023/03/13/alpaca.html)	|
|	3.13	|	[Microsoft lays off team that taught employees how to make AI tools responsibly](https://www.theverge.com/2023/3/13/23638823/microsoft-ethics-society-team-responsible-ai-layoffs)	|
|	3.13	|	[MiniLLM: Large Language Models on Consumer GPUs](https://github.com/kuleshov/minillm)	|
| 3.13 | Chatbot UI ([:octocat:](https://github.com/mckaywrigley/chatbot-ui)(https://img.shields.io/github/stars/mckaywrigley/chatbot-ui?style=social)) |
| 3.12 | Towards General Purpose Medical AI: Continual Learning Medical Foundation Model ([:x:](https://arxiv.org/abs/2303.06580)), ([:paperclip:](https://arxiv.org/pdf/2303.06580.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.06580)), ([:house:](https://huggingface.co/papers/2303.06580)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-general-purpose-medical-ai-continual)) |
|	3.12	|	[GM explores using ChatGPT in vehicles](https://europe.autonews.com/automakers/gm-explores-using-chatgpt-vehicles)	|
|	3.10	|	[Google: PaLM-E](https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html)	|
|	3.9	|	[multi-model playground - https://nat.dev](https://nat.dev/)	|
|	3.9	|	[GPT-4 is coming next week](https://www.heise.de/news/GPT-4-is-coming-next-week-and-it-will-be-multimodal-says-Microsoft-Germany-7540972.html)	|
| 3.8 | Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models ([:x:](https://arxiv.org/abs/2303.04671)), ([:paperclip:](https://arxiv.org/pdf/2303.04671.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.04671)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/visual-chatgpt-talking-drawing-and-editing)), ([SS](https://www.semanticscholar.org/paper/Visual-ChatGPT%3A-Talking%2C-Drawing-and-Editing-with-Wu-Yin/af997821231898a5f8d0fd78dad4eec526acabe5)), ([:octocat:](https://github.com/microsoft/visual-chatgpt)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/visual-chatgpt?style=social))  |
|	3.8	|	[NYT, Opinion - Noam Chomsky: The False Promise of ChatGPT](https://www.nytimes.com/2023/03/08/opinion/noam-chomsky-chatgpt-ai.html)	([archive](https://archive.is/qvR3Q))|
| 3.7 | A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT ([:x:](https://arxiv.org/abs/2303.04226)), ([:paperclip:](https://arxiv.org/pdf/2303.04226.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.04226)) |
| 3.7 | Radiology - [The Role and Limitations of Large Language Models Such as ChatGPT in Clinical Settings and Medical Journalism](https://pubs.rsna.org/doi/10.1148/radiol.230276) |
|	3.7	|	[Stability AI Acquires Image Editing App Clipdrop](https://stability.ai/blog/stability-ai-acquires-init-ml-makers-of-clipdrop-application)	|
|	3.6	|	[Google: Universal Speech Model](https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html)	|
|	3.5	|	[Generative AI: Perspectives from Stanford HAI](https://hai.stanford.edu/generative-ai-perspectives-stanford-hai)	|
|	3.5	|	[UpStage, ChatGPT bot (Askup) on Line](https://github.com/hunkim/line-gpt)	|
|	3.5	|	[UpStage, ChatGPT bot (Askup) on KakaoTalk](https://github.com/hunkim/kakao-gpt)	|
| 3.2 | Consistency Models  ([:x:](https://arxiv.org/abs/2303.01469)), ([:paperclip:](https://arxiv.org/pdf/2303.01469.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.01469)), ([:octocat:](https://github.com/openai/consistency_models)![GitHub Repo stars](https://img.shields.io/github/stars/openai/consistency_models?style=social)) |
| 3.1 | Almanac: Retrieval-Augmented Language Models for Clinical Medicine ([:x:](https://arxiv.org/abs/2303.01229)), ([:paperclip:](https://arxiv.org/pdf/2303.01229.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.01229)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/almanac-knowledge-grounded-language-models)) |
|	3.1	|	[OpenAI: ChatGPT and Whisper API](https://openai.com/blog/introducing-chatgpt-and-whisper-apis)	|
| 2.28 | Large Language Models Are State-of-the-Art Evaluators of Translation Quality ([:x:](https://arxiv.org/abs/2302.14520)), ([:paperclip:](https://arxiv.org/pdf/2302.14520.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2302.14520)) |
| 2.27 | Best Practices for Using AI When Writing Scientific Manuscripts ([ACS Nano 2023, 17, 5, 4091–4093](https://doi.org/10.1021/acsnano.3c01544)) |
|	2.27	|	[Fighting ‘Woke AI,’ Musk Recruits Team to Develop OpenAI Rival](https://www.theinformation.com/articles/fighting-woke-ai-musk-recruits-team-to-develop-openai-rival)	|
| 2.25 | The Lancet - [The promise of large language models in health care](https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(23)00216-7/fulltext) |
| 2.25 | AugGPT: Leveraging ChatGPT for Text Data Augmentation ([:x:](https://arxiv.org/abs/2302.13007)), ([:paperclip:](https://arxiv.org/pdf/2302.13007.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2302.13007)) |
|	2.24	|	[Sam Altman, Planning for AGI and beyond](https://openai.com/blog/planning-for-agi-and-beyond)	|
|	2.24	|	[Meta: LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai)	|
| 2.23 | Radiology - [ChatGPT and the Future of Medical Writing](https://pubs.rsna.org/doi/10.1148/radiol.223312) |
|	2.23	|	[Instagram co-founders launch AI-powered news app Artifact on Android, iOS](https://www.thehindubusinessline.com/info-tech/social-media/instagram-co-founders-launch-ai-powered-news-app-artifact-on-android-ios/article66543779.ece)	|
|	2.23	|	[Notion.AI launch](http://notion.ai/)	|
| 2.22 | The alignment problem from a deep learning perspective ([:x:](https://arxiv.org/abs/2209.00626)), ([:paperclip:](https://arxiv.org/pdf/2209.00626.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2209.00626))	|
|	2.22	|	[Microsoft: Bing announcement on mobile and Skype](https://blogs.microsoft.com/blog/2023/02/22/the-new-bing-preview-experience-arrives-on-bing-and-edge-mobile-apps-introducing-bing-now-in-skype)	|
|	2.22	|	Science - [As scientists explore AI-written text, journals hammer out policies](https://www.science.org/content/article/scientists-explore-ai-written-text-journals-hammer-policies)	|
| 2.21 | BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT ([:x:](https://arxiv.org/abs/2304.12298)), ([:paperclip:](https://arxiv.org/pdf/2304.12298.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2304.12298))	|
| 2.21 | Hyena Hierarchy: Towards Larger Convolutional Language Models ([:x:](https://arxiv.org/abs/2302.10866)), ([:paperclip:](https://arxiv.org/pdf/2302.10866.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2302.10866))	|
|	2.21	|	[The PNAS Journals Outline Their Policies for ChatGPT and Generative AI](https://www.pnas.org/post/update/pnas-policy-for-chatgpt-generative-ai)	|
|	2.21	|	ChatGPT: Jack of all trades, master of none ([:x:](https://arxiv.org/abs/2302.10724)), ([:paperclip:](https://arxiv.org/pdf/2302.10724.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2302.10724))	|
| 2.20 | ChatGPT for Robotics: Design Principles and Model Abilities ([:x:](https://arxiv.org/abs/2306.17582)), ([:paperclip:](https://arxiv.org/pdf/2306.17582.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2306.17582)),  ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-survey-on-generative-diffusion-model)),  ([:octocat:](https://github.com/chq1155/A-Survey-on-Generative-Diffusion-Model)![GitHub Repo stars](https://img.shields.io/github/stars/chq1155/A-Survey-on-Generative-Diffusion-Model?style=social)) |
| 2.18 | A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT ([:x:](https://arxiv.org/abs/2302.09419)), ([:paperclip:](https://arxiv.org/pdf/2302.09419.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2302.09419)), ([:house:](https://huggingface.co/papers/2302.09419)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-comprehensive-survey-on-pretrained)), ([SS](https://www.semanticscholar.org/paper/A-Comprehensive-Survey-on-Pretrained-Foundation-A-Zhou-Li/3599a236f285af48782fc30b1341d13ec7320735)) |
| 2.17 | Complex QA and language models hybrid architectures, Survey ([:x:](https://arxiv.org/abs/2302.09051)), ([:paperclip:](https://arxiv.org/pdf/2302.09051.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2302.09051)), ([:house:](https://huggingface.co/papers/2302.09051)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/complex-qa-and-language-models-hybrid)), ([SS](https://www.semanticscholar.org/paper/Complex-QA-and-language-models-hybrid-Survey-Daull-Bellot/681cee58cf7e54199191cf9e0baf6851d8356704)) |
|	2.17	|	[Time, ChatGPT cover](https://time.com/6255952/ai-impact-chatgpt-microsoft-google/)	|
|	2.17	|	OpenAI, Foundry Product Brief	|
|	2.17	|	[Generative AI on Roblox: Our Vision for the Future of Creation](https://blog.roblox.com/2023/02/generative-ai-roblox-vision-future-creation/)	|
| 2.16 | Auditing large language models: a three-layered approach  ([:x:](https://arxiv.org/abs/2302.08500)), ([:paperclip:](https://arxiv.org/pdf/2302.08500.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2302.08500)), ([:house:](https://huggingface.co/papers/2302.08500)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/auditing-large-language-models-a-three)), ([SS](https://www.semanticscholar.org/paper/Auditing-large-language-models%3A-a-three-layered-Mokander-Schuett/22e2f488ecd88bd2adf79092d0d390d8f7b06a0f)) |
| 2.16 | Do We Still Need Clinical Language Models? ([:x:](https://arxiv.org/abs/2302.08091)), ([:paperclip:](https://arxiv.org/pdf/2302.08091.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2302.08091)) |
|	2.16	|	[Startup Replit launches a ChatGPT-like bot for coders](https://www.semafor.com/article/02/15/2023/startup-replit-launches-a-chatgpt-like-bot-for-coders)	|
|	2.15	|	[A&O announces exclusive launch partnership with Harvey](https://www.allenovery.com/en-gb/global/news-and-insights/news/ao-announces-exclusive-launch-partnership-with-harvey)	|
| 2.14 | ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models ([:x:](https://arxiv.org/abs/2302.07257)), ([:paperclip:](https://arxiv.org/pdf/2302.07257.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2302.07257)), ([:house:](https://huggingface.co/papers/2302.07257)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chatcad-interactive-computer-aided-diagnosis)), ([:octocat:](https://github.com/zhaozh10/ChatCAD)![GitHub Repo stars](https://img.shields.io/github/stars/zhaozh10/ChatCAD?style=social)) |
| 2.14 | What Is ChatGPT Doing … and Why Does It Work? ([Stephen Wolfram Writings](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)) |
|	2.14	|	1M ChatGPT plus user 	|
|	2.14	|	[The Gen AI Conference Hosted by Jasper](https://www.joingen.ai/)	|
|	2.13	|	[Google: Vision Transformer 22B](https://twitter.com/m__dehghani/status/1625186144001396737)	|
| 2.12 | Transformer models: an introduction and catalog ([:x:](https://arxiv.org/abs/2302.07730)), ([:paperclip:](https://arxiv.org/pdf/2302.07730.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2302.07730)), ([Blog](https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/)) |
|	2.10	|	[arXivGPT launches](https://news.ycombinator.com/item?id=34770108)	|
|	2.10	|	[OpenAI, ChatGPT plus announce (20$)](https://openai.com/blog/chatgpt-plus)	|
|	2.9	|	[Disastrous Chatbot Demo Costs Google $140 Billion](https://www.channelnews.com.au/google-shares-tank-after-disastrous-chatbot-demo/)	|
|	2.9	|	[Meta: Toolformer](https://arxiv.org/abs/2302.04761)	|
| 2.8 | A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity ([:x:](https://arxiv.org/abs/2302.04023)), ([:paperclip:](https://arxiv.org/pdf/2302.04023.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2302.04023)) | 
|	2.8	|	[Runway launches ground-breaking Gen-1 video generation AI system](https://www.ghacks.net/2023/02/08/runway-launches-ground-breaking-gen-1-video-generation-ai-system/)	|
|	2.7	|	[Microsoft: Bing ChatGPT](https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/)	|
|	2.7	|	[Getty Images sues AI art generator Stable Diffusion in the US for copyright infringement](https://www.theverge.com/2023/2/6/23587393/ai-art-copyright-lawsuit-getty-images-stable-diffusion)	|
| 2.6 | A Categorical Archive of ChatGPT Failures ([:x:](https://arxiv.org/abs/2302.03494)), ([:paperclip:](https://arxiv.org/pdf/2302.03494.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2302.03494)), ([:house:](https://huggingface.co/papers/2302.03494)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-categorical-archive-of-chatgpt-failures)), ([:octocat:](https://github.com/aliborji/chatgpt_failures)![GitHub Repo stars](https://img.shields.io/github/stars/aliborji/chatgpt_failures?style=social))  |
| 2.6 | The Lancet - [ChatGPT: friend or foe?](https://www.thelancet.com/journals/landig/article/PIIS2589-7500%2823%2900023-7/fulltext) |
|	2.6	|	[Google: Bard announcement](https://blog.google/technology/ai/bard-google-ai-search-updates)	|
| 2.4 | Theory of Mind May Have Spontaneously Emerged in Large Language Models  ([:x:](https://arxiv.org/abs/2302.02083)), ([:paperclip:](https://arxiv.org/pdf/2302.02083.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2302.02083)) |
|	2.4	|	[POE.com open](http://poe.com/)	|
|	2.3	|	[Google invests in Anthropic, maker of ChatGPT rival](https://fortune.com/2023/02/04/google-invests-300m-anthropic-openai-rival-making-chatgpt-challenger-claude-ai-chatbot-battle/) 	|
|	2.3	|	Naver, SearchGPT announcement	|
| 2.2 | Creating a Large Language Model of a Philosopher ([:x:](https://arxiv.org/abs/2302.01339)), ([:paperclip:](https://arxiv.org/pdf/2302.01339.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2302.01339)) |
|	2.2	|	[ChatGPT reaches 100 million users two months after launch](https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app)	|
| 2.1 | The Diagnostic and Triage Accuracy of the GPT-3 Artificial Intelligence Model ([medrXiv](https://www.medrxiv.org/content/10.1101/2023.01.30.23285067v1 )|
|	2.1	|	[OpenAI, released a software tool to help identify text generated by AI](https://eandt.theiet.org/content/articles/2023/02/chatgpt-owner-launches-imperfect-tool-to-detect-ai-generated-text/)	|
| 1.31 | The Flan Collection: Designing Data and Methods for Effective Instruction Tuning ([blog](https://ai.googleblog.com/2023/02/the-flan-collection-advancing-open.html)), ([:x:](https://arxiv.org/abs/2301.13688)), ([:paperclip:](https://arxiv.org/pdf/2301.13688.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2301.13688)), ([:house:](https://huggingface.co/papers/2301.13688)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-flan-collection-designing-data-and)), ([:octocat:](https://github.com/google-research/flan)![GitHub Repo stars](https://img.shields.io/github/stars/google-research/flan?style=social))  |
|	1.31	|	[JAMA Network - Nonhuman “Authors” and Implications for the Integrity of Scientific Publication and Medical Knowledge](https://jamanetwork.com/journals/jama/fullarticle/2801170)	|
| 1.30 | SingSong: Generating musical accompaniments from singing ([:x:](https://arxiv.org/abs/2301.12662)), ([:paperclip:](https://arxiv.org/pdf/2301.12662.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2301.12662)), ([:octocat:](https://storage.googleapis.com/sing-song/index.html)) |
|	1.30	|	[China's biggest search engine is to set launch a ChatGPT rival in March](https://www.engadget.com/chinas-baidu-is-adding-a-chatgpt-type-bot-to-its-search-engine-110452638.html)	|
|	1.26	|	[Science Journal - ChatGPT is fun, but not an author](https://www.science.org/doi/10.1126/science.adg7879)	|
|	1.26	|	DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature ([:x:](https://arxiv.org/abs/2301.11305)), ([:paperclip:](https://arxiv.org/pdf/2301.11305.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2301.11305))	|
|	1.26	|	[ChatGPT Is Coming for Classrooms. Don't Panic](https://www.wired.com/story/chatgpt-is-coming-for-classrooms-dont-panic/)	|
|	1.26	|	[ChatGPT passes exams from law and business schools](https://edition.cnn.com/2023/01/26/tech/chatgpt-passes-exams/index.html)	|
|	1.26	|	[Google’s new AI turns text into music - MusicLM](https://google-research.github.io/seanet/musiclm/examples/)	|
| 1.24 | Putting ChatGPT's Medical Advice to the (Turing) Test ([:x:](https://arxiv.org/abs/2301.10035)), ([:paperclip:](https://arxiv.org/pdf/2301.10035.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2301.10035)) |
|	1.24	|	[Nature policy - Tools such as ChatGPT threaten transparent science; here are our ground rules for their use](https://www.nature.com/articles/d41586-023-00191-1)	|
|	1.20	|	[WAME policy - Chatbots, ChatGPT, and Scholarly Manuscripts](https://wame.org/page3.php?id=106)	|
|	1.17	|	[Meet Claude: Anthropic’s Rival to ChatGPT](https://scale.com/blog/chatgpt-vs-claude)	|
|	1.14	|	[Microsoft in talks to acquire a 49% stake in ChatGPT owner OpenAI](https://watcher.guru/news/microsoft-plans-to-acquire-a-49-stake-in-chatgpt-owner-openai)	|
| 1.12 | Multimodal Deep Learning ([:x:](https://arxiv.org/abs/2301.04856)), ([:paperclip:](https://arxiv.org/pdf/2301.04856.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2301.04856)) |
|	1.11	|	[This Voice Doesn't Exist - Generative Voice AI](https://blog.elevenlabs.io/enter-the-new-year-with-a-bang/)	|
|	1.9	|	[Microsoft is looking at OpenAI’s GPT for Word, Outlook, and PowerPoint](https://www.theverge.com/2023/1/9/23546144/microsoft-openai-word-powerpoint-outlook-gpt-integration-rumor)	|
|	1.5	|	[Apple launches AI-powered book narrations](https://techcrunch.com/2023/01/05/apple-launches-ai-powered-book-narrations/)	|
|	1.5	|	[Microsoft, VALL-E](https://valle-demo.github.io/)	|
|	1.4	|	[ICML conference responds to LLM ethics rule](https://venturebeat.com/ai/thats-so-meta-ml-conference-debates-use-of-chatgpt-in-papers/)	|
|	1.3	|	[Enter GPTZeo](https://twitter.com/edward_the6/status/1610067688449007618?ref_src=twsrc%5Etfw)	|
|	2023.01.01	|	Collected by Jonghong Jeon (hollobit@etri.re.kr)	|

|	Date	|	Announcement	|
|:-:|:--| 
|	12.29	|	GPT Takes the Bar Exam ([:x:](https://arxiv.org/abs/2212.14402)), ([:paperclip:](https://arxiv.org/pdf/2212.14402.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2212.14402)), ([SS](https://www.semanticscholar.org/paper/How-Well-Does-ChatGPT-Do-When-Taking-the-Medical-of-Gilson-Safranek/7d4867e28b02059eef4cb25bfcd304b2071b30a9))	|
|	12.27	|	[bioarXiv - Comparing scientific abstracts generated by ChatGPT to original abstracts using an artificial intelligence output detector, plagiarism detector, and blinded human reviewers](https://www.biorxiv.org/content/10.1101/2022.12.23.521610v1)	|
| 12.26 | A large language model for electronic health records (Nature [https://doi.org/10.1038/s41746-022-00742-2](https://www.nature.com/articles/s41746-022-00742-2)), ([PDF](https://www.nature.com/articles/s41746-022-00742-2.pdf?pdf=button%20sticky)) |
| 12.26 | How Well Does ChatGPT Do When Taking the Medical Licensing Exams? The Implications of Large Language Models for Medical Education and Knowledge Assessment ([medRxiv](https://www.medrxiv.org/content/10.1101/2022.12.23.22283901v1)), ([PDF](https://www.medrxiv.org/content/10.1101/2022.12.23.22283901v1.full.pdf)) |
| 12.20 | Towards Reasoning in Large Language Models: A Survey ([:x:](https://arxiv.org/abs/2212.10403)), ([:paperclip:](https://arxiv.org/pdf/2212.10403.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2212.10403)), ([:house:](https://huggingface.co/papers/2212.10403)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-reasoning-in-large-language-models-a)), ([:octocat:](https://github.com/jeffhj/lm-reasoning)![GitHub Repo stars](https://img.shields.io/github/stars/jeffhj/lm-reasoning?style=social)), ([SS](https://www.semanticscholar.org/paper/Towards-Reasoning-in-Large-Language-Models%3A-A-Huang-Chang/db4ab91d5675c37795e719e997a2827d3d83cd45)) |
| 12.15 | Constitutional AI: Harmlessness from AI Feedback ([:x:](https://arxiv.org/abs/2212.08073)), ([:paperclip:](https://arxiv.org/pdf/2212.08073.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2212.08073)), ([:house:](https://huggingface.co/papers/2212.08073)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/constitutional-ai-harmlessness-from-ai)), ([:octocat:](https://github.com/anthropics/constitutionalharmlessnesspaper)![GitHub Repo stars](https://img.shields.io/github/stars/anthropics/constitutionalharmlessnesspaper?style=social)) |
| 12.3 | The Role of Generative Adversarial Network in Medical Image Analysis: An In-depth Survey (ACM, [https://doi.org/10.1145/3527849](https://dl.acm.org/doi/10.1145/3527849)), ([PDF](https://dl.acm.org/doi/pdf/10.1145/3527849)) |
|	11.30	|	[OpenAI, ChatGPT service](https://openai.com/blog/chatgpt)	|
| 11.29 | MegaBlocks: Efficient Sparse Training with Mixture-of-Experts ([:x:](https://arxiv.org/abs/2311.15841)), ([:book:](https://browse.arxiv.org/pdf/2311.15841.pdf)), ([:paperclip:](https://arxiv.org/pdf/2311.15841.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2311.15841)), ([:house:](https://huggingface.co/papers/2311.15841)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/megablocks-efficient-sparse-training-with)) |
| 11.28 | Fine-tuning language models to find agreement among humans with diverse preferences ([:x:](https://arxiv.org/abs/2211.15006)), ([:paperclip:](https://arxiv.org/pdf/2211.15006.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2211.15006)), ([:house:](https://huggingface.co/papers/2211.15006)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fine-tuning-language-models-to-find-agreement)) |
|	11.28	|	[NeurIPS 2022 conference](https://nips.cc/Conferences/2022)	|
| 11.21 | VectorFusion: Text-to-SVG by Abstracting Pixel-Based Diffusion Models ([:x:](https://arxiv.org/abs/2211.11319)), ([:paperclip:](https://arxiv.org/pdf/2211.11319.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2211.11319)), ([:house:](https://huggingface.co/papers/2211.11319)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vectorfusion-text-to-svg-by-abstracting-pixel)), (CVPR 2023) |
|	11.17	|	[InstructPix2Pix: Learning to Follow Image Editing Instructions](https://arxiv.org/abs/2211.09800)	|
| 11.16 | Holistic Evaluation of Language Models ([:x:](https://arxiv.org/abs/2211.09110)), ([:paperclip:](https://arxiv.org/pdf/2211.09110.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2211.09110)), ([:house:](https://huggingface.co/papers/2211.09110)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/holistic-evaluation-of-language-models)), ([:octocat:](https://github.com/stanford-crfm/helm)![GitHub Repo stars](https://img.shields.io/github/stars/stanford-crfm/helm?style=social)), ([SS](https://www.semanticscholar.org/paper/Holistic-Evaluation-of-Language-Models-Liang-Bommasani/5032c0946ee96ff11a292762f23e6377a6cf2731))	|
| 11.14 | Diffusion Models for Medical Image Analysis: A Comprehensive Survey ([:x:](https://arxiv.org/abs/2211.07804)), ([:paperclip:](https://arxiv.org/pdf/2211.07804.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2211.07804)),  ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/diffusion-models-for-medical-image-analysis-a)),  ([:octocat:](https://github.com/amirhossein-kz/awesome-diffusion-models-in-medical-imaging)![GitHub Repo stars](https://img.shields.io/github/stars/amirhossein-kz/awesome-diffusion-models-in-medical-imaging?style=social)) |
| 11.3 | Large Language Models Are Human-Level Prompt Engineers ([:x:](https://arxiv.org/abs/2211.01910)), ([:paperclip:](https://arxiv.org/pdf/2211.01910.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2211.01910)),  ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-are-human-level-prompt)), ([:octocat:](https://github.com/keirp/automatic_prompt_engineer)![GitHub Repo stars](https://img.shields.io/github/stars/keirp/automatic_prompt_engineer?style=social))  |
| 11.1 | MedSegDiff: Medical Image Segmentation with Diffusion Probabilistic Model ([:x:](https://arxiv.org/abs/2211.00611)), ([:paperclip:](https://arxiv.org/pdf/2211.00611.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2211.00611)),  ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/medsegdiff-medical-image-segmentation-with)),  ([:octocat:](https://github.com/wujunde/medsegdiff)![GitHub Repo stars](https://img.shields.io/github/stars/wujunde/medsegdiff?style=social)) |
|	10.30	|	[LlamaIndex (GPT Index) GitHub project](https://github.com/jerryjliu/llama_index)![GitHub Repo stars](https://img.shields.io/github/stars/jerryjliu/llama_index?style=social)	|
|	10.23	|	[LangChain GitHub project](https://github.com/hwchase17/langchain)![GitHub Repo stars](https://img.shields.io/github/stars/hwchase17/langchain?style=social)	|
| 9.27 | What Does DALL-E 2 Know About Radiology? ([JMIR](https://www.jmir.org/2023/1/e43110/)), ([:x:](https://arxiv.org/abs/2209.13696)), ([:paperclip:](https://arxiv.org/pdf/2209.13696.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2209.13696)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/what-does-dall-e-2-know-about-radiology)) |
|	9.19	|	[SEQUOIA - Generative AI: A Creative New World](https://www.sequoiacap.com/article/generative-ai-a-creative-new-world/)	|
| 9.15 | Brain Imaging Generation with Latent Diffusion Models [:x:](https://arxiv.org/abs/2209.07162)), ([:paperclip:](https://arxiv.org/pdf/2209.07162.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2209.07162)),  ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/brain-imaging-generation-with-latent)) |
| 9.6 | A Survey on Generative Diffusion Model ([:x:](https://arxiv.org/abs/2209.02646)), ([:paperclip:](https://arxiv.org/pdf/2209.02646.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2209.02646)),  ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-survey-on-generative-diffusion-model)),  ([:octocat:](https://github.com/chq1155/A-Survey-on-Generative-Diffusion-Model)![GitHub Repo stars](https://img.shields.io/github/stars/chq1155/A-Survey-on-Generative-Diffusion-Model?style=social)) |
| 8.25 | Understanding Diffusion Models: A Unified Perspective ([:x:](https://arxiv.org/abs/2208.11970)), ([:paperclip:](https://arxiv.org/pdf/2208.11970.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2208.11970)), ([Blog](https://calvinyluo.com/2022/08/26/diffusion-tutorial.html)) |
| 7.4 | Shifting machine learning for healthcare from development to deployment and from models to data (nature biomedical engineering, [https://doi.org/10.1038/s41551-022-00898-y](https://www.nature.com/articles/s41551-022-00898-y)), ([PDF](https://www.nature.com/articles/s41551-022-00898-y.pdf)) |
| 3.29 | Training Compute-Optimal Large Language Models  ([:x:](https://arxiv.org/abs/2303.15556)), ([:paperclip:](https://arxiv.org/pdf/2303.15556.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2303.15556)), ([:house:](https://huggingface.co/papers/2303.15556)) |
|	3.15	|	OpenAI, GPT 3.5 announce	|
| 2.11 | Compute Trends Across Three Eras of Machine Learning ([:x:](https://arxiv.org/abs/2202.05924)), ([:paperclip:](https://arxiv.org/pdf/2202.05924.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2202.05924)) |
| 2.8 | ⭐ Survey of Hallucination in Natural Language Generation ([:x:](https://arxiv.org/abs/2202.03629)), ([:paperclip:](https://arxiv.org/pdf/2202.03629.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2202.03629)),  ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/survey-of-hallucination-in-natural-language)), ([SS](https://www.semanticscholar.org/paper/Survey-of-Hallucination-in-Natural-Language-Ji-Lee/3c9ba25baca64151af4e9d50c7947de28eb2a599)) |
| 1.28 | Chain-of-Thought Prompting Elicits Reasoning in Large Language Models ([:x:](https://arxiv.org/abs/2201.11903)), ([:paperclip:](https://arxiv.org/pdf/2201.11903.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2201.11903)),  ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chain-of-thought-prompting-elicits-reasoning)), ([SS](https://www.semanticscholar.org/paper/Chain-of-Thought-Prompting-Elicits-Reasoning-in-Wei-Wang/1b6e810ce0afd0dd093f789d2b2742d047e316d5)) |
| 2022.01.01 | |
| 12.8 | Ethical and social risks of harm from Language Models ([:x:](https://arxiv.org/abs/2112.04359)), ([:paperclip:](https://arxiv.org/pdf/2112.04359.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2112.04359)),  ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ethical-and-social-risks-of-harm-from)), ([SS](https://www.semanticscholar.org/paper/Ethical-and-social-risks-of-harm-from-Language-Weidinger-Mellor/fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf)) |
| 10.19 | Future directions for chatbot research: an interdisciplinary research agenda ([paper](https://link.springer.com/article/10.1007/s00607-021-01016-7)) |
| 8.16 | ⭐ On the Opportunities and Risks of Foundation Models ([:x:](https://arxiv.org/abs/2108.07258)), ([:paperclip:](https://arxiv.org/pdf/2108.07258.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2108.07258)),  ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/on-the-opportunities-and-risks-of-foundation)) |
| 6.15 | Synthetic data in machine learning for medicine and healthcare (nature biomedical engineering, [https://doi.org/10.1038/s41551-021-00751-8](https://www.nature.com/articles/s41551-021-00751-8)), ([PDF](https://www.nature.com/articles/s41551-021-00751-8.pdf?pdf=button%20sticky)) |
| 4.18 | The Power of Scale for Parameter-Efficient Prompt Tuning ([:x:](https://arxiv.org/abs/2104.08691)), ([:paperclip:](https://arxiv.org/pdf/2104.08691.pdf)), ([:orange_book:](https://www.arxiv-vanity.com/papers/2104.08691))
| 2021.01.01 | | 
|		|	**Last Modified 2023/07/03 PM19:40** KST	|

## Additional Links
* [Awesome Korean LLM](https://github.com/NomaDamas/awesome-korean-llm)
* [Awesome-LLMOps](https://github.com/tensorchord/Awesome-LLMOps)![GitHub Repo stars](https://img.shields.io/github/stars/tensorchord/Awesome-LLMOps?style=social)
* [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)![GitHub Repo stars](https://img.shields.io/github/stars/EleutherAI/lm-evaluation-harness?style=social)
* [A collection of papers and resources related to evaluations on large language models](https://github.com/MLGroupJLU/LLM-eval-survey)![GitHub Repo stars](https://img.shields.io/github/stars/Jianing-Qiu//MLGroupJLU/LLM-eval-survey?style=social)
* [Awesome-Healthcare-Foundation-Models](https://github.com/Jianing-Qiu/Awesome-Healthcare-Foundation-Models)![GitHub Repo stars](https://img.shields.io/github/stars/Jianing-Qiu/Awesome-Healthcare-Foundation-Models?style=social)
* [LLM-evaluation](https://github.com/Hannibal046/Awesome-LLM/blob/main/paper_list/evaluation.md)
* [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM)![GitHub Repo stars](https://img.shields.io/github/stars/Hannibal046/Awesome-LLM?style=social)
* [Examples and guides for using the OpenAI API](https://github.com/openai/openai-cookbook)![GitHub Repo stars](https://img.shields.io/github/stars/openai/openai-cookbook?style=social)
* [Ultimate-Awesome-Transformer-Attention](https://github.com/cmhungsteve/Awesome-Transformer-Attention)![GitHub Repo stars](https://img.shields.io/github/stars/cmhungsteve/Awesome-Transformer-Attention?style=social)
* [Awesome Segment Anything](https://github.com/Hedlen/awesome-segment-anything)![GitHub Repo stars](https://img.shields.io/github/stars/Hedlen/awesome-segment-anything?style=social)
* [Segment Anything Model (SAM) for Medical Image Segmentation](https://github.com/YichiZhang98/SAM4MIS)![GitHub Repo stars](https://img.shields.io/github/stars/YichiZhang98/SAM4MIS?style=social)
* [GPT-4登場以降に出てきたChatGPT/LLMに関する論文や技術の振り返り](https://blog.brainpad.co.jp/entry/2023/06/05/153034) 
* [LLM Collection](https://www.promptingguide.ai/models/collection)
* [🤗 Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
* [AI Incident Database](https://incidentdatabase.ai/)
* [Daily papers by AK](https://huggingface.co/papers)
* [Awesome-Generative-RecSys](https://github.com/jihoo-kim/Awesome-Generative-RecSys)![GitHub Repo stars](https://img.shields.io/github/stars/jihoo-kim/Awesome-Generative-RecSys?style=social) - A curated list of Generative Recommender Systems (Paper & Code)
* [Prompt Engineering Guide](https://www.promptingguide.ai/) - [papers](https://www.promptingguide.ai/papers) - [:octocat:](https://github.com/dair-ai/Prompt-Engineering-Guide)![GitHub Repo stars](https://img.shields.io/github/stars/dair-ai/Prompt-Engineering-Guide?style=social)
* [awesome-ChatGPT-repositories](https://github.com/taishi-i/awesome-ChatGPT-repositories)![GitHub Repo stars](https://img.shields.io/github/stars/taishi-i/awesome-ChatGPT-repositories?style=social) 
* [The Rundown](https://www.therundown.ai/)
* [WEEKLY PAPERS](https://papers.labml.ai/papers/weekly)
* [Primo.ai LLM wiki](https://primo.ai/index.php?title=Large_Language_Model_(LLM))
* [ML Papers of the Week](https://github.com/dair-ai/ML-Papers-of-the-Week)![GitHub Repo stars](https://img.shields.io/github/stars/dair-ai/ML-Papers-of-the-Week?style=social)
* [CS 324 - Advances in Foundation Models](https://stanford-cs324.github.io/winter2023/)
* [ML timeline](https://github.com/osanseviero/ml_timeline)![GitHub Repo stars](https://img.shields.io/github/stars/osanseviero/ml_timeline?style=social)
* [ChatGPT Timeline](https://timelines.issarice.com/wiki/Timeline_of_ChatGPT)
* [OpenAI Timeline](https://www.jointjs.com/demos/chatgpt-timeline)
* [LLM Explained: The LLM Training Landscape](https://liu-gendary.medium.com/llm-explained-the-llm-training-landscape)
<img src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*vZK250i8PIWid6BiaZ1QCA.png">
* [Major Updates on the LLM Survey 🔥](https://github.com/RUCAIBox/LLMSurvey)![GitHub Repo stars](https://img.shields.io/github/stars/RUCAIBox/LLMSurvey?style=social)
<img src="https://pbs.twimg.com/media/F0Re2OiaYAU3p-l?format=jpg&name=large">
* [Generative Artificial Intelligence Stack Language](https://www.reddit.com/r/AILinksandTools/comments/126z1pd/the_generative_ai_tech_stack_for_language_march/)
<img src="https://i.redd.it/r2kr7trwwxqa1.png">
* [Awesome-Multimodal-Large-Language-Models](https://github.com/bradyfu/awesome-multimodal-large-language-models)
<img src="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/blob/main/images/xmind.png?raw=true">
* [SEQUOIA - The New Language Model Stack](https://www.sequoiacap.com/article/llm-stack-perspective/)
<img src="https://www.sequoiacap.com/wp-content/uploads/sites/6/2023/06/llm-landscape-9.png">
* The Rise and Rise of A.I. LLMs
<img src="https://pbs.twimg.com/media/FwTUtyKacAAgQbQ?format=jpg&name=large">
* [The Practical Guides for Large Language Models](https://github.com/Mooler0410/LLMsPracticalGuide)
<img src="https://github.com/Mooler0410/LLMsPracticalGuide/blob/main/imgs/survey-gif-test.gif">
* [AI / ML / LLM / Transformer Models Timeline](https://ai.v-gar.de/ml/transformer/timeline/) 
<img src="https://ai.v-gar.de/ml/transformer/timeline/timeline.png">
* [Transformer models: an introduction and catalog — 2023 Edition](https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/)
<img src="https://amatriain.net/blog/images/02-05.png" bgcolor=white>
<img src="https://amatriain.net/blog/images/02-09.png">
* [open-source LLMs](https://twitter.com/theaievangelist/status/1645809824314298368)
<img src="https://pbs.twimg.com/media/FtZQSU3aMAI4BP1?format=jpg&name=4096x4096">
* Got It AI’s LLM hallucination rate comparison
<img src="https://lh5.googleusercontent.com/cgHE-XSe8AZBUuFIQw-Vu6XqYFxvKWj5BjCPsWAxkre2G8WLLkVLhp0DyDTlSTYFQiUyG_XUvZU2ZtM212SuU9rfbNxEtQI0kEpm8sSKF7CUsJZpu0pY9FaT2qHVpPgrBRLeJZdsdyBaKMw5Tac8M7Y">
* [A summary of large language models (A Survey of Large Language Models)](https://arxiv.org/abs/2303.18223)
 <img src="https://github.com/hollobit/GenAI_LLM_timeline/assets/998803/9a855dea-7223-4523-924e-3952b1f3734d">
* [A history of the most important ge![Uploading 스크린샷 2023-05-18 오후 5.58.51.png…]()
nerative AI models, from 2014 to 2023 @davidtfoster](https://www.linkedin.com/feed/update/urn:li:activity:7044233450295316480/)
 <img src="https://github.com/hollobit/GenAI_LLM_timeline/assets/998803/09140224-638f-448c-a990-1201741927c7">
